# ========================================
# UMIS 환경 변수 설정 (v7.11.0)
# ========================================
#
# 사용법:
#   1. 이 파일을 .env로 복사
#   2. 아래 설정 입력
#   3. 저장
#
# ========================================

# ========================================
# 🎯 UMIS 전역 설정 (6-Agent System)
# ========================================

# ========================================
# LLM 모드 설정 (v7.11.0: LLM Complete Abstraction)
# ========================================
LLM_MODE=cursor
#
# 옵션 (2가지만):
#   cursor    - Cursor AI 사용 (무료, 대화형, 권장!) ✅
#   external  - External LLM API 사용 (자동화, Stage별 모델 선택)
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# cursor 모드 (기본값, 권장):
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#   → Cursor AI 직접 사용, API 불필요
#   → 비용: $0 (RAG 임베딩만)
#   → 대화형 추정 (Composer/Chat에서)
#   → 아래 Stage별 모델 설정 무시
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# external 모드 (자동화 필요 시):
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#   → OpenAI/Anthropic API 호출
#   → 아래 LLM_MODEL_STAGE1/2/3 설정 적용
#   → Stage별 최적 모델 자동 선택
#   → 비용: Stage별 차등 과금 (98% 절감)
#
# 참고:
#   - 설정: LLMProvider (Dependency Injection)
#   - 문서: dev_docs/improvements/LLM_COMPLETE_ABSTRACTION_SUMMARY_v7_11_0.md
#            → Observer: RAG 검색만 (시장 구조 패턴 매칭)
#            → Quantifier: RAG 검색만 (계산 방법론 검색)
#            → Validator: RAG 검색 (데이터 소스 85% 커버)
#            → Estimator: 5-Phase (RAG + Web Search, LLM API 없음)
#            → Guardian: 목표/순환/품질 모니터링
#            
#            비용: $0 (RAG 임베딩만, ~$0.0001/요청)
#            워크플로우: Python RAG → Cursor Composer 분석
#
#   external - External API LLM 사용 (자동화, 비용 발생)
#            
#            Agent별 동작:
#            → Explorer: RAG + OpenAI API 호출 → 완성된 가설
#            → 나머지 Agent: native와 동일 (RAG만)
#            
#            비용: ~$0.10/요청 (100회 시 $10)
#            워크플로우: Python RAG + API → 완성된 결과
#            사용 시나리오: 완전 자동화, 배치 처리, cron job
#
# v7.7.0 변경사항:
#   - 이전 (v7.4.0): Native 모드 개념만 존재, 실제로는 External 동작
#   - 현재 (v7.7.0): Native 모드 실제 구현 완료! ✅
#   - LLMProvider 클래스 추가
#   - Explorer Native/External 분기 처리
#   - 테스트: python scripts/test_native_mode.py

# ========================================
# 참고: Estimator (Fermi) Agent - 4-Stage Fusion Architecture (v7.11.0)
# ========================================
#
# Estimator는 4-Stage Fusion Architecture로 작동합니다. ⭐ v7.11.0
# Native/External 모드 모두에서 LLM API 호출 없이 동작 (비용 $0)
#
# 용어 정의:
#   - Stage: Estimator 전체 프로세스 단계 (1-4)
#   - source: 추정 소스 (Literal/Prior/Fermi/Fusion)
#   - certainty: LLM 내부 확신도 (high/medium/low)
#
# Stage 1: Evidence Collection (증거 수집, <1초)
#   ├─ Literal: 프로젝트 명시 데이터
#   ├─ Direct RAG: 학습 규칙 (95%+ 커버)
#   ├─ Validator: 확정 데이터 (85% 처리!)
#   └─ Guardrail: 논리적/경험적 제약
#   → Early Return (확정값 발견 시 즉시 반환)
#
# Stage 2: Generative Prior (생성적 사전, ~3초)
#   └─ LLM 직접 값 요청 + certainty 평가
#   → certainty == high → 종료
#
# Stage 3: Structural Explanation (구조적 설명, 3-5초)
#   └─ Fermi 분해 (재귀 없음, max_depth=2)
#      - 변수 식별 → Stage 2로 추정 → 공식 계산
#
# Stage 4: Fusion & Validation (융합, <1초)
#   └─ 모든 Stage 결과 가중 합성 → 최종 값
#
# 주요 변경사항 (v7.11.0):
#   - Phase 0-4 (5단계) → Stage 1-4 (4단계)
#   - Phase 4 재귀 제거 → max_depth=2 고정
#   - confidence (0.0-1.0) → certainty (high/medium/low)
#   - PhaseConfig → Budget (자원 제어)
#   - 속도 향상: 3-10배 (10-30초 → 3-5초)
#
# 특징:
#   - 100% 커버리지 (LLM API 없이)
#   - Web Search 자동화 (DuckDuckGo/Google)
#   - Budget 기반 탐색 (max_llm_calls, max_runtime)
#   - 예측 가능한 실행 시간
#
# ⚠️ Deprecated:
#   - Phase 0-4 Architecture (v7.10.2) → Stage 1-4로 대체
#   - 하위 호환성: compat.py (DeprecationWarning)
#
# 상세: docs/api/ESTIMATOR_API_v7_11_0.md
#
# ========================================

# ========================================
# 🔑 API Keys
# ========================================

# OpenAI API 키 (필수!)
OPENAI_API_KEY=your-openai-api-key-here

# Anthropic (Claude) API 키 (선택, 벤치마크용)
# 발급: https://console.anthropic.com/
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# ========================================
# 🏢 한국 공공 데이터 API (Gap #2 실제 데이터)
# ========================================

# DART (전자공시) API 키
# 발급: https://opendart.fss.or.kr → 인증키 신청/관리
# 무료, 즉시 발급
# 형식: 40자 영숫자
DART_API_KEY=your-dart-api-key-here

# KOSIS (통계청) API 키
# 발급: https://kosis.kr/openapi/index/index.jsp → API 신청
# 무료, 승인 필요 (1-2일)
# ⚠️ 주의: Key에 '=' 문자 포함 시 따옴표로 감싸기!
#   예: KOSIS_API_KEY="abc123=def456=xyz"
KOSIS_API_KEY=your-kosis-api-key-here

# 참고: API Key에 특수문자 (=, &, # 등) 포함 시:
#   - 큰따옴표 사용: KEY="value=with=equals"
#   - 작은따옴표 사용: KEY='value=with=equals'
#   - 둘 다 가능, 큰따옴표 권장

# ========================================
# 🌐 Web Search 설정 (v7.6.2)
# ========================================

# 검색 엔진 선택: "duckduckgo" (무료, 기본) or "google" (유료, 고품질)
WEB_SEARCH_ENGINE=duckduckgo

# Web Search 활성화 여부
WEB_SEARCH_ENABLED=true

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# 페이지 크롤링 설정 (v7.7.0+)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# 검색 결과의 전체 페이지를 크롤링할지 여부
# true: URL을 방문해서 실제 페이지 내용 추출 (정확도 향상, snippet의 500자 제한 없음)
# false: snippet만 사용 (빠르지만 정보 제한적)
WEB_SEARCH_FETCH_FULL_PAGE=true

# 페이지당 최대 추출 문자 수 (기본 5000자)
WEB_SEARCH_MAX_CHARS=5000

# 페이지 크롤링 타임아웃 (초, 기본 10초)
WEB_SEARCH_TIMEOUT=10

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Google Custom Search (선택적 - google 사용 시만 필요)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# API 키: https://console.cloud.google.com/apis/credentials
# Engine ID: https://programmablesearchengine.google.com/
GOOGLE_API_KEY=your-google-api-key-here
GOOGLE_SEARCH_ENGINE_ID=your-search-engine-id-here

# 비용: $5/1000 쿼리 (100/일 무료)
# 품질: 최고
# 권장: 프로덕션, 품질 중시

# ========================================
# 📦 RAG 설정
# ========================================

# Embedding 모델
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIMENSION=3072

# ========================================
# 🤖 LLM 최적화 전략 (v7.11.0: Stage 기반)
# ========================================
# Stage별 LLM 모델 설정 (External API 모드 전용)
# ========================================
#
# ⚠️ 주의: 이 설정은 LLM_MODE=cursor가 아닐 때만 적용됩니다!
#
# LLM_MODE=cursor (기본):
#   → Cursor AI 사용, 아래 설정 무시
#   → 비용: $0 (RAG 임베딩만)
#   → 설정 불필요
#
# LLM_MODE=gpt-4o-mini (또는 기타 모델):
#   → External LLM API 사용
#   → 아래 Stage별 모델 설정 적용
#   → Stage별 최적 모델 자동 선택으로 98% 비용 절감!
#
# External API 효과:
#   기존: $15.00/1,000회 (Sonnet Think)
#   최적화: $0.30/1,000회
#   절감: 98% ⭐
#
# Stage별 모델 전략 (External 모드만):
#   Stage 1 (Evidence, 85%): gpt-4.1-nano ($0.000033/작업, <1초, 100% 정확)
#     → Literal, RAG, Validator, Guardrail 검색만
#   
#   Stage 2 (Prior, 10%): gpt-4o-mini ($0.000121/작업, ~3초, 100% 정확)
#     → LLM 직접 추정 + certainty 평가
#   
#   Stage 3 (Fermi, 5%): o1-mini ($0.0033/작업, 3-5초, 95% 정확)
#     → 구조적 분해 (재귀 없음, max_depth=2)
#     → 변수 추정은 Stage 2 사용
#   
#   Stage 4 (Fusion, <1%): 계산만, LLM 불필요
#     → Stage 1-3 결과 가중 합성
#
# v7.11.0 주요 변경:
#   - Phase 0-2 → Stage 1 (Evidence Collection 통합)
#   - Phase 3 → Stage 2 (Generative Prior)
#   - Phase 4 → Stage 3 (Structural Explanation, 재귀 제거)
#   - 신규: Stage 4 (Fusion & Validation)
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# v7.8.0 Model Config 시스템 (중앙 집중식 관리)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#
# 모델 설정은 config/model_configs.yaml에서 중앙 관리됩니다.
# 아래 변수로 모델명만 변경하면 API 파라미터가 자동으로 적용됩니다!
#
# 지원 모델 (17개, 2025-11-24 기준):
#   - o1 시리즈: o1-mini, o1, o1-2024-12-17, o1-pro, o1-pro-2025-03-19
#   - o3 시리즈: o3, o3-2025-04-16, o3-mini, o3-mini-2025-01-31
#   - gpt-5 시리즈: gpt-5.1, gpt-5-pro
#   - gpt-4 시리즈: gpt-4.1-nano, gpt-4o-mini, gpt-4-turbo-preview
#   - gpt-4o 시리즈: gpt-4o, gpt-4o-2024-08-06
#
# 자동 적용 내용:
#   ✅ API 타입 (responses vs chat.completions)
#   ✅ max_output_tokens (모델별 최적값)
#   ✅ reasoning_effort 설정 (지원 모델만)
#   ✅ temperature 설정 (지원 모델만)
#   ✅ Pro 모델 Fast Mode 자동 적용
#
# 예시:
#   LLM_MODEL_STAGE3=o1-mini
#     → Responses API, max_output_tokens=16000, reasoning.effort=medium
#
#   LLM_MODEL_STAGE3=gpt-5.1
#     → Responses API, max_output_tokens=16000, reasoning.effort=high
#
#   LLM_MODEL_STAGE3=gpt-5-pro
#     → Responses API, reasoning.effort=high (고정), Fast Mode 자동
#
#   LLM_MODEL_STAGE3=o3-mini-2025-01-31
#     → 최신 모델 즉시 사용 (코드 수정 불필요)
#
# 신규 모델 추가:
#   1. config/model_configs.yaml에 5줄 추가
#   2. 즉시 사용 가능 (코드 수정 0줄!)
#
# 상세 문서:
#   - config/model_configs.yaml (모델 정의)
#   - dev_docs/improvements/V7_11_0_MIGRATION_COMPLETE.md
#
# ========================================

# Stage별 모델 자동 선택 활성화 (External 모드만 적용)
# Native 모드에서는 이 설정이 무시됩니다
USE_STAGE_BASED_ROUTING=true

# Stage 1: Evidence Collection (85% 작업) - External 모드만
# 권장 모델: gpt-4.1-nano (저비용, 고속, 검색 전용)
LLM_MODEL_STAGE1=gpt-4.1-nano

# Stage 2: Generative Prior (10% 작업) - External 모드만
# 권장 모델: gpt-4o-mini (중간 추론, 직접 추정)
LLM_MODEL_STAGE2=gpt-4o-mini

# Stage 3: Structural Explanation (5% 작업) - External 모드만
# 권장 모델: o1-mini (기본), o3-mini-2025-01-31 (최고 성능), gpt-5.1 (고급 추론)
# Pro 모델: gpt-5-pro, o1-pro (고비용, Fast Mode 자동 적용)
# ⚠️ 주의: max_depth=2 고정, 재귀 없음
LLM_MODEL_STAGE3=o1-mini

# Legacy 모델 (하위 호환성, Stage 라우팅 비활성화 시 사용) - External 모드만
LLM_MODEL=gpt-4-turbo-preview
LLM_TEMPERATURE=0.7

# Chroma DB 경로
CHROMA_PERSIST_DIR=./data/chroma

# ========================================
# 🔗 Neo4j 설정 (Knowledge Graph)
# ========================================

NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=umis_password
NEO4J_DATABASE=neo4j

# ========================================
# 🔍 LangSmith 설정 (선택적, 디버깅/모니터링)
# ========================================

# LangSmith Tracing 활성화 (기본: false)
# true로 설정 시 모든 LLM 호출이 LangSmith에 기록됨
LANGCHAIN_TRACING_V2=false

# LangSmith API 키 (활성화 시 필요)
# 받기: https://smith.langchain.com/
LANGCHAIN_API_KEY=your-langchain-api-key-here

# LangSmith 프로젝트명 (기본: umis-rag)
LANGCHAIN_PROJECT=umis-rag

# 용도:
#   - LLM 호출 추적 및 디버깅
#   - 성능 모니터링
#   - 비용 추적
#   - 품질 개선
#
# 권장:
#   - 개발 중: false (불필요)
#   - 디버깅: true (문제 해결 시)
#   - 프로덕션: false (개인정보 보호)

