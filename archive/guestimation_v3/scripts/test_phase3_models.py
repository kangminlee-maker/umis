#!/usr/bin/env python3
"""
Phase 3 ì „ìš© ëª¨ë¸ í…ŒìŠ¤íŠ¸
gpt-5-mini, gpt-4.1-mini ë“± ì¤‘ê¸‰ ëª¨ë¸ ì„±ëŠ¥ í™•ì¸
"""

import os
import json
import time
from typing import Dict, Any
from datetime import datetime
from openai import OpenAI

from dotenv import load_dotenv
load_dotenv()


class Phase3ModelTest:
    """
    Phase 3 ì¤‘ê¸‰ ì‘ì—… ì „ìš© ëª¨ë¸ í…ŒìŠ¤íŠ¸
    """
    
    def __init__(self):
        self.client = OpenAI()
        
        # í…ŒìŠ¤íŠ¸ ëŒ€ìƒ: nanoì™€ standard ì‚¬ì´ ëª¨ë¸ë“¤
        self.models = [
            'gpt-4o-mini',      # ê¸°ì¤€ (ê²€ì¦ë¨)
            'gpt-5-mini',       # NEW
            'gpt-4.1-mini',     # NEW
            'gpt-4o',           # ë¹„êµìš©
            'gpt-4.1'           # NEW
        ]
        
        # ê°€ê²© ì •ë³´
        self.pricing = {
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-5-mini': {'input': 0.25, 'output': 2.00},
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
            'gpt-4o': {'input': 2.50, 'output': 10.00},
            'gpt-4.1': {'input': 2.00, 'output': 8.00}
        }
        
        self.results = []
    
    def get_phase3_scenarios(self):
        """
        Phase 3 ì‹œë‚˜ë¦¬ì˜¤ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸)
        """
        return [
            {
                'id': 'P3-001',
                'name': 'Phase 3 (í…œí”Œë¦¿ ìˆìŒ) - ê°œì„ ëœ í”„ë¡¬í”„íŠ¸',
                'complexity': 'medium',
                'prompt': '''B2B SaaS í•œêµ­ ì‹œì¥ í‰ê·  ARPUë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  ì˜ˆì‹œ:
ì§ˆë¬¸: "B2B SaaS ê¸€ë¡œë²Œ ARPUëŠ”?"
ë‹µ: $100

ì´ì œ í•œêµ­ ì‹œì¥ì„ ì¶”ì •í•˜ì„¸ìš”.

ë‹¨ê³„ë³„ ê³„ì‚°:
1. ê¸€ë¡œë²Œ ë²¤ì¹˜ë§ˆí¬: $100
2. í•œêµ­ ì¡°ì •: $100 Ã— 0.6 = $60 (GDP per capita ë¹„ìœ¨)
3. B2B ë°°ìˆ˜ ì ìš©: $60 Ã— 3 = $180 (B2BëŠ” B2Cì˜ 3ë°°)
4. í™˜ìœ¨ ì ìš©: $180 Ã— 1,300 = 234,000ì›
5. ë°˜ì˜¬ë¦¼: 200,000ì›

ìœ„ ë‹¨ê³„ë¥¼ ë”°ë¼ ê³„ì‚°í•˜ì„¸ìš”.

JSON í˜•ì‹:
{
  "value": ìˆ«ì,
  "unit": "ì›",
  "confidence": 0.65-0.75,
  "step1_global": ê°’,
  "step2_korea": ê°’,
  "step3_b2b": ê°’,
  "step4_krw": ê°’,
  "reasoning": "í•œ ë¬¸ì¥ ìš”ì•½"
}''',
                'expected_answer': '180,000-240,000ì›',
                'correct_range': (180000, 240000)
            },
            
            {
                'id': 'P3-002',
                'name': 'Phase 3 (í…œí”Œë¦¿ ì—†ìŒ) - ì°½ì˜ì  ì¶”ì •',
                'complexity': 'complex',
                'prompt': '''í•œêµ­ ì˜¨ë¼ì¸ êµìœ¡ í”Œë«í¼ì˜ ì„±ì¸ ì·¨ë¯¸ ê°•ì¢Œ ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤ ì‚¬í•­:
- íƒ€ê²Ÿ: ì„±ì¸ ì·¨ë¯¸ í•™ìŠµì (ì§ì¥ì¸ 30-40ëŒ€)
- ê²½ìŸì‚¬: í´ë˜ìŠ¤101, íƒˆì‰, í”„ë¦½
- ì‹œì¥: í•œêµ­
- ì½˜í…ì¸ : í”¼ì•„ë…¸, ê·¸ë¦¼, ìš”ë¦¬ ë“± ì·¨ë¯¸ ê°•ì¢Œ

ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ì„¸ìš”:
1. ê²½ìŸì‚¬ ê°€ê²© ì¡°ì‚¬ (ì•Œë ¤ì§„ ì •ë³´ í™œìš©)
2. íƒ€ê²Ÿ ê³ ê° ì§€ë¶ˆ ì˜í–¥
3. ì½˜í…ì¸  ê°€ì¹˜
4. ì‹œì¥ í¬ì§€ì…”ë‹

JSON í˜•ì‹:
{
  "value": ìˆ«ì,
  "unit": "ì›",
  "confidence": 0.60-0.75,
  "competitive_analysis": "ê²½ìŸì‚¬ ë¶„ì„",
  "reasoning": "ì¢…í•© íŒë‹¨"
}''',
                'expected_answer': '20,000-50,000ì›',
                'correct_range': (20000, 50000)
            },
            
            {
                'id': 'P3-003',
                'name': 'Phase 3 (ë²¤ì¹˜ë§ˆí¬ ì¡°ì •) - ë‹¤ë¥¸ ë„ë©”ì¸',
                'complexity': 'medium',
                'prompt': '''í•œêµ­ B2C ì»¤í”¼ ë°°ë‹¬ ì•±ì˜ ê±´ë‹¹ ë°°ë‹¬ë¹„ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  ë°ì´í„°:
- ê¸€ë¡œë²Œ ìŒì‹ ë°°ë‹¬ í‰ê·  ë°°ë‹¬ë¹„: $3-5
- í•œêµ­ ë¬¼ê°€ ìˆ˜ì¤€: ê¸€ë¡œë²Œ ëŒ€ë¹„ 70%
- ì»¤í”¼ëŠ” ìŒì‹ë³´ë‹¤ ê°„í¸: 0.8ë°°

ë‹¨ê³„ë³„ ê³„ì‚°:
1. ê¸€ë¡œë²Œ ì¤‘ê°„ê°’: $4
2. í•œêµ­ ì¡°ì •: $4 Ã— 0.7 = $2.8
3. ì»¤í”¼ íŠ¹ì„±: $2.8 Ã— 0.8 = $2.24
4. í™˜ìœ¨: $2.24 Ã— 1,300 = 2,912ì›
5. ë°˜ì˜¬ë¦¼: 3,000ì›

ìœ„ ë…¼ë¦¬ë¥¼ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.

JSON í˜•ì‹:
{
  "value": ìˆ«ì,
  "unit": "ì›",
  "confidence": 0.70,
  "reasoning": "ê³„ì‚° ê³¼ì • ìš”ì•½"
}''',
                'expected_answer': '2,500-3,500ì›',
                'correct_range': (2500, 3500)
            }
        ]
    
    def test_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        print(f"\n{'='*80}")
        print(f"ğŸ¤– ëª¨ë¸: {model}")
        print(f"ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"{'='*80}\n")
        
        print(f"â³ {model} í˜¸ì¶œ ì¤‘...")
        start_time = time.time()
        
        try:
            # API í˜¸ì¶œ
            if model.startswith('o1') or model.startswith('o3'):
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": scenario['prompt']}]
                )
            elif 'gpt-5' in model:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": scenario['prompt']}
                    ],
                    response_format={"type": "json_object"}
                )
            else:
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": scenario['prompt']}
                    ],
                    temperature=0.2,
                    response_format={"type": "json_object"}
                )
            
            elapsed = time.time() - start_time
            
            # íŒŒì‹±
            content = response.choices[0].message.content
            try:
                parsed = json.loads(content)
            except:
                parsed = {'raw': content, 'parse_error': True}
            
            # ë¹„ìš©
            usage = response.usage
            cost = self._calculate_cost(model, usage.prompt_tokens, usage.completion_tokens)
            
            # ì •í™•ë„ ìë™ í‰ê°€
            auto_eval = self._auto_evaluate(parsed, scenario)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nâœ… ì‘ë‹µ ë°›ìŒ")
            print(f"   ë¹„ìš©: ${cost:.6f}")
            print(f"   ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"   í† í°: {usage.total_tokens} ({usage.prompt_tokens}â†’{usage.completion_tokens})")
            print()
            print("ğŸ“„ ì‘ë‹µ:")
            print(json.dumps(parsed, ensure_ascii=False, indent=2))
            print()
            print(f"ğŸ¯ ê¸°ëŒ€: {scenario['expected_answer']}")
            print(f"ğŸ“Š ìë™ í‰ê°€: {auto_eval['accuracy']} ({auto_eval['reason']})")
            print()
            
            return {
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'response': parsed,
                'expected': scenario['expected_answer'],
                'auto_evaluation': auto_eval,
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'tokens': {
                    'input': usage.prompt_tokens,
                    'output': usage.completion_tokens,
                    'total': usage.total_tokens
                },
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            elapsed = time.time() - start_time
            print(f"\nâŒ ì˜¤ë¥˜: {str(e)}")
            
            return {
                'model': model,
                'scenario_id': scenario['id'],
                'error': str(e),
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        rates = self.pricing.get(model, {'input': 0, 'output': 0})
        cost = (input_tokens / 1_000_000 * rates['input'] +
                output_tokens / 1_000_000 * rates['output'])
        return cost
    
    def _auto_evaluate(self, response: Dict, scenario: Dict) -> Dict[str, Any]:
        """ìë™ í‰ê°€ (ë²”ìœ„ í™•ì¸)"""
        if 'parse_error' in response:
            return {'accuracy': 'JSON íŒŒì‹± ì‹¤íŒ¨', 'score': 0, 'reason': 'JSON í˜•ì‹ ì˜¤ë¥˜'}
        
        value = response.get('value')
        if value is None:
            return {'accuracy': 'ê°’ ì—†ìŒ', 'score': 0, 'reason': 'ì‘ë‹µì— value ì—†ìŒ'}
        
        # ë²”ìœ„ í™•ì¸
        min_val, max_val = scenario['correct_range']
        
        if min_val <= value <= max_val:
            return {'accuracy': 'âœ… ë²”ìœ„ ë‚´', 'score': 100, 'reason': f'{min_val:,}-{max_val:,}ì› ë²”ìœ„ ë‚´'}
        elif min_val * 0.7 <= value <= max_val * 1.3:
            deviation = abs(value - (min_val + max_val) / 2) / ((min_val + max_val) / 2) * 100
            return {'accuracy': 'âš ï¸ í—ˆìš© ë²”ìœ„', 'score': 80, 'reason': f'Â±30% ë‚´, í¸ì°¨ {deviation:.0f}%'}
        else:
            deviation = abs(value - (min_val + max_val) / 2) / ((min_val + max_val) / 2) * 100
            return {'accuracy': 'âŒ ë²”ìœ„ ë²—ì–´ë‚¨', 'score': 50, 'reason': f'í¸ì°¨ {deviation:.0f}%'}
    
    def run_test(self):
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("="*80)
        print("Phase 3 ì¤‘ê¸‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("="*80)
        print()
        
        scenarios = self.get_phase3_scenarios()
        
        print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {len(self.models)}ê°œ")
        for idx, model in enumerate(self.models, 1):
            rates = self.pricing[model]
            cost_per_task = (rates['input'] * 1000 / 1_000_000 + rates['output'] * 500 / 1_000_000)
            print(f"  {idx}. {model:20s} - ${cost_per_task:.6f}/ì‘ì—…")
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(scenarios)}ê°œ")
        for idx, sc in enumerate(scenarios, 1):
            print(f"  {idx}. {sc['name']}")
        
        print(f"\nì´ í…ŒìŠ¤íŠ¸: {len(self.models) * len(scenarios)}ê°œ")
        print()
        
        # ì§„í–‰
        total = len(self.models) * len(scenarios)
        count = 0
        
        for model in self.models:
            print(f"\n{'#'*80}")
            print(f"# ëª¨ë¸: {model}")
            print(f"{'#'*80}")
            
            for scenario in scenarios:
                count += 1
                print(f"\n[{count}/{total}] ì§„í–‰ ì¤‘...")
                
                result = self.test_model(model, scenario)
                self.results.append(result)
                
                time.sleep(0.5)  # Rate limit ë°©ì§€
        
        # ì €ì¥
        output_file = f'benchmark_phase3_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_tests': len(self.results),
                    'successful': sum(1 for r in self.results if r['success'])
                },
                'results': self.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë¦¬í¬íŠ¸
        self.generate_report()
    
    def generate_report(self):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n{'='*80}")
        print("ğŸ“Š Phase 3 ëª¨ë¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸")
        print(f"{'='*80}\n")
        
        success_results = [r for r in self.results if r['success']]
        
        # ëª¨ë¸ë³„ í†µê³„
        model_stats = {}
        for result in success_results:
            model = result['model']
            
            if model not in model_stats:
                model_stats[model] = {
                    'costs': [],
                    'times': [],
                    'scores': [],
                    'correct_count': 0,
                    'total_count': 0
                }
            
            model_stats[model]['costs'].append(result['cost'])
            model_stats[model]['times'].append(result['elapsed_seconds'])
            
            auto_eval = result.get('auto_evaluation', {})
            score = auto_eval.get('score', 0)
            model_stats[model]['scores'].append(score)
            model_stats[model]['total_count'] += 1
            
            if score == 100:
                model_stats[model]['correct_count'] += 1
        
        # í…Œì´ë¸” ì¶œë ¥
        print(f"{'ëª¨ë¸':20s} | {'ì •í™•ë„':10s} | {'í‰ê·  ë¹„ìš©':12s} | {'í‰ê·  ì‹œê°„':10s} | {'í‰ê·  ì ìˆ˜':10s} | ê°€ì„±ë¹„")
        print("-" * 95)
        
        summaries = []
        for model, stats in model_stats.items():
            accuracy = stats['correct_count'] / stats['total_count'] * 100
            avg_cost = sum(stats['costs']) / len(stats['costs'])
            avg_time = sum(stats['times']) / len(stats['times'])
            avg_score = sum(stats['scores']) / len(stats['scores'])
            
            value_score = avg_score / (avg_cost * 1_000_000) if avg_cost > 0 else 0
            
            summaries.append({
                'model': model,
                'accuracy': accuracy,
                'avg_cost': avg_cost,
                'avg_time': avg_time,
                'avg_score': avg_score,
                'value_score': value_score
            })
            
            print(f"{model:20s} | {accuracy:7.0f}%   | ${avg_cost:.6f}   | {avg_time:7.2f}ì´ˆ   | {avg_score:7.0f}ì    | {value_score:8.0f}")
        
        # ê°€ì„±ë¹„ ìˆœ ì •ë ¬
        summaries.sort(key=lambda s: s['value_score'], reverse=True)
        
        print(f"\n{'='*80}")
        print("ğŸ† Phase 3 ê°€ì„±ë¹„ ë­í‚¹")
        print(f"{'='*80}\n")
        
        for idx, summary in enumerate(summaries, 1):
            print(f"{idx}ìœ„: {summary['model']}")
            print(f"   ì •í™•ë„: {summary['accuracy']:.0f}%")
            print(f"   í‰ê·  ì ìˆ˜: {summary['avg_score']:.0f}ì ")
            print(f"   ë¹„ìš©: ${summary['avg_cost']:.6f}/ì‘ì—…")
            print(f"   ì‹œê°„: {summary['avg_time']:.2f}ì´ˆ")
            print(f"   ê°€ì„±ë¹„: {summary['value_score']:.0f}")
            print()
        
        # ì¶”ì²œ
        print(f"{'='*80}")
        print("ğŸ’¡ ì¶”ì²œ")
        print(f"{'='*80}\n")
        
        best = summaries[0]
        
        print(f"Phase 3 (í…œí”Œë¦¿ ìˆìŒ) ìµœì  ëª¨ë¸:")
        print(f"  â†’ {best['model']}")
        print(f"     í’ˆì§ˆ: {best['avg_score']:.0f}ì ")
        print(f"     ë¹„ìš©: ${best['avg_cost']:.6f}")
        print(f"     ê°€ì„±ë¹„: {best['value_score']:.0f}")
        print()
        
        # GPT-4o-miniì™€ ë¹„êµ
        mini_stats = next((s for s in summaries if s['model'] == 'gpt-4o-mini'), None)
        
        if mini_stats and best['model'] != 'gpt-4o-mini':
            print(f"vs GPT-4o-mini:")
            print(f"  í’ˆì§ˆ: {best['avg_score']:.0f} vs {mini_stats['avg_score']:.0f} ({best['avg_score'] - mini_stats['avg_score']:+.0f}ì )")
            print(f"  ë¹„ìš©: ${best['avg_cost']:.6f} vs ${mini_stats['avg_cost']:.6f}")
            
            if best['avg_cost'] < mini_stats['avg_cost']:
                saving = (1 - best['avg_cost'] / mini_stats['avg_cost']) * 100
                print(f"  ì ˆê°: {saving:.0f}%")
            else:
                increase = (best['avg_cost'] / mini_stats['avg_cost'] - 1) * 100
                print(f"  ì¶”ê°€ ë¹„ìš©: +{increase:.0f}%")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    tester = Phase3ModelTest()
    tester.run_test()


if __name__ == "__main__":
    main()




