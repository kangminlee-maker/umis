#!/usr/bin/env python3
"""
gpt-5-pro ë¬¸ì œ 1 ì¬í…ŒìŠ¤íŠ¸ (ìµœì í™”ëœ max_output_tokens 48K)
"""

import os
import json
import time
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

from phase4_common import (
    get_model_config,
    build_api_params,
    call_model_api,
    get_phase4_scenarios,
    evaluate_fermi_response
)

load_dotenv()


def test_gpt5_pro_problem1():
    """gpt-5-proë¡œ ë¬¸ì œ 1ë§Œ ì¬í…ŒìŠ¤íŠ¸"""
    
    print("=" * 100)
    print("gpt-5-pro ë¬¸ì œ 1 ì¬í…ŒìŠ¤íŠ¸ (ìµœì í™”ëœ max_output_tokens)")
    print("=" * 100)
    print()
    
    client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    scenarios = get_phase4_scenarios()
    
    # ë¬¸ì œ 1ë§Œ í…ŒìŠ¤íŠ¸
    scenario = scenarios[0]  # Phase 4 - í•œêµ­ ì „ì²´ ì‚¬ì—…ì ìˆ˜
    
    print(f"ğŸ“‹ ë¬¸ì œ: {scenario['name']}")
    print(f"   ì •ë‹µ: {scenario['expected_value']:,} {scenario['expected_unit']}")
    print()
    
    model_name = 'gpt-5-pro'
    reasoning_effort = 'high'
    
    print(f"ğŸ”„ ëª¨ë¸: {model_name} (effort={reasoning_effort})")
    print()
    
    try:
        start = time.time()
        
        # ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ API íŒŒë¼ë¯¸í„° ìƒì„±
        api_type, api_params = build_api_params(
            model_name=model_name,
            prompt=scenario['prompt'],
            reasoning_effort=reasoning_effort
        )
        
        # ëª¨ë¸ ì„¤ì • ì¶œë ¥
        config = get_model_config(model_name)
        print(f"ğŸ”§ {model_name} API ì„¤ì •:")
        print(f"  - API íƒ€ì…: {config['api_type']}")
        print(f"  - reasoning ì§€ì›: {config['reasoning_effort_support']}")
        print(f"  - reasoning.effort: {api_params.get('reasoning', {}).get('effort', 'N/A')}")
        print(f"  - max_output_tokens: {api_params.get('max_output_tokens', 'N/A')} (ìµœì í™”: 272K â†’ 48K)")
        print()
        
        print("â³ API í˜¸ì¶œ ì¤‘...")
        
        # API í˜¸ì¶œ
        response = call_model_api(client, api_type, api_params)
        
        elapsed = time.time() - start
        
        # ì‘ë‹µ ì¶”ì¶œ
        if api_type == 'responses':
            # response.outputì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²˜ë¦¬
            if isinstance(response.output, list):
                # ë§ˆì§€ë§‰ messageì˜ contentì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for item in response.output:
                    if hasattr(item, 'role') and item.role == 'assistant':
                        if hasattr(item, 'content') and isinstance(item.content, list):
                            for content_item in item.content:
                                if hasattr(content_item, 'text'):
                                    output = content_item.text
                                    break
            else:
                output = response.output
        else:
            output = response.choices[0].message.content
        
        print(f"âœ… ì‘ë‹µ ì™„ë£Œ! (ì†Œìš”: {elapsed:.1f}ì´ˆ)")
        print()
        
        # JSON íŒŒì‹±
        import json as json_lib
        import re
        
        # JSON ì¶”ì¶œ ì‹œë„
        try:
            # ì½”ë“œ ë¸”ë¡ ì œê±°
            clean_output = re.sub(r'```json\s*', '', output)
            clean_output = re.sub(r'```\s*$', '', clean_output)
            clean_output = clean_output.strip()
            
            parsed = json_lib.loads(clean_output)
        except:
            print("âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, ì‘ë‹µ ì¼ë¶€ ì¶œë ¥:")
            print(output[:500])
            print()
            raise
        
        # í‰ê°€
        evaluation = evaluate_fermi_response(
            model_name=model_name,
            response=parsed,
            expected_value=scenario['expected_value'],
            problem_id=scenario['id']
        )
        
        # ê²°ê³¼ ì¶œë ¥
        print("=" * 100)
        print("ğŸ“Š í‰ê°€ ê²°ê³¼")
        print("=" * 100)
        print()
        print(f"ì¶”ì •ê°’: {evaluation['value']:,} {evaluation['unit']}")
        print(f"ì •ë‹µ: {scenario['expected_value']:,} {scenario['expected_unit']}")
        print()
        print(f"ì´ì : {evaluation['total_score']:.1f}/100")
        print(f"  - ì •í™•ë„: {evaluation['accuracy']['score']}/25")
        print(f"  - ì—°ê²°ì„±: {evaluation['calculation_connectivity']['score']:.1f}/50")
        print(f"  - ë¶„í•´: {evaluation['decomposition']['score']}/15")
        print(f"  - ê°œë…: {evaluation['conceptual_coherence']['score']}/15")
        print(f"  - ë…¼ë¦¬: {evaluation['logic']['score']}/10")
        print()
        print(f"ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'model': model_name,
            'problem': scenario['name'],
            'problem_id': scenario['id'],
            'expected_value': scenario['expected_value'],
            'reasoning_effort': reasoning_effort,
            'max_output_tokens': api_params.get('max_output_tokens'),
            'elapsed': elapsed,
            'response': output,
            **evaluation
        }
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'gpt5_pro_problem1_retest_{timestamp}.json'
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
        
        return result
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == '__main__':
    test_gpt5_pro_problem1()

