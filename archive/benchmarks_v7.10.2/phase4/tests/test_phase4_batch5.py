#!/usr/bin/env python3
"""
Phase 4 Batch 5: Low Effort í…ŒìŠ¤íŠ¸ (í•„ìš” ëª¨ë¸ë§Œ)
- gpt-5.1 (reasoning_effort='low')
- o4-mini (reasoning_effort='low')

âš ï¸ ì œì™¸ëœ ëª¨ë¸ (effort ì¡°ì • ë¶ˆê°€):
- gpt-5-pro (high ê³ ì •)
- o1-pro (high ê³ ì •)
- o1-pro-2025-03-19 (high ê³ ì •)

ëª¨ë¸ë³„ API íŒŒë¼ë¯¸í„° ëª…ì‹œì  ê´€ë¦¬ + ê°œë…ì  ì¼ê´€ì„± í‰ê°€
"""

import os
import json
import time
import re
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# phase4_common ëª¨ë“ˆì—ì„œ ê³µí†µ í•¨ìˆ˜ import
from phase4_common import (
    get_model_config,
    build_api_params,
    call_model_api,
    get_phase4_scenarios,
    evaluate_fermi_response
)

load_dotenv()


def test_model_responses_api(client, model_name, scenario, reasoning_effort='low'):
    """Responses APIë¡œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ - phase4_common ì‚¬ìš©"""

    try:
        start = time.time()

        # phase4_commonì—ì„œ ëª¨ë¸ë³„ API íŒŒë¼ë¯¸í„° ìƒì„±
        api_type, api_params = build_api_params(
            model_name=model_name,
            prompt=scenario['prompt'],
            reasoning_effort=reasoning_effort
        )

        # ëª¨ë¸ ì„¤ì • ì¶œë ¥ (ë””ë²„ê¹…)
        config = get_model_config(model_name)
        print(f"\nğŸ”§ {model_name} API ì„¤ì •:")
        print(f"  - API íƒ€ì…: {api_type}")
        print(f"  - reasoning ì§€ì›: {config['reasoning_effort_support']}")
        if config['reasoning_effort_support']:
            actual_effort = api_params.get('reasoning', {}).get('effort', 'N/A')
            print(f"  - reasoning.effort: {actual_effort} (ìš”ì²­: {reasoning_effort})")
        print(f"  - max_output_tokens: {api_params['max_output_tokens']}")

        # API í˜¸ì¶œ
        response = call_model_api(client, api_type, api_params)

        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ (API íƒ€ì…ë³„)
        if api_type == 'responses':
            content = getattr(response, 'output_text', None) or getattr(response, 'output', str(response))
        else:  # 'chat'
            content = response.choices[0].message.content

        elapsed = time.time() - start

        # JSON íŒŒì‹±
        try:
            if '```json' in content:
                json_start = content.find('```json') + 7
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()
            elif '```' in content:
                json_start = content.find('```') + 3
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()

            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)

            parsed = json.loads(content)
        except Exception as e:
            parsed = {'parse_error': str(e), 'raw': content[:200]}

        return {
            'success': True,
            'elapsed': round(elapsed, 2),
            'response': parsed
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


def save_partial_results(all_results, scenarios, test_config, error_info=None):
    """ë¶€ë¶„ ê²°ê³¼ ì €ì¥ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë˜ëŠ” ì •ìƒ ì™„ë£Œ ì‹œ)"""
    from collections import defaultdict

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    by_model = defaultdict(list)
    for r in all_results:
        by_model[r['model']].append(r)

    model_averages = []
    for model_name, results in by_model.items():
        avg = {
            'model': model_name,
            'avg_total': sum(r['total_score'] for r in results) / len(results),
            'avg_accuracy': sum(r['accuracy']['score'] for r in results) / len(results),
            'avg_connectivity': sum(r['calculation_connectivity']['score'] for r in results) / len(results),
            'avg_decomp': sum(r['decomposition']['score'] for r in results) / len(results),
            'avg_logic': sum(r['logic']['score'] for r in results) / len(results),
            'count': len(results)
        }
        model_averages.append(avg)

    model_averages.sort(key=lambda x: x['avg_total'], reverse=True)

    status = 'PARTIAL' if error_info else 'COMPLETE'
    output_file = f"phase4_batch5_low_{status.lower()}_{timestamp}.json"

    data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'batch': 'Batch 5 - Low Effort Test',
            'reasoning_effort': 'low',
            'status': status,
            'models_tested': len(by_model),
            'total_models': len(test_config),
            'problems_tested': len(set(r['problem_id'] for r in all_results)),
            'total_problems': len(scenarios),
            'completed_tests': len(all_results)
        },
        'problems': {
            s['id']: {
                'name': s['name'],
                'expected_value': s['expected_value'],
                'expected_unit': s['expected_unit']
            } for s in scenarios
        },
        'results': all_results,
        'summary': model_averages
    }

    if error_info:
        data['error'] = error_info

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    return output_file, model_averages


def run_batch5_test():
    """Batch 5 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - Low Effort (í•„ìš” ëª¨ë¸ë§Œ)"""
    print("=" * 120)
    print("Phase 4 Batch 5 - Low Effort í…ŒìŠ¤íŠ¸")
    print("=" * 120)
    print("âœ… í…ŒìŠ¤íŠ¸ ëª¨ë¸: gpt-5.1, o4-mini (reasoning_effort='low')")
    print("âŒ ì œì™¸ ëª¨ë¸: gpt-5-pro, o1-pro ê³„ì—´ (effort='high' ê³ ì •, Batch 3ê³¼ ë™ì¼)")
    print()

    client = OpenAI()

    # Batch 5: effort='low' í…ŒìŠ¤íŠ¸
    test_config = [
        {'model': 'gpt-5.1', 'effort': 'low', 'tier': 'Batch 5'},
        {'model': 'o4-mini', 'effort': 'low', 'tier': 'Batch 5'},
    ]

    all_results = []

    error_occurred = False
    error_info = None

    try:
        for model_idx, config in enumerate(test_config, 1):
            model_name = config['model']
            effort = config['effort']
            tier = config['tier']
            
            # ëª¨ë¸ë³„ë¡œ scenarios ìƒì„± (pro ëª¨ë¸ì€ Fast Mode ì¶”ê°€)
            scenarios = get_phase4_scenarios(model_name)
            
            print(f"\n{'='*120}")
            print(f"ğŸ¤– ëª¨ë¸ {model_idx}/{len(test_config)}: {model_name} ({tier}, effort={effort})")
            print(f"{'='*120}\n")

            for scenario_idx, scenario in enumerate(scenarios, 1):
                print(f"\nğŸ“‹ ë¬¸ì œ {scenario_idx}/{len(scenarios)}: {scenario['name']}")
                print(f"   ì •ë‹µ: {scenario['expected_value']:,} {scenario['expected_unit']}\n")

                try:
                    test_result = test_model_responses_api(client, model_name, scenario, effort)

                    if test_result['success']:
                        eval_result = evaluate_fermi_response(
                            model_name=f"{model_name}",
                            response=test_result['response'],
                            expected_value=scenario['expected_value'],
                            problem_id=scenario['id']  # ê°œë…ì  ì¼ê´€ì„± í‰ê°€ë¥¼ ìœ„í•´ ì¶”ê°€
                        )

                        eval_result['elapsed'] = test_result['elapsed']
                        eval_result['response'] = test_result['response']
                        eval_result['problem'] = scenario['name']
                        eval_result['problem_id'] = scenario['id']
                        eval_result['tier'] = tier
                        eval_result['reasoning_effort'] = effort

                        problem_results.append(eval_result)
                        all_results.append(eval_result)

                        # ê°œë…ì  ì¼ê´€ì„± ì ìˆ˜ ì¶”ê°€ ì¶œë ¥
                        conceptual_score = eval_result.get('conceptual_coherence', {}).get('score', 0)
                        print(f"   âœ… {eval_result['value']:,} {eval_result['unit']} | ì´ì : {eval_result['total_score']}/100")
                        print(f"      ì—°ê²°ì„±: {eval_result['calculation_connectivity']['score']}/50 | ê°œë…: {conceptual_score}/15")
                    else:
                        error_msg = test_result['error']
                        print(f"   âŒ API ì˜¤ë¥˜: {error_msg[:100]}")

                        # ì¹˜ëª…ì  ì˜¤ë¥˜ (API í‚¤, ê¶Œí•œ ë“±)ì¸ ê²½ìš° ì¦‰ì‹œ ì¤‘ë‹¨
                        if any(key in error_msg.lower() for key in ['api_key', 'authentication', 'unauthorized', 'forbidden']):
                            raise Exception(f"ì¹˜ëª…ì  API ì˜¤ë¥˜: {error_msg}")

                        # ëª¨ë¸ ì§€ì› ì•ˆ ë¨ ë“±ì€ ê³„ì† ì§„í–‰
                        print(f"   âš ï¸  ëª¨ë¸ '{model_name}' ê±´ë„ˆëœ€")

                except KeyboardInterrupt:
                    print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨ (Ctrl+C)")
                    error_occurred = True
                    error_info = {
                        'type': 'USER_INTERRUPT',
                        'message': 'ì‚¬ìš©ìê°€ í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤',
                        'failed_at': {
                            'problem': scenario['name'],
                            'problem_id': scenario['id'],
                            'model': model_name,
                            'scenario_progress': f"{scenario_idx}/{len(scenarios)}",
                            'model_progress': f"{model_idx}/{len(test_config)}"
                        }
                    }
                    raise

                except Exception as e:
                    print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ!")
                    print(f"   ì˜¤ë¥˜ ë‚´ìš©: {str(e)}")
                    error_occurred = True
                    error_info = {
                        'type': 'RUNTIME_ERROR',
                        'message': str(e),
                        'failed_at': {
                            'problem': scenario['name'],
                            'problem_id': scenario['id'],
                            'model': model_name,
                            'scenario_progress': f"{scenario_idx}/{len(scenarios)}",
                            'model_progress': f"{model_idx}/{len(test_config)}"
                        }
                    }
                    raise

                time.sleep(2)

            # ë¬¸ì œë³„ ìˆœìœ„
            if problem_results:
                print(f"\nğŸ“Š {scenario['name']} ìˆœìœ„:\n")
                problem_results.sort(key=lambda x: x['total_score'], reverse=True)

                for i, r in enumerate(problem_results, 1):
                    marker = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
                    print(f"{marker}{i}. {r['model']:<35} {r['total_score']:>3}/100 (ì •í™•ë„: {r['accuracy']['score']}/25, ì—°ê²°ì„±: {r['calculation_connectivity']['score']}/50)")

                print()

    except (KeyboardInterrupt, Exception) as e:
        error_occurred = True
        print("\n" + "=" * 120)
        print("âš ï¸  í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨ - ë¶€ë¶„ ê²°ê³¼ ì €ì¥ ì¤‘")
        print("=" * 120)

    # ì¢…í•© ê²°ê³¼ ë° ì €ì¥
    print("\n" + "=" * 120)
    if error_occurred:
        print(f"âš ï¸  Batch 5 ë¶€ë¶„ ê²°ê³¼ ({len(all_results)}ê°œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ)")
    else:
        print("ğŸ† Batch 5 ìµœì¢… ìˆœìœ„ (3ê°œ ë¬¸ì œ í‰ê· )")
    print("=" * 120)
    print()

    if all_results:
        # ê²°ê³¼ ì €ì¥
        output_file, model_averages = save_partial_results(all_results, scenarios, test_config, error_info)

        # ìˆœìœ„ ì¶œë ¥
        print(f"{'ìˆœìœ„':<4} | {'ëª¨ë¸':<35} | {'í‰ê· ':<10} | {'ì •í™•ë„':<10} | {'ì—°ê²°ì„±':<10} | {'ë¶„í•´':<10} | {'ë…¼ë¦¬':<8}")
        print("-" * 120)

        for i, m in enumerate(model_averages, 1):
            marker = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            print(f"{marker}{i:<3} | {m['model']:<35} | {m['avg_total']:>8.1f}/100 | {m['avg_accuracy']:>8.1f}/25 | {m['avg_connectivity']:>8.1f}/50 | {m['avg_decomp']:>8.1f}/15 | {m['avg_logic']:>6.1f}/10")

        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    else:
        print("âš ï¸  ì™„ë£Œëœ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ì˜¤ë¥˜ ìƒì„¸ ì¶œë ¥
    if error_occurred and error_info:
        print("\n" + "=" * 120)
        print("âŒ ì˜¤ë¥˜ ìƒì„¸ ì •ë³´")
        print("=" * 120)
        print(f"\nì˜¤ë¥˜ ìœ í˜•: {error_info['type']}")
        print(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {error_info['message']}")
        print(f"\nì‹¤íŒ¨ ìœ„ì¹˜:")
        print(f"  - ë¬¸ì œ: {error_info['failed_at']['problem']}")
        print(f"  - ëª¨ë¸: {error_info['failed_at']['model']}")
        print(f"  - ì§„í–‰ ìƒí™©: ë¬¸ì œ {error_info['failed_at']['scenario_progress']}, ëª¨ë¸ {error_info['failed_at']['model_progress']}")
        print(f"\nì™„ë£Œëœ í…ŒìŠ¤íŠ¸: {len(all_results)}ê°œ")
        print()

    if error_occurred:
        print("\nâš ï¸  í…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶€ë¶„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print("\nğŸ‰ Batch 5 í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    return not error_occurred


if __name__ == "__main__":
    run_batch5_test()

