#!/usr/bin/env python3
"""
GPT-5 reasoning_effort ì˜µì…˜ í…ŒìŠ¤íŠ¸
ë™ì¼ ëª¨ë¸(gpt-5, gpt-5.1)ì— ë‹¤ì–‘í•œ reasoning_effort ì ìš©
"""

import sys
import os
import time

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark


def test_reasoning_effort_options():
    """reasoning_effort ì˜µì…˜ë³„ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("GPT-5 reasoning_effort ì˜µì…˜ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„±: (ëª¨ë¸, reasoning_effort)
    test_configs = [
        # GPT-5 ì‹œë¦¬ì¦ˆ
        ('gpt-5', 'minimal'),
        ('gpt-5', 'low'),
        ('gpt-5', 'medium'),
        ('gpt-5', 'high'),
        ('gpt-5.1', 'minimal'),
        ('gpt-5.1', 'low'),
        ('gpt-5.1', 'medium'),
        ('gpt-5.1', 'high'),
    ]
    
    # Responses API ëª¨ë¸ë¡œ ë“±ë¡
    benchmark.responses_api_models.extend(['gpt-5', 'gpt-5.1'])
    
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ êµ¬ì„±: 8ê°œ")
    print("   ëª¨ë¸: gpt-5, gpt-5.1")
    print("   reasoning_effort: minimal, low, medium, high")
    print()
    
    scenarios = benchmark.get_test_scenarios()[:1]  # Phase 0ë§Œ
    results = []
    
    for scenario in scenarios:
        print(f"ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print()
        
        for model, effort in test_configs:
            config_name = f"{model} (effort={effort})"
            print(f"í…ŒìŠ¤íŠ¸: {config_name}")
            
            try:
                # reasoning_effortë¥¼ ì‹œë‚˜ë¦¬ì˜¤ì— ì„ì‹œë¡œ ì¶”ê°€
                test_scenario = scenario.copy()
                test_scenario['reasoning_effort'] = effort
                
                start = time.time()
                
                # Responses APIëŠ” ë‹¤ë¥´ê²Œ í˜¸ì¶œí•´ì•¼ í•¨
                # ì¼ë‹¨ ìˆ˜ë™ìœ¼ë¡œ API í˜¸ì¶œ
                from openai import OpenAI
                client = OpenAI()
                
                input_text = scenario['prompt'] + "\n\nâš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ìˆœìˆ˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
                
                # Responses API í˜¸ì¶œ
                response = client.responses.create(
                    model=model,
                    input=input_text,
                    reasoning_effort=effort
                )
                
                elapsed = time.time() - start
                
                # ì‘ë‹µ íŒŒì‹±
                if hasattr(response, 'output_text'):
                    content = response.output_text
                elif hasattr(response, 'output'):
                    content = response.output
                else:
                    content = str(response)
                
                # JSON ì¶”ì¶œ
                import json
                import re
                
                try:
                    if '```json' in content:
                        json_start = content.find('```json') + 7
                        json_end = content.find('```', json_start)
                        content = content[json_start:json_end].strip()
                    elif '```' in content:
                        json_start = content.find('```') + 3
                        json_end = content.find('```', json_start)
                        content = content[json_start:json_end].strip()
                    
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        content = json_match.group(0)
                    
                    parsed = json.loads(content)
                except:
                    parsed = {'raw': content, 'parse_error': True}
                
                # í† í° ë° ë¹„ìš©
                tokens = {
                    'input': getattr(response, 'input_tokens', 0),
                    'output': getattr(response, 'output_tokens', 0),
                }
                
                if tokens['input'] == 0:
                    tokens['input'] = len(input_text) // 4
                    tokens['output'] = len(content) // 4
                
                cost = benchmark._calculate_cost(model, tokens['input'], tokens['output'])
                quality = benchmark._evaluate_quality(parsed, scenario.get('expected', {}), scenario['phase'])
                
                result = {
                    'model': model,
                    'reasoning_effort': effort,
                    'config_name': config_name,
                    'cost': cost,
                    'elapsed_seconds': elapsed,
                    'quality_score': quality,
                    'response': parsed,
                    'success': True
                }
                
                results.append(result)
                
                print(f"   âœ… ì„±ê³µ!")
                print(f"      ë¹„ìš©: ${cost:.6f}")
                print(f"      ì‹œê°„: {elapsed:.2f}ì´ˆ")
                print(f"      í’ˆì§ˆ: {quality['total_score']}/100")
                
                time.sleep(2)
            
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {str(e)[:80]}")
                results.append({
                    'model': model,
                    'reasoning_effort': effort,
                    'config_name': config_name,
                    'error': str(e),
                    'success': False
                })
                time.sleep(2)
            
            print()
    
    # ê²°ê³¼ ë¶„ì„
    print("=" * 80)
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„")
    print("=" * 80)
    print()
    
    success = [r for r in results if r.get('success', False)]
    
    if success:
        print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
        print(f"ì„±ê³µ: {len(success)}ê°œ")
        print()
        
        # ëª¨ë¸ë³„ ê·¸ë£¹í™”
        from collections import defaultdict
        
        by_model = defaultdict(list)
        for r in success:
            by_model[r['model']].append(r)
        
        for model in ['gpt-5', 'gpt-5.1']:
            if model not in by_model:
                continue
            
            print(f"{'='*80}")
            print(f"{model} - reasoning_effortë³„ ë¹„êµ")
            print(f"{'='*80}")
            print()
            
            model_results = by_model[model]
            
            print(f"{'effort':<10} | {'ë¹„ìš©':<12} | {'ì‹œê°„':<10} | {'í’ˆì§ˆ':<8} | {'ê°€ì„±ë¹„':<10}")
            print("-" * 70)
            
            for r in sorted(model_results, key=lambda x: ['minimal', 'low', 'medium', 'high'].index(x['reasoning_effort'])):
                effort = r['reasoning_effort']
                cost = r['cost']
                time_val = r['elapsed_seconds']
                quality = r['quality_score']['total_score']
                efficiency = quality / (cost * 1000) if cost > 0 else 0
                
                print(f"{effort:<10} | ${cost:<11.6f} | {time_val:<9.2f}ì´ˆ | {quality:>6}/100 | {efficiency:>8.1f}")
            
            print()
            
            # ìµœì  ì˜µì…˜ ì¶”ì²œ
            best_efficiency = max(model_results, key=lambda r: r['quality_score']['total_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0)
            best_quality = max(model_results, key=lambda r: r['quality_score']['total_score'])
            fastest = min(model_results, key=lambda r: r['elapsed_seconds'])
            
            print(f"ğŸ’¡ {model} ê¶Œì¥ ì˜µì…˜:")
            print(f"   - ìµœê³  ê°€ì„±ë¹„: {best_efficiency['reasoning_effort']} (ê°€ì„±ë¹„ {best_efficiency['quality_score']['total_score'] / (best_efficiency['cost'] * 1000):.1f})")
            print(f"   - ìµœê³  í’ˆì§ˆ: {best_quality['reasoning_effort']} ({best_quality['quality_score']['total_score']}/100)")
            print(f"   - ê°€ì¥ ë¹ ë¦„: {fastest['reasoning_effort']} ({fastest['elapsed_seconds']:.2f}ì´ˆ)")
            print()
    
    # ì €ì¥
    import json
    from datetime import datetime
    
    output_file = f"benchmark_reasoning_effort_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'reasoning_effort_comparison'
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
    print()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_reasoning_effort_options()


