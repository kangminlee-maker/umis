#!/usr/bin/env python3
"""
GPT-5 reasoning effort ì˜µì…˜ í…ŒìŠ¤íŠ¸ (ì˜¬ë°”ë¥¸ ë°©ë²•)
Responses API: reasoning={"effort": "..."} í˜•íƒœ ì‚¬ìš©
Chat API: reasoning_effort="..." í˜•íƒœ ì‚¬ìš©
"""

import sys
import os
import time
import json
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()


def test_responses_api_reasoning_efforts():
    """Responses API - reasoning effort ì˜µì…˜ í…ŒìŠ¤íŠ¸"""
    print("=" * 90)
    print("Responses API - reasoning effort ì˜µì…˜ í…ŒìŠ¤íŠ¸")
    print("=" * 90)
    print()
    
    client = OpenAI()
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„±: (ëª¨ë¸, effort, verbosity)
    test_configs = [
        # GPT-5.1 ì „ì²´ ì¡°í•©
        ('gpt-5.1', 'none', 'low'),
        ('gpt-5.1', 'none', 'medium'),
        ('gpt-5.1', 'low', 'low'),
        ('gpt-5.1', 'low', 'medium'),
        ('gpt-5.1', 'medium', 'low'),
        ('gpt-5.1', 'medium', 'medium'),
        ('gpt-5.1', 'high', 'low'),
        ('gpt-5.1', 'high', 'medium'),
        
        # GPT-5 ë¹„êµ
        ('gpt-5', 'low', 'low'),
        ('gpt-5', 'medium', 'low'),
        
        # GPT-5-mini, nano
        ('gpt-5-mini', 'none', 'low'),
        ('gpt-5-mini', 'low', 'low'),
        ('gpt-5-nano', 'none', 'low'),
        ('gpt-5-nano', 'low', 'low'),
    ]
    
    prompt_text = '''ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS ARPU" ê°’ì„ ì •í™•íˆ ì°¾ì•„ ì¶”ì¶œí•˜ì„¸ìš”.

ì£¼ì–´ì§„ ë°ì´í„°:
- í•œêµ­ B2B SaaS ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ARPU: 70,000ì›

ìš”êµ¬ì‚¬í•­: ìˆœìˆ˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
{"value": ìˆ«ì, "unit": "ì›", "confidence": 1.0}'''
    
    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ êµ¬ì„±: {len(test_configs)}ê°œ")
    print("   ëª¨ë¸: gpt-5.1, gpt-5, gpt-5-mini, gpt-5-nano")
    print("   reasoning effort: none, low, medium, high")
    print("   verbosity: low, medium")
    print()
    
    results = []
    
    for model, effort, verbosity in test_configs:
        config_name = f"{model} (effort={effort}, verb={verbosity})"
        print(f"í…ŒìŠ¤íŠ¸: {config_name}")
        
        try:
            start = time.time()
            
            # Responses API í˜¸ì¶œ (ì˜¬ë°”ë¥¸ í˜•íƒœ)
            response = client.responses.create(
                model=model,
                input=prompt_text,
                reasoning={"effort": effort},  # â† Dict í˜•íƒœ!
                text={"verbosity": verbosity}  # â† Dict í˜•íƒœ!
            )
            
            elapsed = time.time() - start
            
            # ì‘ë‹µ ì¶”ì¶œ
            if hasattr(response, 'output_text'):
                content = response.output_text
            elif hasattr(response, 'output'):
                content = response.output
            else:
                content = str(response)
            
            # JSON íŒŒì‹±
            import re
            
            try:
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
                
                parsed = json.loads(content)
            except:
                parsed = {'raw': content[:100], 'parse_error': True}
            
            # í† í° ì •ë³´
            tokens = {
                'input': getattr(response, 'input_tokens', 0),
                'output': getattr(response, 'output_tokens', 0),
            }
            
            # í† í°ì´ 0ì´ë©´ ì¶”ì •
            if tokens['input'] == 0:
                tokens['input'] = len(prompt_text) // 4
                tokens['output'] = len(content) // 4
            
            tokens['total'] = tokens['input'] + tokens['output']
            
            # ë¹„ìš© ê³„ì‚°
            pricing = {
                'gpt-5': {'input': 1.25, 'output': 10.00},
                'gpt-5.1': {'input': 1.25, 'output': 10.00},
                'gpt-5-mini': {'input': 0.25, 'output': 2.00},
                'gpt-5-nano': {'input': 0.05, 'output': 0.40}
            }
            
            rates = pricing.get(model, {'input': 0, 'output': 0})
            cost = (tokens['input'] / 1_000_000 * rates['input'] + 
                   tokens['output'] / 1_000_000 * rates['output'])
            
            # í’ˆì§ˆ í‰ê°€
            has_value = 'value' in parsed
            correct_value = parsed.get('value') == 200000 if has_value else False
            has_confidence = 'confidence' in parsed
            correct_confidence = parsed.get('confidence') == 1.0 if has_confidence else False
            json_valid = 'parse_error' not in parsed
            
            quality = 0
            if json_valid: quality += 25
            if has_value: quality += 25
            if has_confidence: quality += 20
            if correct_value: quality += 20
            if correct_confidence: quality += 10
            
            result = {
                'model': model,
                'reasoning_effort': effort,
                'verbosity': verbosity,
                'config_name': config_name,
                'cost': cost,
                'elapsed_seconds': elapsed,
                'quality_score': quality,
                'tokens': tokens,
                'response': parsed,
                'success': True
            }
            
            results.append(result)
            
            print(f"   âœ… ì„±ê³µ!")
            print(f"      ë¹„ìš©: ${cost:.6f} | ì‹œê°„: {elapsed:.2f}ì´ˆ | í’ˆì§ˆ: {quality}/100")
            
            time.sleep(2)
        
        except Exception as e:
            error_msg = str(e)
            print(f"   âŒ ì˜¤ë¥˜: {error_msg[:100]}")
            
            results.append({
                'model': model,
                'reasoning_effort': effort,
                'verbosity': verbosity,
                'config_name': config_name,
                'error': error_msg,
                'success': False
            })
            
            time.sleep(2)
        
        print()
    
    # ê²°ê³¼ ë¶„ì„
    analyze_results(results)
    
    # ì €ì¥
    output_file = f"benchmark_responses_reasoning_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'responses_api_reasoning_effort',
                'api': 'responses'
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
    print()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def analyze_results(results):
    """ê²°ê³¼ ë¶„ì„"""
    print("=" * 90)
    print("ğŸ“Š ê²°ê³¼ ë¶„ì„")
    print("=" * 90)
    print()
    
    success = [r for r in results if r.get('success', False)]
    failed = [r for r in results if not r.get('success', False)]
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(success)}ê°œ")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
    print()
    
    if not success:
        print("âŒ ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ëª¨ë¸ë³„ ê·¸ë£¹í™”
    from collections import defaultdict
    
    by_model = defaultdict(list)
    for r in success:
        by_model[r['model']].append(r)
    
    # ê° ëª¨ë¸ë³„ ë¶„ì„
    for model in sorted(by_model.keys()):
        model_results = by_model[model]
        
        print(f"{'='*90}")
        print(f"{model} - reasoning effort & verbosity ë¹„êµ")
        print(f"{'='*90}")
        print()
        
        print(f"{'effort':<8} | {'verb':<8} | {'ë¹„ìš©':<12} | {'ì‹œê°„':<10} | {'í’ˆì§ˆ':<8} | {'ê°€ì„±ë¹„':<10}")
        print("-" * 80)
        
        for r in sorted(model_results, 
                       key=lambda x: (['none', 'low', 'medium', 'high'].index(x['reasoning_effort']), 
                                     ['low', 'medium', 'high'].index(x['verbosity']))):
            effort = r['reasoning_effort']
            verb = r['verbosity']
            cost = r['cost']
            time_val = r['elapsed_seconds']
            quality = r['quality_score']
            efficiency = quality / (cost * 1000) if cost > 0 else 0
            
            marker = "â­" if quality == 100 else "  "
            
            print(f"{marker}{effort:<8} | {verb:<8} | ${cost:<11.6f} | {time_val:<9.2f}ì´ˆ | {quality:>6}/100 | {efficiency:>8.1f}")
        
        print()
        
        # ìµœì  ì˜µì…˜ ì°¾ê¸°
        perfect = [r for r in model_results if r['quality_score'] == 100]
        
        if perfect:
            best = min(perfect, key=lambda r: r['cost'])
            print(f"ğŸ’¡ {model} ìµœì  ì˜µì…˜:")
            print(f"   â­ effort={best['reasoning_effort']}, verbosity={best['verbosity']}")
            print(f"      ë¹„ìš©: ${best['cost']:.6f} | ì‹œê°„: {best['elapsed_seconds']:.2f}ì´ˆ | í’ˆì§ˆ: 100/100")
        else:
            best = max(model_results, key=lambda r: r['quality_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0)
            print(f"ğŸ’¡ {model} ìµœê³  ê°€ì„±ë¹„:")
            print(f"   â€¢ effort={best['reasoning_effort']}, verbosity={best['verbosity']}")
            print(f"      ë¹„ìš©: ${best['cost']:.6f} | ì‹œê°„: {best['elapsed_seconds']:.2f}ì´ˆ | í’ˆì§ˆ: {best['quality_score']}/100")
        
        print()


if __name__ == "__main__":
    test_responses_api_reasoning_efforts()

