#!/usr/bin/env python3
"""
Fermi ì¶”ì • ì¢…í•© í‰ê°€ - AI ê¸°ì¤€ì„  ë¹„êµ
Phase 4ì—ì„œ 80ì  ì´ìƒ ëª¨ë¸ ëŒ€ìƒ, 3ê°œ ì‹¤ì œ ë°ì´í„° ë¬¸ì œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from datetime import datetime
import math

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()


# =====================================
# AI ê¸°ì¤€ì„  Fermi ë¶„í•´ (ë¬¸ì„œì—ì„œ)
# =====================================

FERMI_PROBLEMS = {
    'korean_businesses': {
        'name': 'í•œêµ­ ì „ì²´ ì‚¬ì—…ì ìˆ˜',
        'ground_truth': 7837000,
        'unit': 'ê°œ',
        'ai_baseline': {
            'estimate': 6000000,
            'error_rate': 0.23,
            'decomposition': [
                {
                    'step': 'ê²½ì œí™œë™ì¸êµ¬',
                    'value': 20300000,
                    'assumption': 'ì¸êµ¬ 5200ë§Œ Ã— 60% Ã— 65% = 2030ë§Œëª…'
                },
                {
                    'step': 'ìì˜ì—…ì ë¹„ìœ¨',
                    'value': 4060000,
                    'assumption': 'ê²½ì œí™œë™ì¸êµ¬ì˜ 20% = 406ë§Œëª…'
                },
                {
                    'step': 'ë²•ì¸ ì‚¬ì—…ì',
                    'value': 970000,
                    'assumption': 'ê·¼ë¡œì 1624ë§Œ / 20ëª… Ã— 1.2 = 97ë§Œê°œ'
                },
                {
                    'step': 'ì´ ì‚¬ì—…ì',
                    'value': 6000000,
                    'assumption': '(406ë§Œ + 97ë§Œ) Ã— 1.2 = 600ë§Œê°œ'
                }
            ],
            'strengths': [
                'ëª…í™•í•œ 4ë‹¨ê³„ ë¶„í•´',
                'ì¸êµ¬ ê¸°ë°˜ bottom-up',
                'í•©ë¦¬ì ì¸ ê°€ì •'
            ],
            'weaknesses': [
                'ì‹¤ì œë³´ë‹¤ ë‚®ê²Œ ì¶”ì • (23% ì˜¤ì°¨)',
                'ë‹¤ì¤‘ ì‚¬ì—…ìë“±ë¡ ë¯¸ë°˜ì˜'
            ]
        },
        'prompt': '''í•œêµ­ ì „ì²´ ì‚¬ì—…ì ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

íŒíŠ¸:
- í•œêµ­ ì¸êµ¬, ê²½ì œí™œë™ì¸êµ¬ ê³ ë ¤
- ìì˜ì—…ì, ë²•ì¸ ì‚¬ì—…ì êµ¬ë¶„
- ë‹¤ì¤‘ ì‚¬ì—…ìë“±ë¡ ê°€ëŠ¥ì„± ê³ ë ¤

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "value": <ì¶”ì •ê°’>,
    "unit": "ê°œ",
    "confidence": <0.3-0.7>,
    "method": "ì ‘ê·¼ ë°©ë²•",
    "decomposition": [
        {"step": "ë‹¨ê³„ëª…", "value": <ê°’>, "assumption": "ê°€ì •", "reasoning": "ê·¼ê±°"}
    ],
    "reasoning": "ì „ì²´ ì¶”ì • ë…¼ë¦¬"
}'''
    },
    
    'seoul_population': {
        'name': 'ì„œìš¸ì‹œ ì¸êµ¬',
        'ground_truth': 9668465,
        'unit': 'ëª…',
        'ai_baseline': {
            'estimate': 8700000,
            'estimate_range': [8000000, 9400000],
            'error_rate': 0.10,
            'decomposition': [
                {
                    'step': 'í•œêµ­ ì „ì²´ ì¸êµ¬',
                    'value': 52000000,
                    'assumption': 'í•œêµ­ ì¸êµ¬ 5200ë§Œëª…'
                },
                {
                    'step': 'ìˆ˜ë„ê¶Œ ë¹„ì¤‘',
                    'value': 26000000,
                    'assumption': 'ìˆ˜ë„ê¶Œ ì§‘ì¤‘ë„ 50% = 2600ë§Œëª…'
                },
                {
                    'step': 'ì„œìš¸ ë¹„ì¤‘',
                    'value': 7440000,
                    'assumption': 'ì„œìš¸:ê²½ê¸°:ì¸ì²œ = 1:2:0.5, ì„œìš¸ 28.6%'
                },
                {
                    'step': 'ë©´ì  ê¸°ë°˜ ê²€ì¦',
                    'value': 9360000,
                    'assumption': 'ì„œìš¸ 605kmÂ² Ã— ë°€ì§‘ë„ 300ë°°'
                }
            ],
            'strengths': [
                'ë‘ ê°€ì§€ ë°©ë²• ì œì‹œ (ë¹„ìœ¨ + ë©´ì )',
                'ë²”ìœ„ ì¶”ì •ìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± í‘œí˜„',
                'í•©ë¦¬ì  ì¶”ì • (ì˜¤ì°¨ 10%)'
            ],
            'weaknesses': [
                'ìˆ˜ë„ê¶Œ ë¹„ìœ¨ ê°€ì • ê·¼ê±° ë¶€ì¡±'
            ]
        },
        'prompt': '''ì„œìš¸ì‹œ ì¸êµ¬ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

íŒíŠ¸:
- í•œêµ­ ì „ì²´ ì¸êµ¬ ëŒ€ë¹„ ì„œìš¸ ë¹„ì¤‘
- ìˆ˜ë„ê¶Œ ì§‘ì¤‘ë„ ê³ ë ¤
- ë˜ëŠ” ë©´ì  ê¸°ë°˜ ì ‘ê·¼

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "value": <ì¶”ì •ê°’>,
    "unit": "ëª…",
    "confidence": <0.3-0.7>,
    "method": "ì ‘ê·¼ ë°©ë²•",
    "decomposition": [
        {"step": "ë‹¨ê³„ëª…", "value": <ê°’>, "assumption": "ê°€ì •", "reasoning": "ê·¼ê±°"}
    ],
    "reasoning": "ì „ì²´ ì¶”ì • ë…¼ë¦¬"
}'''
    },
    
    'coffee_shops': {
        'name': 'í•œêµ­ ì»¤í”¼ ì „ë¬¸ì  ìˆ˜',
        'ground_truth': 100000,
        'unit': 'ê°œ',
        'ai_baseline': {
            'estimate': 78000,
            'error_rate': 0.22,
            'decomposition': [
                {
                    'step': 'ì»¤í”¼ ì†Œë¹„ì¸µ',
                    'value': 10400000,
                    'assumption': '20-60ì„¸ 50% Ã— ì •ê¸°ì†Œë¹„ 40% = 1040ë§Œëª…'
                },
                {
                    'step': 'ì í¬ë‹¹ ê³ ê°',
                    'value': 800,
                    'assumption': 'ìƒê¶Œ ë°˜ê²½ 500m, ì¸êµ¬ 2000ëª… Ã— 40% = 800ëª…'
                },
                {
                    'step': 'í•„ìš” ì í¬',
                    'value': 13000,
                    'assumption': '1040ë§Œ / 800 = 13000ê°œ'
                },
                {
                    'step': 'ê²½ìŸ ë³´ì •',
                    'value': 78000,
                    'assumption': 'ë¸Œëœë“œ ë‹¤ì–‘ì„± Ã— 6ë°° = 78000ê°œ'
                }
            ],
            'strengths': [
                'ê³ ê° ì¤‘ì‹¬ bottom-up',
                'ìƒê¶Œ ê°œë… ë„ì…',
                'ê²½ìŸ ë³´ì • ê³ ë ¤'
            ],
            'weaknesses': [
                'ì¤‘ë³µ ìƒê¶Œ ê³„ìˆ˜ ê·¼ê±° ë¶€ì¡±',
                'ì‹¤ì œë³´ë‹¤ ë‚®ê²Œ ì¶”ì • (22% ì˜¤ì°¨)'
            ]
        },
        'prompt': '''í•œêµ­ ì»¤í”¼ ì „ë¬¸ì  ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

íŒíŠ¸:
- ì»¤í”¼ ì†Œë¹„ ì¸êµ¬
- ì í¬ë‹¹ ê³ ê° ìˆ˜
- ë¸Œëœë“œ ê²½ìŸ ë° ìƒê¶Œ ì¤‘ë³µ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "value": <ì¶”ì •ê°’>,
    "unit": "ê°œ",
    "confidence": <0.3-0.7>,
    "method": "ì ‘ê·¼ ë°©ë²•",
    "decomposition": [
        {"step": "ë‹¨ê³„ëª…", "value": <ê°’>, "assumption": "ê°€ì •", "reasoning": "ê·¼ê±°"}
    ],
    "reasoning": "ì „ì²´ ì¶”ì • ë…¼ë¦¬"
}'''
    }
}


def test_model_on_problem(client, model, api_type, problem_id, problem_def, config=None):
    """íŠ¹ì • ë¬¸ì œì—ì„œ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    prompt = problem_def['prompt']
    
    try:
        start = time.time()
        
        if api_type == 'responses':
            api_params = {
                "model": model,
                "input": prompt,
            }
            
            if config:
                api_params["reasoning"] = config.get("reasoning", {"effort": "medium"})
                api_params["text"] = config.get("text", {"verbosity": "low"})
            
            response = client.responses.create(**api_params)
            
            if hasattr(response, 'output_text'):
                content = response.output_text
            elif hasattr(response, 'output'):
                content = response.output
            else:
                content = str(response)
            
            tokens = {
                'input': getattr(response, 'input_tokens', len(prompt) // 4),
                'output': getattr(response, 'output_tokens', len(content) // 4),
            }
            tokens['total'] = tokens['input'] + tokens['output']
            
        else:
            api_params = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
            }
            
            if config and config.get('reasoning_effort'):
                api_params["reasoning_effort"] = config['reasoning_effort']
            
            response = client.chat.completions.create(**api_params)
            content = response.choices[0].message.content
            
            tokens = {
                'input': response.usage.prompt_tokens,
                'output': response.usage.completion_tokens,
                'total': response.usage.total_tokens
            }
        
        elapsed = time.time() - start
        
        # JSON íŒŒì‹±
        import re
        
        try:
            if '```json' in content:
                json_start = content.find('```json') + 7
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()
            elif '```' in content:
                json_start = content.find('```') + 3
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()
            
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)
            
            parsed = json.loads(content)
        except Exception as e:
            parsed = {'raw': content[:500], 'parse_error': str(e)}
        
        return {
            'success': True,
            'elapsed_seconds': round(elapsed, 2),
            'tokens': tokens,
            'response': parsed,
            'raw_content': content
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


def evaluate_accuracy(model_value, ground_truth, ai_baseline_value):
    """
    ì •í™•ë„ í‰ê°€ (40ì )
    - ì‹¤ì œê°’ ëŒ€ë¹„ ì˜¤ì°¨
    - AI ê¸°ì¤€ì„  ëŒ€ë¹„ ë¹„êµ
    """
    # valueê°€ dictì¸ ê²½ìš° ì²˜ë¦¬
    if isinstance(model_value, dict):
        model_value = 0
    
    if not isinstance(model_value, (int, float)) or model_value <= 0 or ground_truth <= 0:
        return {
            'score': 0,
            'details': 'ìœ íš¨í•˜ì§€ ì•Šì€ ê°’'
        }
    
    # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì˜¤ì°¨
    model_error = abs(math.log10(model_value) - math.log10(ground_truth))
    ai_error = abs(math.log10(ai_baseline_value) - math.log10(ground_truth))
    
    # ì ˆëŒ€ ì •í™•ë„ (25ì )
    if model_error < 0.05:  # 5% ì´ë‚´
        abs_score = 25
    elif model_error < 0.1:  # 25% ì´ë‚´
        abs_score = 20
    elif model_error < 0.3:  # 2ë°° ì´ë‚´
        abs_score = 15
    elif model_error < 0.5:  # 3ë°° ì´ë‚´
        abs_score = 10
    elif model_error < 1.0:  # 10ë°° ì´ë‚´
        abs_score = 5
    else:
        abs_score = 0
    
    # AI ëŒ€ë¹„ ìƒëŒ€ í‰ê°€ (15ì )
    if model_error < ai_error * 0.5:  # AIë³´ë‹¤ 2ë°° ì •í™•
        relative_score = 15
    elif model_error < ai_error * 0.8:  # AIë³´ë‹¤ 1.25ë°° ì •í™•
        relative_score = 12
    elif model_error < ai_error:  # AIë³´ë‹¤ ì •í™•
        relative_score = 10
    elif model_error < ai_error * 1.5:  # AIì™€ ìœ ì‚¬
        relative_score = 7
    else:  # AIë³´ë‹¤ ë¶€ì •í™•
        relative_score = 3
    
    total = abs_score + relative_score
    
    return {
        'score': total,
        'absolute_score': abs_score,
        'relative_score': relative_score,
        'model_error_pct': round((10**model_error - 1) * 100, 1),
        'ai_error_pct': round((10**ai_error - 1) * 100, 1),
        'vs_ai': 'better' if model_error < ai_error else 'worse'
    }


def evaluate_decomposition_quality(decomp, ai_baseline_decomp):
    """
    ë¶„í•´ í•©ë¦¬ì„± í‰ê°€ (30ì )
    - ë‹¨ê³„ ìˆ˜ ë° êµ¬ì¡°
    - ê°€ì •ì˜ í•©ë¦¬ì„±
    - AI ê¸°ì¤€ì„  ëŒ€ë¹„
    """
    if not isinstance(decomp, list) or len(decomp) == 0:
        return {
            'score': 0,
            'details': 'decomposition ì—†ìŒ'
        }
    
    score = 0
    details = []
    
    # 1. ë‹¨ê³„ ìˆ˜ (10ì )
    ai_steps = len(ai_baseline_decomp)
    model_steps = len(decomp)
    
    if model_steps >= ai_steps:
        step_score = 10
        details.append(f"âœ… ë‹¨ê³„ ì¶©ë¶„ ({model_steps}ë‹¨ê³„, AI: {ai_steps}ë‹¨ê³„)")
    elif model_steps >= ai_steps * 0.75:
        step_score = 7
        details.append(f"âš ï¸ ë‹¨ê³„ ë¶€ì¡± ({model_steps}ë‹¨ê³„, AI: {ai_steps}ë‹¨ê³„)")
    else:
        step_score = 3
        details.append(f"âŒ ë‹¨ê³„ ë§¤ìš° ë¶€ì¡± ({model_steps}ë‹¨ê³„, AI: {ai_steps}ë‹¨ê³„)")
    
    score += step_score
    
    # 2. ê° ë‹¨ê³„ì˜ ì™„ì„±ë„ (20ì )
    quality_score = 0
    for step in decomp:
        # ê¸°ë³¸ êµ¬ì¡° (ê° 2ì )
        if 'step' in step and step['step']:
            quality_score += 2
        if 'assumption' in step and step['assumption']:
            quality_score += 2
        if 'value' in step and step['value']:
            quality_score += 1
    
    max_quality = min(len(decomp), 4) * 5
    quality_score = min(quality_score, 20)
    score += quality_score
    
    details.append(f"ê° ë‹¨ê³„ ì™„ì„±ë„: {quality_score}/20")
    
    return {
        'score': min(score, 30),
        'details': details
    }


def evaluate_logic_coherence(decomp, final_value, method):
    """
    ëª©í‘œ ì •ì˜ ê·¼ì ‘ì„± í‰ê°€ (30ì )
    - ê³„ì‚° ë¡œì§ ì¼ê´€ì„±
    - ë°©ë²•ë¡  ì ì ˆì„±
    """
    if not isinstance(decomp, list) or len(decomp) == 0:
        return {
            'score': 0,
            'details': 'ë¶„í•´ ì—†ìŒ'
        }
    
    score = 0
    details = []
    
    # 1. ê³„ì‚° ì¼ê´€ì„± (20ì )
    try:
        values = [step.get('value', 0) for step in decomp if 'value' in step and step['value']]
        
        if len(values) >= 2 and final_value > 0:
            # ê³±ì…ˆ ë˜ëŠ” í•©ì‚° ì²´í¬
            product = 1
            total_sum = 0
            
            for v in values:
                if v > 0:
                    product *= v
                    total_sum += v
            
            # ë‹¤ì–‘í•œ ë‹¨ìœ„ ì¡°ì • ì‹œë„
            test_values = [
                product,
                product / 10000,
                product / 100000000,
                total_sum,
                values[-1] if values else 0
            ]
            
            best_match = min([
                abs(math.log10(tv / final_value)) if tv > 0 and final_value > 0 else 999
                for tv in test_values
            ])
            
            if best_match < 0.1:  # 25% ì´ë‚´
                score += 20
                details.append("âœ… ê³„ì‚° ë¡œì§ ì™„ë²½")
            elif best_match < 0.3:  # 2ë°° ì´ë‚´
                score += 15
                details.append("âœ… ê³„ì‚° ë¡œì§ ì–‘í˜¸")
            elif best_match < 1.0:  # 10ë°° ì´ë‚´
                score += 10
                details.append("âš ï¸ ê³„ì‚° ë¡œì§ ë¶€ë¶„ ì¼ì¹˜")
            else:
                score += 5
                details.append("âŒ ê³„ì‚° ë¡œì§ ë¶ˆì¼ì¹˜")
    except:
        details.append("âš ï¸ ê³„ì‚° ê²€ì¦ ì‹¤íŒ¨")
    
    # 2. ë°©ë²•ë¡  ëª…ì‹œ (10ì )
    if method and method.lower() in ['top-down', 'bottom-up', 'hybrid']:
        score += 10
        details.append(f"âœ… ë°©ë²•ë¡  ëª…ì‹œ: {method}")
    else:
        score += 5
        details.append("âš ï¸ ë°©ë²•ë¡  ë¶ˆëª…í™•")
    
    return {
        'score': min(score, 30),
        'details': details
    }


def comprehensive_evaluate(model_name, response, problem_def):
    """ì¢…í•© í‰ê°€"""
    
    ground_truth = problem_def['ground_truth']
    ai_baseline = problem_def['ai_baseline']
    
    result = {
        'model': model_name,
        'problem': problem_def['name'],
        'value': response.get('value', 0),
        'unit': response.get('unit', ''),
        'ground_truth': ground_truth
    }
    
    # 1. ì •í™•ë„ (40ì )
    accuracy = evaluate_accuracy(
        response.get('value', 0),
        ground_truth,
        ai_baseline['estimate']
    )
    result['accuracy'] = accuracy
    
    # 2. ë¶„í•´ í•©ë¦¬ì„± (30ì )
    decomp_quality = evaluate_decomposition_quality(
        response.get('decomposition', []),
        ai_baseline['decomposition']
    )
    result['decomposition_quality'] = decomp_quality
    
    # 3. ëª©í‘œ ê·¼ì ‘ì„± (30ì )
    logic = evaluate_logic_coherence(
        response.get('decomposition', []),
        response.get('value', 0),
        response.get('method', '')
    )
    result['logic_coherence'] = logic
    
    # ì´ì 
    result['total_score'] = (
        accuracy['score'] +
        decomp_quality['score'] +
        logic['score']
    )
    
    return result


def run_comprehensive_fermi_test():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 120)
    print("Fermi ì¶”ì • ì¢…í•© í‰ê°€ - AI ê¸°ì¤€ì„  ë¹„êµ")
    print("Phase 4ì—ì„œ 80ì  ì´ìƒ ëª¨ë¸ Ã— 3ê°œ ì‹¤ì œ ë°ì´í„° ë¬¸ì œ")
    print("=" * 120)
    print()
    
    client = OpenAI()
    
    # í…ŒìŠ¤íŠ¸ ëª¨ë¸ (Phase 4ì—ì„œ 80ì  ì´ìƒ)
    test_configs = [
        ('gpt-4.1-nano', 'chat', None),
        ('gpt-4o-mini', 'chat', None),
        ('gpt-4.1-mini', 'chat', None),
        ('gpt-5.1', 'chat', {'reasoning_effort': 'medium'}),
        ('gpt-5.1', 'responses', {
            'reasoning': {'effort': 'medium'},
            'text': {'verbosity': 'low'}
        }),
    ]
    
    all_results = []
    
    # ê° ë¬¸ì œì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for problem_id, problem_def in FERMI_PROBLEMS.items():
        print(f"\n{'='*120}")
        print(f"ğŸ“‹ ë¬¸ì œ: {problem_def['name']}")
        print(f"   ì •ë‹µ: {problem_def['ground_truth']:,} {problem_def['unit']}")
        print(f"   AI ì¶”ì •: {problem_def['ai_baseline']['estimate']:,} {problem_def['unit']} (ì˜¤ì°¨: {problem_def['ai_baseline']['error_rate']*100:.1f}%)")
        print(f"{'='*120}\n")
        
        problem_results = []
        
        for model, api_type, config in test_configs:
            config_name = f"{model} ({api_type})"
            print(f"í…ŒìŠ¤íŠ¸: {config_name}")
            
            test_result = test_model_on_problem(
                client, model, api_type, problem_id, problem_def, config
            )
            
            if test_result['success']:
                response = test_result['response']
                
                # value íƒ€ì… ì²´í¬ ë° ë³´ì •
                if isinstance(response.get('value'), dict):
                    # dictì¸ ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬
                    response['value'] = 0
                elif not isinstance(response.get('value'), (int, float)):
                    response['value'] = 0
                
                # í‰ê°€
                eval_result = comprehensive_evaluate(
                    config_name, response, problem_def
                )
                
                eval_result['test_info'] = {
                    'elapsed': test_result['elapsed_seconds'],
                    'tokens': test_result['tokens']
                }
                eval_result['raw_response'] = response
                
                problem_results.append(eval_result)
                all_results.append(eval_result)
                
                print(f"  âœ… ì™„ë£Œ")
                print(f"     ì¶”ì •ê°’: {response.get('value', 0):,} {response.get('unit', '')}")
                print(f"     ì •í™•ë„: {eval_result['accuracy']['score']}/40")
                print(f"     ë¶„í•´ í’ˆì§ˆ: {eval_result['decomposition_quality']['score']}/30")
                print(f"     ë…¼ë¦¬ ì¼ê´€ì„±: {eval_result['logic_coherence']['score']}/30")
                print(f"     ì´ì : {eval_result['total_score']}/100")
            else:
                print(f"  âŒ ì˜¤ë¥˜: {test_result['error'][:50]}")
            
            print()
            time.sleep(2)
        
        # ë¬¸ì œë³„ ìš”ì•½
        if problem_results:
            print(f"\nğŸ“Š {problem_def['name']} ê²°ê³¼ ìš”ì•½\n")
            problem_results.sort(key=lambda x: x['total_score'], reverse=True)
            
            print(f"{'ìˆœìœ„':<4} | {'ëª¨ë¸':<30} | {'ì¶”ì •ê°’':<15} | {'ì´ì ':<8} | {'ì •í™•ë„':<8} | {'ë¶„í•´':<8} | {'ë…¼ë¦¬':<8}")
            print("-" * 120)
            
            for i, r in enumerate(problem_results, 1):
                marker = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
                print(f"{marker}{i:<3} | {r['model']:<30} | {r['value']:>13,}{r['unit']:<2} | {r['total_score']:>6}/100 | {r['accuracy']['score']:>6}/40 | {r['decomposition_quality']['score']:>6}/30 | {r['logic_coherence']['score']:>6}/30")
    
    # ìµœì¢… ì¢…í•© ê²°ê³¼
    print("\n\n" + "=" * 120)
    print("ğŸ† ìµœì¢… ì¢…í•© ê²°ê³¼ (3ê°œ ë¬¸ì œ í‰ê· )")
    print("=" * 120)
    print()
    
    # ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
    from collections import defaultdict
    
    by_model = defaultdict(list)
    for r in all_results:
        by_model[r['model']].append(r)
    
    model_averages = []
    for model_name, results in by_model.items():
        avg_score = sum(r['total_score'] for r in results) / len(results)
        avg_accuracy = sum(r['accuracy']['score'] for r in results) / len(results)
        avg_decomp = sum(r['decomposition_quality']['score'] for r in results) / len(results)
        avg_logic = sum(r['logic_coherence']['score'] for r in results) / len(results)
        
        model_averages.append({
            'model': model_name,
            'avg_total': avg_score,
            'avg_accuracy': avg_accuracy,
            'avg_decomp': avg_decomp,
            'avg_logic': avg_logic,
            'results': results
        })
    
    model_averages.sort(key=lambda x: x['avg_total'], reverse=True)
    
    print(f"{'ìˆœìœ„':<4} | {'ëª¨ë¸':<30} | {'í‰ê·  ì´ì ':<10} | {'ì •í™•ë„':<10} | {'ë¶„í•´':<10} | {'ë…¼ë¦¬':<10}")
    print("-" * 120)
    
    for i, m in enumerate(model_averages, 1):
        marker = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
        print(f"{marker}{i:<3} | {m['model']:<30} | {m['avg_total']:>8.1f}/100 | {m['avg_accuracy']:>8.1f}/40 | {m['avg_decomp']:>8.1f}/30 | {m['avg_logic']:>8.1f}/30")
    
    # ìƒì„¸ ë¬¸ì„œ ì €ì¥
    output_file = f"fermi_comprehensive_evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'fermi_comprehensive_ai_baseline',
                'models_tested': len(test_configs),
                'problems': list(FERMI_PROBLEMS.keys()),
                'evaluation_criteria': {
                    'accuracy': '40ì  (ì ˆëŒ€ 25ì  + ìƒëŒ€ 15ì )',
                    'decomposition': '30ì  (êµ¬ì¡° ë° í•©ë¦¬ì„±)',
                    'logic': '30ì  (ê³„ì‚° ì¼ê´€ì„± + ë°©ë²•ë¡ )'
                }
            },
            'problems': FERMI_PROBLEMS,
            'results': all_results,
            'summary': {
                'by_model': model_averages
            }
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\n\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    
    # ìƒì„¸ ë¶„ì„ ë¬¸ì„œ ìƒì„±
    generate_detailed_report(all_results, model_averages, FERMI_PROBLEMS)
    
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


def generate_detailed_report(all_results, model_averages, problems):
    """ìƒì„¸ ë¶„ì„ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±"""
    
    report_file = f"docs/FERMI_TEST_DETAILED_REPORT_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# Fermi ì¶”ì • ì¢…í•© í‰ê°€ - ìƒì„¸ ë³´ê³ ì„œ\n\n")
        f.write(f"**ì‘ì„±ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        # ê°œìš”
        f.write("## ğŸ“‹ ê°œìš”\n\n")
        f.write(f"- **í…ŒìŠ¤íŠ¸ ëª¨ë¸**: {len(model_averages)}ê°œ\n")
        f.write(f"- **ë¬¸ì œ ìˆ˜**: {len(problems)}ê°œ\n")
        f.write("- **í‰ê°€ ë°©ì‹**: AI ê¸°ì¤€ì„  ë¹„êµ\n\n")
        
        # ë¬¸ì œë³„ ìƒì„¸ ê²°ê³¼
        for problem_id, problem_def in problems.items():
            f.write(f"\n## ğŸ¯ ë¬¸ì œ: {problem_def['name']}\n\n")
            f.write(f"**ì •ë‹µ**: {problem_def['ground_truth']:,} {problem_def['unit']}\n\n")
            
            # AI ê¸°ì¤€ì„ 
            ai = problem_def['ai_baseline']
            f.write(f"### AI ê¸°ì¤€ì„  (Assistant)\n\n")
            f.write(f"- **ì¶”ì •ê°’**: {ai['estimate']:,} {problem_def['unit']}\n")
            f.write(f"- **ì˜¤ì°¨ìœ¨**: {ai['error_rate']*100:.1f}%\n\n")
            
            f.write("**ë¶„í•´ ê³¼ì •**:\n")
            for i, step in enumerate(ai['decomposition'], 1):
                f.write(f"{i}. {step['step']}: {step['value']:,}\n")
                f.write(f"   - ê°€ì •: {step['assumption']}\n")
            
            f.write("\n")
            
            # ëª¨ë¸ë³„ ê²°ê³¼
            problem_results = [r for r in all_results if r['problem'] == problem_def['name']]
            problem_results.sort(key=lambda x: x['total_score'], reverse=True)
            
            f.write(f"### ëª¨ë¸ ê²°ê³¼\n\n")
            f.write(f"| ìˆœìœ„ | ëª¨ë¸ | ì¶”ì •ê°’ | ì˜¤ì°¨ | ì´ì  | ì •í™•ë„ | ë¶„í•´ | ë…¼ë¦¬ |\n")
            f.write(f"|------|------|--------|------|------|--------|------|------|\n")
            
            for i, r in enumerate(problem_results, 1):
                error_pct = r['accuracy'].get('model_error_pct', 0) if r['accuracy']['score'] > 0 else 999
                f.write(f"| {i} | {r['model']} | {r['value']:,} | {error_pct:.1f}% | {r['total_score']}/100 | {r['accuracy']['score']}/40 | {r['decomposition_quality']['score']}/30 | {r['logic_coherence']['score']}/30 |\n")
            
            f.write("\n#### ìƒì„¸ ë¶„ì„\n\n")
            
            for r in problem_results:
                f.write(f"**{r['model']}**\n\n")
                f.write(f"- ì¶”ì •ê°’: {r['value']:,} {r['unit']}\n")
                f.write(f"- ì´ì : {r['total_score']}/100\n\n")
                
                # ì •í™•ë„
                acc = r['accuracy']
                f.write(f"**ì •í™•ë„** ({acc['score']}/40):\n")
                f.write(f"- ì ˆëŒ€ ì˜¤ì°¨: {acc['model_error_pct']:.1f}% (AI: {acc['ai_error_pct']:.1f}%)\n")
                f.write(f"- AI ëŒ€ë¹„: {acc['vs_ai']}\n\n")
                
                # ë¶„í•´
                decomp = r['decomposition_quality']
                f.write(f"**ë¶„í•´ í•©ë¦¬ì„±** ({decomp['score']}/30):\n")
                for detail in decomp['details']:
                    f.write(f"- {detail}\n")
                f.write("\n")
                
                # ë…¼ë¦¬
                logic = r['logic_coherence']
                f.write(f"**ë…¼ë¦¬ ì¼ê´€ì„±** ({logic['score']}/30):\n")
                for detail in logic['details']:
                    f.write(f"- {detail}\n")
                
                # ì‹¤ì œ ë¶„í•´ ë‚´ìš©
                if 'decomposition' in r['raw_response']:
                    f.write("\n**ë¶„í•´ ê³¼ì •**:\n")
                    for i, step in enumerate(r['raw_response']['decomposition'][:5], 1):
                        f.write(f"{i}. {step.get('step', 'N/A')}\n")
                        if 'value' in step:
                            f.write(f"   - ê°’: {step['value']}\n")
                        if 'assumption' in step:
                            f.write(f"   - ê°€ì •: {step['assumption'][:100]}\n")
                
                f.write("\n---\n\n")
        
        # ìµœì¢… ì¢…í•©
        f.write("\n## ğŸ† ìµœì¢… ì¢…í•© ìˆœìœ„\n\n")
        f.write(f"| ìˆœìœ„ | ëª¨ë¸ | í‰ê·  ì ìˆ˜ | ì •í™•ë„ | ë¶„í•´ | ë…¼ë¦¬ |\n")
        f.write(f"|------|------|----------|--------|------|------|\n")
        
        for i, m in enumerate(model_averages, 1):
            f.write(f"| {i} | {m['model']} | {m['avg_total']:.1f}/100 | {m['avg_accuracy']:.1f}/40 | {m['avg_decomp']:.1f}/30 | {m['avg_logic']:.1f}/30 |\n")
        
        f.write("\n## ğŸ’¡ ê²°ë¡ \n\n")
        f.write("ì´ í‰ê°€ëŠ” AI(Assistant)ê°€ ì§ì ‘ ì‘ì„±í•œ Fermi ë¶„í•´ë¥¼ ê¸°ì¤€ì„ ìœ¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n\n")
        f.write("**ì¥ì **:\n")
        f.write("- íˆ¬ëª…í•œ í‰ê°€ ê¸°ì¤€\n")
        f.write("- ìƒëŒ€ì  ë¹„êµ ê°€ëŠ¥\n")
        f.write("- ì‹¤ì œ ë°ì´í„° ê¸°ë°˜\n\n")
        
        f.write("**í•œê³„**:\n")
        f.write("- AI ê¸°ì¤€ì„ ë„ ì™„ë²½í•˜ì§€ ì•ŠìŒ (10-23% ì˜¤ì°¨)\n")
        f.write("- ë” ë‚˜ì€ ì ‘ê·¼ë²•ì´ ìˆì„ ìˆ˜ ìˆìŒ\n\n")
    
    print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")


if __name__ == "__main__":
    run_comprehensive_fermi_test()

