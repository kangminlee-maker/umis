#!/usr/bin/env python3
"""
ì¢…í•© LLM ë²¤ì¹˜ë§ˆí¬ (2025-11-21)
OpenAI + Claude ì „ì²´ ë¼ì¸ì—… í…ŒìŠ¤íŠ¸
- ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸
- Extended Thinking ëª¨ë“œ í¬í•¨
- í’ˆì§ˆ, ê°€ê²©, ì†ë„ ì¢…í•© í‰ê°€
"""

import os
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import OpenAI
import anthropic
import backoff

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


class ComprehensiveLLMBenchmark:
    """ì¢…í•© LLM ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.openai_client = OpenAI()
        self.anthropic_client = anthropic.Anthropic()
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ)
        self.models = {
            # OpenAI - nano (ì´ˆì €ê°€)
            'openai_nano': [
                'gpt-4.1-nano',
                'gpt-5-nano'
            ],
            # OpenAI - mini (ì €ê°€)
            'openai_mini': [
                'gpt-4o-mini',
                'gpt-4.1-mini',
                'gpt-5-mini'
            ],
            # OpenAI - standard (ì¼ë°˜)
            'openai_standard': [
                'gpt-4o',
                'gpt-4.1',
                'gpt-5',
                'gpt-5.1'
            ],
            # OpenAI - codex (ì½”ë“œ íŠ¹í™”)
            'openai_codex': [
                'gpt-5-codex',
                'gpt-5.1-codex'
            ],
            # OpenAI - pro (ìµœê³ ê¸‰)
            'openai_pro': [
                'gpt-5-pro'
            ],
            # OpenAI - thinking (o ì‹œë¦¬ì¦ˆ)
            'openai_thinking': [
                'o1',
                'o3',
                'o3-mini',
                'o4-mini'
            ],
            # OpenAI - thinking pro
            'openai_thinking_pro': [
                'o1-pro'
            ],
            # Claude - standard (ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ)
            'claude_standard': [
                'claude-haiku-3.5',
                'claude-sonnet-3.7',
                'claude-sonnet-4',
                'claude-opus-4'
            ]
        }
        
        # ê°€ê²© ì •ë³´ ($/1M í† í°) - 2025-11-21 ê¸°ì¤€
        self.pricing = {
            # OpenAI nano
            'gpt-4.1-nano': {'input': 0.10, 'output': 0.40},
            'gpt-5-nano': {'input': 0.05, 'output': 0.40},
            # OpenAI mini
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
            'gpt-5-mini': {'input': 0.25, 'output': 2.00},
            # OpenAI standard
            'gpt-4o': {'input': 2.50, 'output': 10.00},
            'gpt-4.1': {'input': 2.00, 'output': 8.00},
            'gpt-5': {'input': 1.25, 'output': 10.00},
            'gpt-5.1': {'input': 1.25, 'output': 10.00},
            # OpenAI codex
            'gpt-5-codex': {'input': 1.25, 'output': 10.00},
            'gpt-5.1-codex': {'input': 1.25, 'output': 10.00},
            # OpenAI pro
            'gpt-5-pro': {'input': 15.00, 'output': 120.00},
            # OpenAI thinking
            'o1': {'input': 15.00, 'output': 60.00},
            'o3': {'input': 2.00, 'output': 8.00},
            'o3-mini': {'input': 1.10, 'output': 4.40},
            'o4-mini': {'input': 1.10, 'output': 4.40},
            'o1-pro': {'input': 150.00, 'output': 600.00},
            # Claude standard (ì‹¤ì œ ê°€ê²©)
            'claude-haiku-3.5': {'input': 0.80, 'output': 4.00},
            'claude-sonnet-3.7': {'input': 3.00, 'output': 15.00},
            'claude-sonnet-4': {'input': 3.00, 'output': 15.00},
            'claude-opus-4': {'input': 15.00, 'output': 75.00}
        }
        
        # Claude API ì´ë¦„ ë§¤í•‘ (2025-11-21 ì—…ë°ì´íŠ¸)
        # ì°¸ì¡°: https://platform.claude.com/docs/ko/about-claude/models/migrating-to-claude-4
        self.claude_api_names = {
            'claude-haiku-3.5': 'claude-3-5-haiku-20241022',
            'claude-sonnet-3.7': 'claude-3-7-sonnet-20250219',
            'claude-sonnet-4': 'claude-sonnet-4-20250514',
            'claude-sonnet-4.5': 'claude-sonnet-4-5-20250929',  # Claude 4.5 (ìµœì‹ )
            'claude-haiku-4.5': 'claude-haiku-4-5-20251001',    # Claude 4.5 (ìµœì‹ )
            'claude-opus-4': 'claude-opus-4-20250514',
            'claude-opus-4.1': 'claude-opus-4-1-20250805'       # Claude 4.1 (ìµœì‹ )
        }
        
        # Responses APIë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ëª¨ë¸ë“¤
        self.responses_api_models = [
            'gpt-5-codex',
            'gpt-5.1-codex',
            'gpt-5-pro',
            'o1-pro'
        ]
        
        self.results = []
    
    def get_test_scenarios(self) -> List[Dict]:
        """UMIS 5-Phase í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (Phaseë³„ ìµœì  íŒŒë¼ë¯¸í„° + JSON Schema í¬í•¨)"""
        return [
            {
                'id': 'phase0',
                'name': 'Phase 0: Literal',
                'phase': 0,
                'prompt': '''ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS ARPU" ê°’ì„ ì •í™•íˆ ì°¾ì•„ ì¶”ì¶œí•˜ì„¸ìš”.

ì£¼ì–´ì§„ ë°ì´í„°:
- í•œêµ­ B2B SaaS ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ARPU: 70,000ì›

ìš”êµ¬ì‚¬í•­: B2B SaaS ê°’ë§Œ ì¶”ì¶œ, confidenceëŠ” 1.0ìœ¼ë¡œ ì„¤ì •''',
                'expected': {'value': 200000, 'confidence': 1.0},
                'temperature': 0.0,
                'reasoning_effort': 'low',
                'json_schema': {
                    "name": "literal_extraction",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì¶”ì¶œëœ ARPU ê°’"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 1.0, "maximum": 1.0}
                        },
                        "required": ["value", "unit", "confidence"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase1',
                'name': 'Phase 1: Direct RAG',
                'phase': 1,
                'prompt': '''RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì½”ì›¨ì´ì˜ ì›” ë Œíƒˆë£Œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

RAG ê²°ê³¼:
ì½”ì›¨ì´ ë Œíƒˆ ì‚¬ì—… ê°œìš”
- ì›” í‰ê·  ë Œíƒˆë£Œ: 33,000ì›
- ì´ êµ¬ë…ì ìˆ˜: 720ë§Œëª…
- ì£¼ìš” ì œí’ˆ: ì •ìˆ˜ê¸°, ê³µê¸°ì²­ì •ê¸°, ë¹„ë°

ìš”êµ¬ì‚¬í•­: ì›” ë Œíƒˆë£Œë§Œ ì¶”ì¶œ, confidenceëŠ” 1.0ìœ¼ë¡œ ì„¤ì •''',
                'expected': {'value': 33000},
                'temperature': 0.0,
                'reasoning_effort': 'low',
                'json_schema': {
                    "name": "rag_extraction",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì›” ë Œíƒˆë£Œ"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 1.0, "maximum": 1.0}
                        },
                        "required": ["value", "unit", "confidence"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase2',
                'name': 'Phase 2: Calculation',
                'phase': 2,
                'prompt': '''ì£¼ì–´ì§„ ê³µì‹ê³¼ ê°’ì„ ì‚¬ìš©í•˜ì—¬ LTVë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

ê³µì‹: LTV = ARPU / Churn_Rate

ì£¼ì–´ì§„ ê°’:
- ARPU = 80,000ì›
- Churn_Rate = 0.05

ê³„ì‚° ê³¼ì •:
1. ê³µì‹ì— ê°’ ëŒ€ì…
2. ë‚˜ëˆ—ì…ˆ ìˆ˜í–‰
3. ê²°ê³¼ë¥¼ ì› ë‹¨ìœ„ë¡œ í‘œí˜„

ìš”êµ¬ì‚¬í•­: ì •í™•í•œ ê³„ì‚° ê²°ê³¼, confidenceëŠ” 1.0ìœ¼ë¡œ ì„¤ì •''',
                'expected': {'value': 1600000},
                'temperature': 0.0,
                'reasoning_effort': 'low',
                'json_schema': {
                    "name": "calculation",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ê³„ì‚°ëœ LTV"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 1.0, "maximum": 1.0}
                        },
                        "required": ["value", "unit", "confidence"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase3_template',
                'name': 'Phase 3: Guestimation (í…œí”Œë¦¿)',
                'phase': 3,
                'prompt': '''ì£¼ì–´ì§„ í…œí”Œë¦¿ì„ ì°¸ê³ í•˜ì—¬ í•œêµ­ B2B SaaS ARPUë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  í…œí”Œë¦¿:
- ê¸€ë¡œë²Œ í‰ê·  ARPU: $100
- í•œêµ­ ì‹œì¥ ì¡°ì • ê³„ìˆ˜: 0.6 (êµ¬ë§¤ë ¥, ì‹œì¥ ì„±ìˆ™ë„ ê³ ë ¤)
- í™˜ìœ¨: 1,300ì›/$
- ê³„ì‚°: $100 Ã— 0.6 = $60 â†’ 78,000ì›

ì‘ì—…:
1. í…œí”Œë¦¿ì˜ ë…¼ë¦¬ë¥¼ ì´í•´
2. ìœ ì‚¬í•œ ì ‘ê·¼ë²• ì ìš©
3. í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œ ì¶”ì •

ì œì•½:
- reasoningì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
- confidenceëŠ” 0.7~1.0 ë²”ìœ„''',
                'expected': {'value_range': [50000, 150000], 'confidence_min': 0.7},
                'temperature': 0.3,
                'reasoning_effort': 'medium',
                'json_schema': {
                    "name": "guestimation_template",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì¶”ì •ëœ ARPU"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "reasoning": {"type": "string", "maxLength": 200, "description": "ì¶”ì • ê·¼ê±° ìš”ì•½"}
                        },
                        "required": ["value", "unit", "confidence", "reasoning"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase3_no_template',
                'name': 'Phase 3: Guestimation (í…œí”Œë¦¿ ì—†ìŒ)',
                'phase': 3,
                'prompt': '''í•œêµ­ ì˜¨ë¼ì¸ ì„±ì¸ ì·¨ë¯¸êµìœ¡ í”Œë«í¼ì˜ ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì‹œì¥ ì •ë³´:
- íƒ€ê²Ÿ ê³ ê°: ì§ì¥ì¸ 30-40ëŒ€
- ì£¼ìš” ê²½ìŸì‚¬: í´ë˜ìŠ¤101, íƒˆì‰
- ì„œë¹„ìŠ¤: ì˜¨ë¼ì¸ ì‹¤ì‹œê°„/VOD ê°•ì˜

ê³ ë ¤ì‚¬í•­:
1. íƒ€ê²Ÿ ê³ ê°ì˜ ì§€ë¶ˆ ëŠ¥ë ¥
2. ê²½ìŸì‚¬ ê°€ê²©ëŒ€ (ì‹œì¥ ì¡°ì‚¬ ë¶ˆê°€, ì¶”ì • í•„ìš”)
3. ì œê³µ ê°€ì¹˜ (í¸ì˜ì„±, í’ˆì§ˆ)
4. êµ¬ë… vs ë‹¨ê±´ ê²°ì œ ëª¨ë¸

ì œì•½:
- reasoningì€ ì¶”ì • ë…¼ë¦¬ë¥¼ ëª…í™•íˆ ì„¤ëª… (200ì ì´ë‚´)
- confidenceëŠ” 0.6~0.9 ë²”ìœ„ (í…œí”Œë¦¿ ì—†ìœ¼ë¯€ë¡œ ë‚®ìŒ)''',
                'expected': {'value_range': [10000, 50000], 'confidence_min': 0.6},
                'temperature': 0.5,
                'reasoning_effort': 'medium',
                'json_schema': {
                    "name": "guestimation_free",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì¶”ì •ëœ ì›” êµ¬ë…ë£Œ"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "reasoning": {"type": "string", "maxLength": 200, "description": "ì¶”ì • ë…¼ë¦¬"}
                        },
                        "required": ["value", "unit", "confidence", "reasoning"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase4_simple',
                'name': 'Phase 4: Simple Fermi',
                'phase': 4,
                'prompt': '''Fermi ì¶”ì • ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì„œìš¸ì‹œ í”¼ì•„ë…¸ í•™ì› ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ë‹¨ê³„ë³„ ì ‘ê·¼:
1. í•„ìš” ë³€ìˆ˜ ì‹ë³„ (ì˜ˆ: ì¸êµ¬, í•™ìŠµë¥ , í•™ì›ë‹¹ í•™ìƒ ìˆ˜)
2. ê° ë³€ìˆ˜ ê°’ ì¶”ì • (í•©ë¦¬ì  ê°€ì • ê¸°ë°˜)
3. ìµœì¢… ê³„ì‚° ìˆ˜í–‰
4. ê²°ê³¼ì˜ í•©ë¦¬ì„± ê²€ì¦

ë³€ìˆ˜ ì˜ˆì‹œ:
- ì„œìš¸ ì¸êµ¬
- í”¼ì•„ë…¸ í•™ìŠµ ì¸êµ¬ ë¹„ìœ¨
- í•™ì›ë‹¹ í‰ê·  í•™ìƒ ìˆ˜
- ì˜¨ë¼ì¸ vs ì˜¤í”„ë¼ì¸ ë¹„ìœ¨

ìš”êµ¬ì‚¬í•­:
- decomposition: ì£¼ìš” ë³€ìˆ˜ì™€ ê°’ì„ JSON ê°ì²´ë¡œ í‘œí˜„
- reasoning: ì¶”ì • ë…¼ë¦¬ë¥¼ ë‹¨ê³„ë³„ë¡œ ìš”ì•½ (300ì ì´ë‚´)
- confidence: 0.6~0.8 ë²”ìœ„''',
                'expected': {'value_range': [1500, 4000], 'confidence_min': 0.6},
                'temperature': 0.7,
                'reasoning_effort': 'high',
                'json_schema': {
                    "name": "fermi_simple",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì¶”ì •ëœ í•™ì› ìˆ˜"},
                            "unit": {"type": "string", "enum": ["ê°œ"]},
                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "reasoning": {"type": "string", "maxLength": 300, "description": "ë‹¨ê³„ë³„ ì¶”ì • ë…¼ë¦¬"}
                        },
                        "required": ["value", "unit", "confidence", "reasoning"],
                        "additionalProperties": False
                    }
                }
            },
            {
                'id': 'phase4_complex',
                'name': 'Phase 4: Complex Fermi',
                'phase': 4,
                'prompt': '''ë‹¤ì¸µ Fermi ì¶”ì •ìœ¼ë¡œ í•œêµ­ ì„±ì¸ í”¼ì•„ë…¸ í•™ìŠµ ì‹œì¥ì˜ ì—°ê°„ ì´ ì§€ì¶œì•¡ì„ ê³„ì‚°í•˜ì„¸ìš”.

í•„ìˆ˜ êµ¬ì„±ìš”ì†Œ:
1. í•™ìŠµì ìˆ˜ ì¶”ì •
   - ì—°ë ¹ëŒ€ë³„ í•™ìŠµ ë¹„ìœ¨
   - ì§€ì—­ë³„ ë¶„í¬
   
2. ì§€ì¶œ í•­ëª©ë³„ ì¶”ì •
   - í•™ì›ë¹„ (ì›”í‰ê·  Ã— 12ê°œì›”)
   - êµì¬ë¹„ (ì—°ê°„)
   - ì•…ê¸° êµ¬ë§¤/ë Œíƒˆ (ì´ˆê¸° + ìœ ì§€)
   - ê¸°íƒ€ ë¹„ìš© (ì¡°ìœ¨, ì•…ì„¸ì„œë¦¬ ë“±)

3. ì¶”ì • ëª¨ë¸
   - Top-down: ì „ì²´ ì‹œì¥ì—ì„œ í•˜í–¥ì‹
   - Bottom-up: ê°œì¸ ì§€ì¶œì—ì„œ ìƒí–¥ì‹
   - ìµœì†Œ 2ê°œ ëª¨ë¸ ì‚¬ìš©, models ë°°ì—´ì— ëª…ì‹œ

ìš”êµ¬ì‚¬í•­:
- decomposition: ì£¼ìš” ë³€ìˆ˜ë¥¼ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ
- models: ì‚¬ìš©í•œ ì¶”ì • ëª¨ë¸ ëª©ë¡ (ì˜ˆ: ["top_down", "bottom_up"])
- reasoning: ê° ëª¨ë¸ì˜ ë…¼ë¦¬ì™€ ìµœì¢… ê°’ ì„ íƒ ê·¼ê±° (500ì ì´ë‚´)
- confidence: 0.5~0.8 ë²”ìœ„ (ë³µì¡ë„ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±)''',
                'expected': {'value_range': [50000000000, 500000000000], 'confidence_min': 0.5},
                'temperature': 0.8,
                'reasoning_effort': 'high',
                'json_schema': {
                    "name": "fermi_complex",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "value": {"type": "number", "description": "ì¶”ì •ëœ ì—°ê°„ ì´ ì§€ì¶œì•¡"},
                            "unit": {"type": "string", "enum": ["ì›"]},
                            "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                            "models": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1,
                                "description": "ì‚¬ìš©í•œ ì¶”ì • ëª¨ë¸ ëª©ë¡"
                            },
                            "reasoning": {"type": "string", "maxLength": 500, "description": "ëª¨ë¸ë³„ ë…¼ë¦¬ì™€ ìµœì¢… ì„ íƒ ê·¼ê±°"}
                        },
                        "required": ["value", "unit", "confidence", "models", "reasoning"],
                        "additionalProperties": False
                    }
                }
            }
        ]
    
    @backoff.on_exception(
        backoff.expo,
        (Exception),
        max_tries=3,
        max_time=30,
        giveup=lambda e: "429" not in str(e) and "rate limit" not in str(e).lower() and "timeout" not in str(e).lower()
    )
    def _call_openai_with_retry(self, api_params: Dict) -> Any:
        """OpenAI API í˜¸ì¶œ with retry (exponential backoff)"""
        return self.openai_client.chat.completions.create(**api_params)
    
    @backoff.on_exception(
        backoff.expo,
        (Exception),
        max_tries=3,
        max_time=30,
        giveup=lambda e: "429" not in str(e) and "rate limit" not in str(e).lower() and "timeout" not in str(e).lower()
    )
    def _call_openai_responses_with_retry(self, model: str, input_text: str) -> Any:
        """OpenAI Responses API í˜¸ì¶œ with retry (exponential backoff)"""
        return self.openai_client.responses.create(
            model=model,
            input=input_text
        )
    
    @backoff.on_exception(
        backoff.expo,
        (Exception),
        max_tries=3,
        max_time=30,
        giveup=lambda e: "429" not in str(e) and "rate limit" not in str(e).lower() and "timeout" not in str(e).lower()
    )
    def _call_claude_with_retry(self, api_params: Dict) -> Any:
        """Claude API í˜¸ì¶œ with retry (exponential backoff)"""
        return self.anthropic_client.messages.create(**api_params)
    
    def test_openai_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Chat Completions ë˜ëŠ” Responses API)"""
        
        # Responses API ëª¨ë¸ì¸ì§€ í™•ì¸
        if model in self.responses_api_models:
            return self.test_openai_responses_model(model, scenario)
        
        # ê¸°ì¡´ Chat Completions API ë¡œì§
        start_time = time.time()
        
        try:
            # ëª¨ë¸ íƒ€ì… êµ¬ë¶„
            is_o_series = model.startswith(('o1', 'o3', 'o4'))  # o1/o3/o4 ì‹œë¦¬ì¦ˆ
            is_gpt5 = model.startswith('gpt-5')  # gpt-5 ì‹œë¦¬ì¦ˆ
            is_reasoning_model = is_o_series or is_gpt5
            
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            user_prompt = scenario['prompt']
            
            # reasoning ëª¨ë¸ìš© JSON ê°•ì¡° ì¶”ê°€
            if is_reasoning_model:
                user_prompt += "\n\nâš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ìˆœìˆ˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì–´ë– í•œ ì„¤ëª…ì´ë‚˜ ë§ˆí¬ë‹¤ìš´ ì—†ì´ JSON ê°ì²´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
            
            messages = [{"role": "user", "content": user_prompt}]
            if not is_reasoning_model:
                messages.insert(0, {"role": "system", "content": "ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€. JSONë§Œ ë°˜í™˜."})
            
            # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            api_params = {
                "model": model,
                "messages": messages
            }
            
            # Phaseë³„ ìµœì  íŒŒë¼ë¯¸í„° ì‚¬ìš©
            if is_reasoning_model:
                # reasoning_effort ì‚¬ìš© (Phaseë³„ ì°¨ë³„í™”)
                reasoning_effort = scenario.get('reasoning_effort', 'medium')
                api_params["reasoning_effort"] = reasoning_effort
                
                # GPT-5 ì „ìš©: verbosity ì¶”ê°€ (JSON ì‘ë‹µì´ë¯€ë¡œ low)
                if is_gpt5:
                    api_params["verbosity"] = "low"  # ê°„ê²°í•œ JSON ì‘ë‹µ
                
                # reasoning ëª¨ë¸ì€ response_formatì„ ì§€ì›í•˜ì§€ ì•ŠìŒ!
                # JSON Schemaë„ ì‚¬ìš© ë¶ˆê°€
            else:
                # temperature ì‚¬ìš© (Phaseë³„ ì°¨ë³„í™”)
                temperature = scenario.get('temperature', 0.2)
                
                # Claude ë²”ìœ„ ì´ˆê³¼ ë°©ì§€ (0~1)
                if 'claude' in model.lower():
                    temperature = min(temperature, 1.0)
                
                api_params["temperature"] = temperature
                
                # JSON Schema ì ìš© (structured outputs) - ì¼ë°˜ ëª¨ë¸ë§Œ
                json_schema = scenario.get('json_schema')
                if json_schema:
                    api_params["response_format"] = {
                        "type": "json_schema",
                        "json_schema": json_schema
                    }
                else:
                    # fallback: ì¼ë°˜ json_object
                    api_params["response_format"] = {"type": "json_object"}
            
            # API í˜¸ì¶œ with retry
            response = self._call_openai_with_retry(api_params)
            
            elapsed = time.time() - start_time
            content = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ (ê°•í™”ëœ íŒŒì‹±)
            import re
            
            try:
                # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                # 2. JSON ê°ì²´ ì¶”ì¶œ (ì •ê·œì‹)
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
                
                parsed = json.loads(content)
            except:
                parsed = {'raw': content, 'parse_error': True}
            
            # í† í° ì‚¬ìš©ëŸ‰ (reasoning_tokens í¬í•¨)
            tokens = {
                'input': response.usage.prompt_tokens,
                'output': response.usage.completion_tokens,
                'total': response.usage.total_tokens
            }
            
            # reasoning_tokens ì¶”ê°€ (reasoning ëª¨ë¸ë§Œ)
            if hasattr(response.usage, 'completion_tokens_details'):
                details = response.usage.completion_tokens_details
                if hasattr(details, 'reasoning_tokens') and details.reasoning_tokens:
                    tokens['reasoning'] = details.reasoning_tokens
            
            cost = self._calculate_cost(model, response.usage.prompt_tokens, response.usage.completion_tokens)
            quality = self._evaluate_quality(parsed, scenario.get('expected', {}), scenario['phase'])
            
            # ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ê¸°ë¡
            used_params = {}
            if is_reasoning_model:
                used_params['reasoning_effort'] = api_params.get('reasoning_effort')
                if is_gpt5:
                    used_params['verbosity'] = api_params.get('verbosity')
            else:
                used_params['temperature'] = api_params.get('temperature')
            
            return {
                'provider': 'openai',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'response': parsed,
                'quality_score': quality,
                'tokens': tokens,
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'parameters': used_params,  # ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ê¸°ë¡
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            return {
                'provider': 'openai',
                'model': model,
                'scenario_id': scenario['id'],
                'error': str(e),
                'elapsed_seconds': round(time.time() - start_time, 2),
                'success': False
            }
    
    def test_openai_responses_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """OpenAI Responses API ì „ìš© í…ŒìŠ¤íŠ¸ (codex, pro ëª¨ë¸)"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            input_text = scenario['prompt']
            
            # JSON í˜•ì‹ ìš”ì²­ ì¶”ê°€
            input_text += "\n\nâš ï¸ ì¤‘ìš”: ë°˜ë“œì‹œ ìˆœìˆ˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
            
            # Responses API í˜¸ì¶œ with retry
            response = self._call_openai_responses_with_retry(model, input_text)
            
            elapsed = time.time() - start_time
            
            # output_text ì¶”ì¶œ
            if hasattr(response, 'output_text'):
                content = response.output_text
            elif hasattr(response, 'output'):
                content = response.output
            else:
                content = str(response)
            
            # JSON ì¶”ì¶œ (ê°•í™”ëœ íŒŒì‹±)
            import re
            
            try:
                # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                # 2. JSON ê°ì²´ ì¶”ì¶œ (ì •ê·œì‹)
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
                
                parsed = json.loads(content)
            except:
                parsed = {'raw': content, 'parse_error': True}
            
            # í† í° ì‚¬ìš©ëŸ‰ (Responses APIëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            tokens = {
                'input': getattr(response, 'input_tokens', 0),
                'output': getattr(response, 'output_tokens', 0),
                'total': getattr(response, 'total_tokens', 0)
            }
            
            # í† í°ì´ 0ì´ë©´ ì¶”ì • (ë¬¸ì ìˆ˜ ê¸°ë°˜)
            if tokens['total'] == 0:
                tokens['input'] = len(input_text) // 4  # ëŒ€ëµì  ì¶”ì •
                tokens['output'] = len(content) // 4
                tokens['total'] = tokens['input'] + tokens['output']
            
            cost = self._calculate_cost(model, tokens['input'], tokens['output'])
            quality = self._evaluate_quality(parsed, scenario.get('expected', {}), scenario['phase'])
            
            return {
                'provider': 'openai_responses',
                'api_type': 'responses',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'response': parsed,
                'quality_score': quality,
                'tokens': tokens,
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            return {
                'provider': 'openai_responses',
                'api_type': 'responses',
                'model': model,
                'scenario_id': scenario['id'],
                'error': str(e),
                'elapsed_seconds': round(time.time() - start_time, 2),
                'success': False
            }
    
    def test_claude_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            api_model = self.claude_api_names.get(model, model)
            
            # Phaseë³„ ìµœì  temperature ì‚¬ìš© (ClaudeëŠ” 0~1 ë²”ìœ„)
            temperature = scenario.get('temperature', 0.2)
            temperature = min(temperature, 1.0)  # ClaudeëŠ” ìµœëŒ€ 1.0
            
            # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            api_params = {
                "model": api_model,
                "max_tokens": 2048,
                "temperature": temperature,
                "system": "ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€. JSONë§Œ ë°˜í™˜.",
                "messages": [{"role": "user", "content": scenario['prompt']}]
            }
            
            # API í˜¸ì¶œ with retry
            response = self._call_claude_with_retry(api_params)
            
            elapsed = time.time() - start_time
            
            # refusal ì¤‘ì§€ ì´ìœ  ì²˜ë¦¬ (Claude 4.5 ìš”êµ¬ì‚¬í•­)
            if response.stop_reason == "refusal":
                return {
                    'provider': 'claude',
                    'model': model,
                    'api_model': api_model,
                    'scenario_id': scenario['id'],
                    'scenario_name': scenario['name'],
                    'phase': scenario['phase'],
                    'error': 'Model refused to respond (safety/policy)',
                    'stop_reason': 'refusal',
                    'elapsed_seconds': round(elapsed, 2),
                    'timestamp': datetime.now().isoformat(),
                    'success': False
                }
            
            content = response.content[0].text if hasattr(response.content[0], 'text') else str(response.content[0])
            
            # JSON ì¶”ì¶œ ì‹œë„ (ê°•í™”ëœ íŒŒì‹±)
            import re
            
            try:
                # 1. ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                # 2. JSON ê°ì²´ ì¶”ì¶œ (ì •ê·œì‹)
                json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                if json_match:
                    content = json_match.group(0)
                
                parsed = json.loads(content)
            except:
                parsed = {'raw': content, 'parse_error': True}
            
            cost = self._calculate_cost(model, 
                                       response.usage.input_tokens,
                                       response.usage.output_tokens)
            quality = self._evaluate_quality(parsed, scenario.get('expected', {}), scenario['phase'])
            
            return {
                'provider': 'claude',
                'model': model,
                'api_model': api_model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'response': parsed,
                'quality_score': quality,
                'tokens': {
                    'input': response.usage.input_tokens,
                    'output': response.usage.output_tokens,
                    'total': response.usage.input_tokens + response.usage.output_tokens
                },
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'parameters': {'temperature': temperature},  # ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„° ê¸°ë¡
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            return {
                'provider': 'claude',
                'model': model,
                'scenario_id': scenario['id'],
                'error': str(e),
                'elapsed_seconds': round(time.time() - start_time, 2),
                'success': False
            }
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        if model not in self.pricing:
            return 0.0
        rates = self.pricing[model]
        return round((input_tokens / 1_000_000 * rates['input'] + 
                     output_tokens / 1_000_000 * rates['output']), 6)
    
    def _evaluate_quality(self, response: Dict, expected: Dict, phase: int = 0) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€ (Phaseë³„ ì°¨ë³„í™”, 0-100ì )"""
        score = {
            'has_value': 'value' in response,
            'has_confidence': 'confidence' in response,
            'has_reasoning': 'reasoning' in response,
            'json_valid': 'parse_error' not in response,
            'value_in_range': False,
            'confidence_sufficient': False
        }
        
        # ê°’ ë²”ìœ„ ê²€ì¦
        if score['has_value'] and 'value_range' in expected:
            value = response.get('value')
            if isinstance(value, (int, float)):
                min_val, max_val = expected['value_range']
                score['value_in_range'] = min_val <= value <= max_val
        elif score['has_value'] and 'value' in expected:
            score['value_in_range'] = response.get('value') == expected['value']
        
        # confidence ê²€ì¦ (Phaseë³„ ì°¨ë³„í™”)
        if score['has_confidence']:
            if phase <= 2:  # Phase 0-2: ì •í™•íˆ 1.0ì´ì–´ì•¼ í•¨
                conf = response.get('confidence')
                score['confidence_sufficient'] = (conf == 1.0 or conf == 1)
            elif 'confidence_min' in expected:  # Phase 3-4: ìµœì†Œê°’ ì´ìƒ
                score['confidence_sufficient'] = response.get('confidence', 0) >= expected['confidence_min']
        
        # Phaseë³„ ì ìˆ˜ ì²´ê³„
        total = 0
        
        if phase <= 2:  # Phase 0-2: ê²°ì •ë¡ ì  ì‘ì—…
            # reasoning ë¶ˆí•„ìš”, ì •í™•ì„± ì¤‘ì‹œ
            if score['json_valid']: total += 25        # JSON íŒŒì‹± ì„±ê³µ
            if score['has_value']: total += 25         # value í•„ë“œ ì¡´ì¬
            if score['has_confidence']: total += 20    # confidence í•„ë“œ ì¡´ì¬
            if score['value_in_range']: total += 20    # ê°’ì´ ì •í™•í•¨
            if score['confidence_sufficient']: total += 10  # confidence=1.0
        else:  # Phase 3-4: ì¶”ë¡  ì‘ì—…
            # reasoning í•„ìˆ˜, ì¶”ë¡  í’ˆì§ˆ ì¤‘ì‹œ
            if score['json_valid']: total += 20        # JSON íŒŒì‹± ì„±ê³µ
            if score['has_value']: total += 20         # value í•„ë“œ ì¡´ì¬
            if score['has_confidence']: total += 15    # confidence í•„ë“œ ì¡´ì¬
            if score['has_reasoning']: total += 15     # reasoning í•„ë“œ ì¡´ì¬
            if score['value_in_range']: total += 20    # ê°’ì´ í•©ë¦¬ì  ë²”ìœ„
            if score['confidence_sufficient']: total += 10  # confidenceê°€ ì¶©ë¶„íˆ ë†’ìŒ
        
        score['total_score'] = total
        score['phase'] = phase
        return score
    
    def run_benchmark(self, category_filter: Optional[List[str]] = None):
        """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        scenarios = self.get_test_scenarios()
        
        print(f"\nğŸš€ ì¢…í•© LLM ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print(f"   ì‹œë‚˜ë¦¬ì˜¤: {len(scenarios)}ê°œ")
        
        # í•„í„°ë§ëœ ëª¨ë¸ ì¹´í…Œê³ ë¦¬
        test_categories = category_filter or list(self.models.keys())
        total_models = sum(len(self.models[cat]) for cat in test_categories if cat in self.models)
        print(f"   ëª¨ë¸: {total_models}ê°œ")
        print()
        
        for scenario_idx, scenario in enumerate(scenarios, 1):
            print(f"\n{'='*100}")
            print(f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx}/{len(scenarios)}: {scenario['name']}")
            print(f"{'='*100}")
            
            for category in test_categories:
                if category not in self.models:
                    continue
                
                print(f"\nğŸ“¦ {category}")
                models = self.models[category]
                
                for model in models:
                    try:
                        if 'claude' in category:
                            result = self.test_claude_model(model, scenario)
                        else:
                            result = self.test_openai_model(model, scenario)
                        
                        self.results.append(result)
                        self._print_result(result)
                        
                        # Rate limiting: ëª¨ë¸ë³„ ì°¨ë³„í™”ëœ ëŒ€ê¸° ì‹œê°„
                        if model.startswith(('o1', 'o3', 'o4')):  # thinking ëª¨ë¸
                            time.sleep(3)
                        elif 'claude' in category:  # Claude ëª¨ë¸
                            time.sleep(2)
                        else:  # ì¼ë°˜ ëª¨ë¸
                            time.sleep(1.5)
                    
                    except Exception as e:
                        print(f"   âŒ {model}: {str(e)[:100]}")
                        self.results.append({
                            'model': model,
                            'scenario_id': scenario['id'],
                            'error': str(e),
                            'success': False
                        })
                        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ê¸´ ëŒ€ê¸°
                        time.sleep(3)
        
        # ê²°ê³¼ ì €ì¥
        output_file = f"benchmark_comprehensive_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.save_results(output_file)
        self.generate_report()
    
    def _print_result(self, result: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        if not result['success']:
            print(f"   âŒ {result['model']}: {result.get('error', '')[:80]}")
            return
        
        quality = result['quality_score']['total_score']
        
        print(f"   âœ… {result['model']}")
        print(f"      ë¹„ìš©: ${result['cost']:.6f} | ì‹œê°„: {result['elapsed_seconds']}ì´ˆ | í’ˆì§ˆ: {quality}/100")
        
        if 'value' in result['response']:
            print(f"      ë‹µë³€: {result['response'].get('value')} {result['response'].get('unit', '')}")
    
    def save_results(self, filename: str):
        """ê²°ê³¼ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_tests': len(self.results),
                    'success_count': sum(1 for r in self.results if r['success'])
                },
                'results': self.results
            }, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {filename}")
    
    def generate_report(self):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        success = [r for r in self.results if r['success']]
        
        print(f"\n{'='*100}")
        print("ğŸ“Š ì¢…í•© ë¦¬í¬íŠ¸")
        print(f"{'='*100}\n")
        print(f"ì´ í…ŒìŠ¤íŠ¸: {len(self.results)}ê°œ")
        print(f"ì„±ê³µ: {len(success)}ê°œ ({len(success)/len(self.results)*100:.1f}%)")
        
        # ëª¨ë¸ë³„ í‰ê· 
        from collections import defaultdict
        stats = defaultdict(lambda: {'costs': [], 'quality': [], 'times': []})
        
        for r in success:
            model = r['model']
            stats[model]['costs'].append(r['cost'])
            stats[model]['quality'].append(r['quality_score']['total_score'])
            stats[model]['times'].append(r['elapsed_seconds'])
        
        # ê°€ì„±ë¹„ ê³„ì‚°
        model_avg = []
        for model, data in stats.items():
            avg_cost = sum(data['costs']) / len(data['costs'])
            avg_quality = sum(data['quality']) / len(data['quality'])
            avg_time = sum(data['times']) / len(data['times'])
            efficiency = avg_quality / (avg_cost * 1000) if avg_cost > 0 else 0
            
            model_avg.append({
                'model': model,
                'avg_cost': avg_cost,
                'avg_quality': avg_quality,
                'avg_time': avg_time,
                'efficiency': efficiency,
                'count': len(data['costs'])
            })
        
        # ê°€ì„±ë¹„ ìˆœ
        model_avg.sort(key=lambda x: x['efficiency'], reverse=True)
        
        print(f"\nğŸ† ìµœê³  ê°€ì„±ë¹„ TOP 10:")
        for idx, m in enumerate(model_avg[:10], 1):
            print(f"   {idx:2d}. {m['model']:30s} | ê°€ì„±ë¹„: {m['efficiency']:7.1f} | í’ˆì§ˆ: {m['avg_quality']:5.1f} | ë¹„ìš©: ${m['avg_cost']:.6f}")


def main():
    """ë©”ì¸"""
    print("="*100)
    print("ì¢…í•© LLM ë²¤ì¹˜ë§ˆí¬ (2025-11-21)")
    print("="*100)
    
    # API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY') or not os.getenv('ANTHROPIC_API_KEY'):
        print("âŒ API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\nâœ… API í‚¤ í™•ì¸ ì™„ë£Œ")
    print("\ní…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. ì „ì²´ ëª¨ë¸ (ëŠë¦¼, ë¹„ìŒˆ, ~20-30ë¶„)")
    print("2. í•µì‹¬ ëª¨ë¸ë§Œ (ê¶Œì¥, ~10ë¶„)")
    print("3. nano/minië§Œ (ë¹ ë¦„, ì €ë ´, ~5ë¶„)")
    print("4. thinking ëª¨ë¸ë§Œ (ì¤‘ê°„, ~8ë¶„)")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    if choice == '2':
        categories = ['openai_mini', 'openai_standard', 'openai_thinking', 'claude_standard']
    elif choice == '3':
        categories = ['openai_nano', 'openai_mini']
    elif choice == '4':
        categories = ['openai_thinking', 'openai_thinking_pro']
    else:
        categories = None
    
    try:
        benchmark.run_benchmark(category_filter=categories)
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
        if benchmark.results:
            benchmark.save_results('benchmark_partial.json')
    
    print("\nğŸ‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

