"""
Fermi Model Search Engine
v1.0 - 2025-11-05

Fermi ì¶”ì •ì˜ ë³¸ì§ˆ êµ¬í˜„: "ë…¼ë¦¬ì˜ í¼ì¦ ë§ì¶”ê¸°"
- ëª¨í˜• ë§Œë“¤ê¸° (Model Building)
- Bottom-up âŸ· Top-down ë°˜ë³µ
- ì¬ê·€ êµ¬ì¡° (ë³€ìˆ˜ë„ Guestimation ëŒ€ìƒ, max depth 4)

ê¸°ë°˜: config/fermi_model_search.yaml
"""

from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any, Tuple
from enum import Enum
import re
import os


@dataclass
class FermiVariable:
    """ëª¨í˜•ì˜ ë³€ìˆ˜"""
    name: str
    value: Optional[float] = None
    available: bool = False
    source: Optional[str] = None  # "project", "recursive", "estimated"
    confidence: float = 0.0
    depth: int = 0  # ì¬ê·€ ê¹Šì´


@dataclass
class FermiModel:
    """Fermi ëª¨í˜•"""
    id: str
    formula: str  # "market = customers Ã— rate Ã— arpu Ã— 12"
    description: str
    variables: List[FermiVariable]
    
    def variable_count(self) -> int:
        """ë³€ìˆ˜ ê°œìˆ˜ (ìƒìˆ˜ ì œì™¸)"""
        return len([v for v in self.variables if not v.available or v.value is None])
    
    def unknown_count(self) -> int:
        """Unknown ë³€ìˆ˜ ê°œìˆ˜"""
        return len([v for v in self.variables if not v.available])
    
    def is_feasible(self) -> bool:
        """ëª¨ë“  ë³€ìˆ˜ê°€ ì±„ì›Œì¡ŒëŠ”ê°€?"""
        return all(v.value is not None for v in self.variables)
    
    def calculate(self) -> float:
        """ëª¨í˜• ê³„ì‚° (eval)"""
        # ë³€ìˆ˜ëª… â†’ ê°’ ë§¤í•‘
        context = {v.name: v.value for v in self.variables}
        
        # ìˆ˜ì‹ íŒŒì‹± (= ì œê±°)
        formula = self.formula
        if '=' in formula:
            # "result = A Ã— B" â†’ "A Ã— B"
            formula = formula.split('=')[1].strip()
        
        # ìˆ˜ì‹ í‰ê°€
        try:
            # Ã— â†’ * ë³€í™˜
            formula = formula.replace('Ã—', '*').replace('Ã·', '/')
            result = eval(formula, {"__builtins__": {}}, context)
            return float(result)
        except Exception as e:
            # ë‹¨ì¼ ë³€ìˆ˜ì¸ ê²½ìš°
            if formula in context:
                return float(context[formula])
            raise ValueError(f"ëª¨í˜• ê³„ì‚° ì‹¤íŒ¨: {e}\nFormula: {formula}\nContext: {context}")


@dataclass
class FermiResult:
    """Fermi ì¶”ì • ê²°ê³¼"""
    question: str
    value: Optional[float] = None
    
    # Fermi í•µì‹¬!
    model: Optional[FermiModel] = None
    components: List[FermiVariable] = field(default_factory=list)
    calculation_steps: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    confidence: float = 0.0
    max_depth_used: int = 0
    total_models_tried: int = 0
    selection_reason: str = ""
    
    # ì¶”ì 
    logic_trace: List[str] = field(default_factory=list)
    alternative_models: List[Dict] = field(default_factory=list)


class FermiModelSearch:
    """
    Fermi Model Search Engine
    
    í•µì‹¬: ê°€ìš©í•œ ìˆ«ì(Bottom-up)ì™€ ê°œë… ë¶„í•´(Top-down)ë¥¼ ë°˜ë³µí•˜ë©°
         "ì±„ìš¸ ìˆ˜ ìˆëŠ” ëª¨í˜•" ì°¾ê¸° (ë…¼ë¦¬ì˜ í¼ì¦)
    
    Usage:
        fermi = FermiModelSearch()
        result = fermi.estimate("ìŒì‹ì  SaaS ì‹œì¥ ê·œëª¨ëŠ”?")
    """
    
    MAX_DEPTH = 4
    MAX_VARIABLES = 6
    
    def __init__(self, project_context: Optional[Dict] = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            project_context: í”„ë¡œì íŠ¸ ë°ì´í„° (í™•ì •ëœ ê°’ë“¤)
        """
        self.project_context = project_context or {}
        self.call_stack = []  # ìˆœí™˜ ê°ì§€ìš©
        
        # LLM ëª¨ë“œ
        import umis_rag
        self.llm_mode = umis_rag.LLM_MODE
    
    def estimate(
        self,
        question: str,
        depth: int = 0
    ) -> FermiResult:
        """
        Fermi ì¶”ì • (ì¬ê·€ í•¨ìˆ˜)
        
        Args:
            question: ì¶”ì • ì§ˆë¬¸
            depth: ì¬ê·€ ê¹Šì´ (0-4)
        
        Returns:
            FermiResult
        """
        
        result = FermiResult(question=question)
        result.logic_trace.append(f"[Depth {depth}] ì§ˆë¬¸: {question}")
        
        # Base Case 1: Depth í•œê³„
        if depth >= self.MAX_DEPTH:
            result.logic_trace.append(f"âš ï¸ Depth {depth} ë„ë‹¬ â†’ ì¶”ì •ê°’ ì‚¬ìš©")
            result.value = self._get_estimated_value(question)
            result.confidence = 0.4
            result.max_depth_used = depth
            return result
        
        # Base Case 2: ìˆœí™˜ ê°ì§€
        if self._detect_circular(question):
            result.logic_trace.append(f"ğŸš¨ ìˆœí™˜ ê°ì§€: {question} â†’ ì¤‘ë‹¨")
            result.value = self._get_estimated_value(question)
            result.confidence = 0.3
            return result
        
        # Call stack ì¶”ê°€
        self.call_stack.append(question)
        
        try:
            # Phase 1: ì´ˆê¸° ìŠ¤ìº”
            available, unknown = self._initial_scan(question)
            result.logic_trace.append(f"Phase 1: ê°€ìš© {len(available)}ê°œ, Unknown {len(unknown)}ê°œ")
            
            # Phase 2: ëª¨í˜• ìƒì„±
            models = self._generate_models(question, available, unknown)
            result.total_models_tried = len(models)
            result.logic_trace.append(f"Phase 2: {len(models)}ê°œ ëª¨í˜• ìƒì„±")
            
            if not models:
                # ëª¨í˜• ë¶ˆí•„ìš” (ë‹¨ìˆœ ì§ˆë¬¸)
                result.logic_trace.append("â†’ ë‹¨ìˆœ ì§ˆë¬¸ (ëª¨í˜• ë¶ˆí•„ìš”)")
                result.value = self._get_estimated_value(question)
                result.confidence = 0.5
                result.max_depth_used = depth
                return result
            
            # Phase 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬
            feasible_models = []
            for model in models:
                feasibility = self._check_feasibility(model, depth)
                
                if feasibility['feasible']:
                    feasible_models.append((model, feasibility))
            
            # Phase 4: ìµœì„  ëª¨í˜• ì‹¤í–‰
            if feasible_models:
                best_model, best_values = self._select_best_model(feasible_models)
                
                # ì¬ì¡°ë¦½
                result.model = best_model
                result.components = best_model.variables
                result.value = best_model.calculate()
                result.confidence = self._calculate_confidence(best_model.variables)
                result.max_depth_used = max((v.depth for v in best_model.variables), default=depth)
                result.selection_reason = f"Unknown {best_model.unknown_count()}ê°œ, ì ìˆ˜ ìµœê³ "
                
                # ê³„ì‚° ë‹¨ê³„ ê¸°ë¡
                result.calculation_steps = self._trace_calculation(best_model)
                result.logic_trace.append(f"Phase 4: {best_model.id} ì‹¤í–‰ â†’ {result.value}")
                
                return result
            else:
                # ëª¨ë“  ëª¨í˜• ì‹¤í–‰ ë¶ˆê°€
                result.logic_trace.append("âŒ ëª¨ë“  ëª¨í˜• ì‹¤í–‰ ë¶ˆê°€")
                result.value = self._get_estimated_value(question)
                result.confidence = 0.3
                result.max_depth_used = depth
                return result
        
        finally:
            # Call stack ì œê±°
            if self.call_stack and self.call_stack[-1] == question:
                self.call_stack.pop()
    
    # =========================================
    # Phase 1: ì´ˆê¸° ìŠ¤ìº”
    # =========================================
    
    def _initial_scan(self, question: str) -> Tuple[List[str], List[str]]:
        """
        ê°€ìš© ë°ì´í„° íŒŒì•…
        
        Returns:
            (available_data, unknown_data)
        """
        available = []
        unknown = []
        
        # Project context í™•ì¸
        for key in self.project_context.keys():
            available.append(key)
        
        # í–¥í›„: LLM Quick scan, ëª…ë°±í•œ ì¶œì²˜ ë“±
        
        return available, unknown
    
    # =========================================
    # Phase 2: ëª¨í˜• ìƒì„± (LLM)
    # =========================================
    
    def _generate_models(
        self,
        question: str,
        available: List[str],
        unknown: List[str]
    ) -> List[FermiModel]:
        """
        LLMìœ¼ë¡œ í›„ë³´ ëª¨í˜• ìƒì„±
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë°ì´í„°
            unknown: ëª¨ë¥´ëŠ” ë°ì´í„°
        
        Returns:
            3-5ê°œ í›„ë³´ ëª¨í˜•
        """
        
        # LLM ëª¨ë“œ ì²´í¬
        if self.llm_mode == 'cursor':
            # Cursor: ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
            print(f"\nğŸ’¡ [Fermi Phase 2] ëª¨í˜• ìƒì„± í•„ìš”")
            print(f"   ì§ˆë¬¸: {question}")
            print(f"   ê°€ìš© ë°ì´í„°: {available}")
            print(f"   â†’ Cursorì—ì„œ LLMì—ê²Œ ëª¨í˜• 3-5ê°œ ìš”ì²­í•˜ì„¸ìš”")
            print(f"   â†’ ê° ëª¨í˜•ì˜ formulaì™€ variablesë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            
            # í˜„ì¬: ê¸°ë³¸ ëª¨í˜• ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
            return self._get_default_models(question)
        
        else:
            # External LLM: OpenAI API í˜¸ì¶œ
            return self._generate_models_with_llm(question, available, unknown)
    
    def _generate_models_with_llm(
        self,
        question: str,
        available: List[str],
        unknown: List[str]
    ) -> List[FermiModel]:
        """LLM APIë¡œ ëª¨í˜• ìƒì„±"""
        
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            prompt = f"""ì§ˆë¬¸: {question}

ê°€ìš©í•œ ë°ì´í„°:
{available}

ì„ë¬´:
1. ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ê³„ì‚° ëª¨í˜•ì„ 3-5ê°œ ì œì‹œí•˜ì„¸ìš”.
2. ê° ëª¨í˜•ì€ ë‹¤ë¥¸ ë¶„í•´ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ê°€ìš© ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”.
4. Unknown ë³€ìˆ˜ëŠ” ìµœì†Œí™”í•˜ì„¸ìš”.
5. ë³€ìˆ˜ëŠ” 2-6ê°œë¡œ ì œí•œí•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
Model 1:
  formula: "ëª©í‘œ = A Ã— B Ã— C"
  variables: ["A", "B", "C"]
  description: "ì„¤ëª…"

Model 2:
  ...
"""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1000
            )
            
            # íŒŒì‹± (ê°„ì†Œí™”)
            return self._parse_llm_models(response.choices[0].message.content)
        
        except Exception as e:
            print(f"âš ï¸ LLM ëª¨í˜• ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_default_models(question)
    
    def _get_default_models(self, question: str) -> List[FermiModel]:
        """
        ê¸°ë³¸ ëª¨í˜• í…œí”Œë¦¿ (LLM ì—†ì„ ë•Œ)
        
        ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œë³„ ì—¬ëŸ¬ ëª¨í˜• ì œê³µ
        """
        
        # ì‹œì¥ ê·œëª¨ (TAM, SAM, SOM)
        if "ì‹œì¥" in question or "TAM" in question or "SAM" in question:
            return [
                FermiModel(
                    id="MODEL_001",
                    formula="market = customers * adoption_rate * arpu * 12",
                    description="ì‹œì¥ = ê³ ê° ìˆ˜ Ã— ë„ì…ë¥  Ã— ARPU Ã— 12",
                    variables=[
                        FermiVariable(name="customers", available=False),
                        FermiVariable(name="adoption_rate", available=False),
                        FermiVariable(name="arpu", available=False),
                        FermiVariable(name="12", value=12, available=True, confidence=1.0),
                    ]
                ),
                FermiModel(
                    id="MODEL_002",
                    formula="market = customers * digital_rate * conversion_rate * arpu * 12",
                    description="ì‹œì¥ = ê³ ê° Ã— ë””ì§€í„¸ìœ¨ Ã— ì „í™˜ìœ¨ Ã— ARPU Ã— 12",
                    variables=[
                        FermiVariable(name="customers", available=False),
                        FermiVariable(name="digital_rate", available=False),
                        FermiVariable(name="conversion_rate", available=False),
                        FermiVariable(name="arpu", available=False),
                        FermiVariable(name="12", value=12, available=True, confidence=1.0),
                    ]
                ),
            ]
        
        # LTV (ê³ ê° ìƒì•  ê°€ì¹˜)
        elif "LTV" in question or "ìƒì• ê°€ì¹˜" in question or "ê³ ê°ê°€ì¹˜" in question:
            return [
                FermiModel(
                    id="MODEL_LTV_001",
                    formula="ltv = arpu * (1 / churn)",
                    description="LTV = ARPU / Churn Rate",
                    variables=[
                        FermiVariable(name="arpu", available=False),
                        FermiVariable(name="churn", available=False),
                    ]
                ),
                FermiModel(
                    id="MODEL_LTV_002",
                    formula="ltv = arpu * average_lifetime",
                    description="LTV = ARPU Ã— í‰ê·  ìƒì•  (ê°œì›”)",
                    variables=[
                        FermiVariable(name="arpu", available=False),
                        FermiVariable(name="average_lifetime", available=False),
                    ]
                ),
            ]
        
        # CAC (ê³ ê° íšë“ ë¹„ìš©)
        elif "CAC" in question or "íšë“.*ë¹„ìš©" in question or "íšë“.*ë‹¨ê°€" in question:
            return [
                FermiModel(
                    id="MODEL_CAC_001",
                    formula="cac = marketing_cost / new_customers",
                    description="CAC = ë§ˆì¼€íŒ… ë¹„ìš© / ì‹ ê·œ ê³ ê°",
                    variables=[
                        FermiVariable(name="marketing_cost", available=False),
                        FermiVariable(name="new_customers", available=False),
                    ]
                ),
                FermiModel(
                    id="MODEL_CAC_002",
                    formula="cac = cpc * (1 / cvr)",
                    description="CAC = CPC / CVR",
                    variables=[
                        FermiVariable(name="cpc", available=False),
                        FermiVariable(name="cvr", available=False),
                    ]
                ),
            ]
        
        # Unit Economics (LTV/CAC)
        elif "LTV/CAC" in question or "Unit.*Economics" in question:
            return [
                FermiModel(
                    id="MODEL_UE_001",
                    formula="ratio = ltv / cac",
                    description="ë¹„ìœ¨ = LTV / CAC",
                    variables=[
                        FermiVariable(name="ltv", available=False),
                        FermiVariable(name="cac", available=False),
                    ]
                ),
            ]
        
        # Churn Rate (í•´ì§€ìœ¨)
        elif "churn" in question.lower() or "í•´ì§€ìœ¨" in question or "ì´íƒˆë¥ " in question:
            return [
                FermiModel(
                    id="MODEL_CHURN_001",
                    formula="churn = churned / total",
                    description="Churn = í•´ì§€ ê³ ê° / ì „ì²´ ê³ ê°",
                    variables=[
                        FermiVariable(name="churned", available=False),
                        FermiVariable(name="total", available=False),
                    ]
                ),
            ]
        
        # Conversion Rate (ì „í™˜ìœ¨)
        elif "ì „í™˜ìœ¨" in question or "conversion" in question.lower():
            return [
                FermiModel(
                    id="MODEL_CVR_001",
                    formula="cvr = converted / total",
                    description="ì „í™˜ìœ¨ = ì „í™˜ ê³ ê° / ì „ì²´ ë°©ë¬¸ì",
                    variables=[
                        FermiVariable(name="converted", available=False),
                        FermiVariable(name="total", available=False),
                    ]
                ),
            ]
        
        # ARPU (ìƒì„¸ ë¶„í•´)
        elif "ARPU" in question or "ê°ë‹¨ê°€" in question:
            return [
                FermiModel(
                    id="MODEL_ARPU_001",
                    formula="arpu = base_fee + extra_fee",
                    description="ARPU = ê¸°ë³¸ë£Œ + ì¶”ê°€ë£Œ",
                    variables=[
                        FermiVariable(name="base_fee", available=False),
                        FermiVariable(name="extra_fee", available=False),
                    ]
                ),
                FermiModel(
                    id="MODEL_ARPU_002",
                    formula="arpu = (tier1_price * tier1_ratio) + (tier2_price * tier2_ratio)",
                    description="ARPU = Tierë³„ ê°€ì¤‘ í‰ê· ",
                    variables=[
                        FermiVariable(name="tier1_price", available=False),
                        FermiVariable(name="tier1_ratio", available=False),
                        FermiVariable(name="tier2_price", available=False),
                        FermiVariable(name="tier2_ratio", available=False),
                    ]
                ),
            ]
        
        # ì„±ì¥ë¥ 
        elif "ì„±ì¥ë¥ " in question or "growth" in question.lower():
            return [
                FermiModel(
                    id="MODEL_GROWTH_001",
                    formula="growth = (this_year - last_year) / last_year",
                    description="ì„±ì¥ë¥  = (ì˜¬í•´ - ì‘ë…„) / ì‘ë…„",
                    variables=[
                        FermiVariable(name="this_year", available=False),
                        FermiVariable(name="last_year", available=False),
                    ]
                ),
            ]
        
        # ê¸°ë³¸ ëª¨í˜• (ë‹¨ìˆœ ì§ˆë¬¸ - ëª¨í˜• ë¶ˆí•„ìš”)
        else:
            return []
    
    def _parse_llm_models(self, llm_response: str) -> List[FermiModel]:
        """LLM ì‘ë‹µ íŒŒì‹± (ê°„ì†Œí™”)"""
        # ì‹¤ì œ êµ¬í˜„ì€ ì •êµí•œ íŒŒì‹± í•„ìš”
        return self._get_default_models("")
    
    def _extract_var_name(self, question: str) -> str:
        """ì§ˆë¬¸ì—ì„œ ë³€ìˆ˜ëª… ì¶”ì¶œ"""
        # "ARPUëŠ”?" â†’ "arpu"
        # "ê³ ê° ìˆ˜ëŠ”?" â†’ "customers"
        
        if "ARPU" in question:
            return "arpu"
        elif "ê³ ê°" in question:
            return "customers"
        elif "churn" in question.lower() or "í•´ì§€" in question:
            return "churn"
        else:
            return "value"
    
    # =========================================
    # Phase 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬
    # =========================================
    
    def _check_feasibility(
        self,
        model: FermiModel,
        parent_depth: int
    ) -> Dict[str, Any]:
        """
        ëª¨í˜• ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬
        
        ê° ë³€ìˆ˜ë¥¼ ì±„ìš¸ ìˆ˜ ìˆëŠ”ì§€ í™•ì¸ (ì¬ê·€)
        
        Returns:
            {
                'feasible': bool,
                'filled_values': Dict,
                'max_depth': int
            }
        """
        
        filled_values = {}
        max_depth = parent_depth
        
        for var in model.variables:
            # Available â†’ ì¦‰ì‹œ ì‚¬ìš©
            if var.available and var.value is not None:
                filled_values[var.name] = var.value
                var.source = "available"
                var.depth = parent_depth
                continue
            
            # Project context í™•ì¸
            if var.name in self.project_context:
                var.value = self.project_context[var.name]
                var.available = True
                var.source = "project"
                var.confidence = 1.0
                var.depth = parent_depth
                filled_values[var.name] = var.value
                continue
            
            # Unknown â†’ ì¦‰ì‹œ ì¬ê·€ í˜¸ì¶œ!
            if parent_depth + 1 < self.MAX_DEPTH:
                # ì¬ê·€ í˜¸ì¶œ
                recursive_question = f"{var.name}ì€(ëŠ”)?"
                recursive_result = self.estimate(
                    recursive_question,
                    depth=parent_depth + 1
                )
                
                if recursive_result.value is not None:
                    var.value = recursive_result.value
                    var.available = True
                    var.source = "recursive"
                    var.confidence = recursive_result.confidence
                    var.depth = recursive_result.max_depth_used
                    filled_values[var.name] = var.value
                    max_depth = max(max_depth, recursive_result.max_depth_used)
                else:
                    # ì¬ê·€ë„ ì‹¤íŒ¨
                    var.value = self._get_estimated_value(recursive_question)
                    var.available = True
                    var.source = "estimated"
                    var.confidence = 0.3
                    var.depth = parent_depth + 1
                    filled_values[var.name] = var.value
            else:
                # Depth í•œê³„ â†’ ì¶”ì •ê°’
                var.value = self._get_estimated_value(f"{var.name}ì€?")
                var.available = True
                var.source = "estimated"
                var.confidence = 0.3
                var.depth = parent_depth + 1
                filled_values[var.name] = var.value
        
        return {
            'feasible': len(filled_values) == len(model.variables),
            'filled_values': filled_values,
            'max_depth': max_depth
        }
    
    def _select_best_model(
        self,
        feasible_models: List[Tuple[FermiModel, Dict]]
    ) -> Tuple[FermiModel, Dict]:
        """
        ìµœì„ ì˜ ëª¨í˜• ì„ íƒ
        
        ê¸°ì¤€:
        1. Unknown ê°œìˆ˜ (50%)
        2. Confidence (30%)
        3. ë³µì¡ë„ (20%)
        4. Depth (10% ë³´ë„ˆìŠ¤)
        """
        
        scored = []
        
        for model, feasibility in feasible_models:
            # ì ìˆ˜ ê³„ì‚°
            unknown_score = self._score_unknown(model)
            confidence_score = self._calculate_confidence(model.variables)
            complexity_score = self._score_complexity(model.variable_count())
            depth_score = self._score_depth(feasibility['max_depth'])
            
            total_score = (
                unknown_score * 0.5 +
                confidence_score * 0.3 +
                complexity_score * 0.2 +
                depth_score * 0.1
            )
            
            scored.append((model, feasibility, total_score))
        
        # ìµœê³  ì ìˆ˜ ì„ íƒ
        scored.sort(key=lambda x: x[2], reverse=True)
        best = scored[0]
        
        return best[0], best[1]
    
    def _score_unknown(self, model: FermiModel) -> float:
        """Unknown ê°œìˆ˜ ì ìˆ˜"""
        unknown = model.unknown_count()
        total = model.variable_count()
        
        if total == 0:
            return 1.0
        
        return (total - unknown) / total
    
    def _score_complexity(self, var_count: int) -> float:
        """ë³µì¡ë„ ì ìˆ˜ (2-6ê°œ)"""
        scores = {
            1: 1.0,
            2: 1.0,
            3: 0.9,
            4: 0.7,
            5: 0.5,
            6: 0.3,
        }
        return scores.get(var_count, 0.0)
    
    def _score_depth(self, depth: int) -> float:
        """Depth ì ìˆ˜"""
        scores = {
            0: 1.0,
            1: 0.8,
            2: 0.6,
            3: 0.4,
            4: 0.2,
        }
        return scores.get(depth, 0.0)
    
    def _calculate_confidence(self, variables: List[FermiVariable]) -> float:
        """ë³€ìˆ˜ë“¤ì˜ confidence ì¡°í•© (geometric mean)"""
        if not variables:
            return 0.0
        
        confidences = [v.confidence for v in variables if v.confidence > 0]
        
        if not confidences:
            return 0.5
        
        # Geometric mean
        product = 1.0
        for c in confidences:
            product *= c
        
        return product ** (1 / len(confidences))
    
    def _trace_calculation(self, model: FermiModel) -> List[str]:
        """ê³„ì‚° ë‹¨ê³„ ì¶”ì """
        steps = []
        steps.append(f"ëª¨í˜•: {model.formula}")
        
        for var in model.variables:
            steps.append(f"  {var.name} = {var.value} ({var.source})")
        
        result = model.calculate()
        steps.append(f"ê²°ê³¼: {result}")
        
        return steps
    
    # =========================================
    # ìœ í‹¸ë¦¬í‹°
    # =========================================
    
    def _detect_circular(self, question: str) -> bool:
        """ìˆœí™˜ ì˜ì¡´ì„± ê°ì§€"""
        return question in self.call_stack
    
    def _get_estimated_value(self, question: str) -> float:
        """
        ì¶”ì •ê°’ ë°˜í™˜ (Fallback)
        
        í–¥í›„: Layer 6 (í†µê³„ ê¸°ë³¸ê°’) ë“± í™œìš©
        í˜„ì¬: ê°„ë‹¨í•œ ê¸°ë³¸ê°’
        """
        
        # ê°„ë‹¨í•œ ê¸°ë³¸ê°’ (ì—…ê³„ í‰ê· )
        if "arpu" in question.lower():
            return 80000  # 8ë§Œì›
        elif "churn" in question.lower() or "í•´ì§€" in question:
            return 0.05  # 5%
        elif "ì „í™˜" in question or "conversion" in question.lower():
            return 0.10  # 10%
        elif "ë„ì…" in question:
            return 0.20  # 20%
        elif "ê³ ê°" in question or "customers" in question.lower():
            return 100000  # 10ë§Œ
        else:
            return 1.0  # ê¸°ë³¸ê°’


# =========================================
# í¸ì˜ í•¨ìˆ˜
# =========================================

def fermi_estimate(
    question: str,
    project_context: Optional[Dict] = None
) -> FermiResult:
    """
    ë¹ ë¥¸ Fermi ì¶”ì •
    
    Usage:
        result = fermi_estimate("ìŒì‹ì  SaaS ì‹œì¥ì€?")
    """
    fermi = FermiModelSearch(project_context=project_context)
    return fermi.estimate(question)

