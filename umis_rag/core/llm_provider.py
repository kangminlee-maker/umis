"""
LLM Provider Module for UMIS RAG System

UMIS ì „ì—­ ì„¤ì •(llm_mode)ì— ë”°ë¼ ì ì ˆí•œ LLM ì œê³µ:
- cursor: Cursor Agent LLM ì‚¬ìš© (ë¹„ìš© $0, RAGë§Œ ìˆ˜í–‰)
- gpt-4o-mini, o1-mini ë“±: External LLM API í˜¸ì¶œ (ì™„ì „ ìë™í™”)

í•µì‹¬ ì² í•™:
----------
Cursor ëª¨ë“œëŠ” "RAG ê²€ìƒ‰ë§Œ ìˆ˜í–‰ â†’ Cursor LLMì´ ë¶„ì„"
External LLM ëª¨ë“œëŠ” "RAG ê²€ìƒ‰ + API í˜¸ì¶œ â†’ ì™„ì„±ëœ ê²°ê³¼"

v7.8.1 ë³€ê²½ (2025-11-25): umis_mode â†’ llm_mode, native/external â†’ ì§ì ‘ ëª¨ë¸ëª…
v7.7.0 ì‹ ê·œ ì¶”ê°€ (2025-11-10)
"""

from typing import Optional, Any, Dict
from pathlib import Path

from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel

import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.core.config import settings
from umis_rag.utils.logger import logger


class LLMProvider:
    """
    UMIS LLM Provider

    ì—­í• :
    -----
    llm_mode ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ LLM ê°ì²´ ìƒì„±

    ì‚¬ìš© ì˜ˆì‹œ:
    ---------
    ```python
    # Agentì—ì„œ ì‚¬ìš©
    from umis_rag.core.llm_provider import LLMProvider

    class ExplorerRAG:
        def __init__(self):
            self.llm = LLMProvider.create_llm()
            self.mode = settings.llm_mode

        def generate_hypothesis(self, ...):
            if self.mode == "cursor":
                # RAG ê²€ìƒ‰ë§Œ ìˆ˜í–‰
                return self._prepare_for_cursor(rag_results)
            else:
                # API í˜¸ì¶œ
                return self._call_llm_api(rag_results)
    ```

    ëª¨ë“œë³„ ë™ì‘:
    -----------
    Cursor Mode (llm_mode='cursor'):
        - LLM ê°ì²´ ìƒì„±í•˜ì§€ ì•ŠìŒ (None ë°˜í™˜)
        - AgentëŠ” RAG ê²€ìƒ‰ë§Œ ìˆ˜í–‰
        - ê²°ê³¼ë¥¼ Cursor Composer/Chatì— ì „ë‹¬
        - Cursor LLMì´ ì§ì ‘ ë¶„ì„ ìˆ˜í–‰
        - ë¹„ìš©: $0 (Cursor êµ¬ë…ì— í¬í•¨)

    External LLM Mode (llm_mode='gpt-4o-mini', 'o1-mini' ë“±):
        - ChatOpenAI ê°ì²´ ìƒì„±
        - Agentê°€ RAG + API í˜¸ì¶œê¹Œì§€ ì™„ë£Œ
        - ì™„ì„±ëœ ê²°ê³¼ ë°˜í™˜
        - ë¹„ìš©: í† í°ë‹¹ ê³¼ê¸ˆ ($0.01-0.10/ìš”ì²­)
    """

    @staticmethod
    def create_llm() -> Optional[BaseChatModel]:
        """
        llm_modeì— ë”°ë¼ LLM ê°ì²´ ìƒì„±

        Returns:
        --------
        - None: Cursor ëª¨ë“œ (LLM ì‚¬ìš© ì•ˆ í•¨, RAGë§Œ)
        - ChatOpenAI: External LLM ëª¨ë“œ (API í˜¸ì¶œ)

        Raises:
        -------
        ValueError: ì•Œ ìˆ˜ ì—†ëŠ” llm_mode ê°’
        """
        mode = settings.llm_mode.lower()

        if mode == "cursor":
            logger.info("ğŸ¯ Cursor ëª¨ë“œ: LLM ê°ì²´ ìƒì„± ì•ˆ í•¨ (Cursorê°€ ì§ì ‘ ì²˜ë¦¬)")
            return None

        else:
            # External LLM (gpt-4o-mini, o1-mini ë“±)
            logger.info(f"ğŸŒ External LLM ëª¨ë“œ: OpenAI API ì‚¬ìš© (ëª¨ë¸: {settings.llm_model})")
            return ChatOpenAI(
                model=settings.llm_model,
                temperature=settings.llm_temperature,
                openai_api_key=settings.openai_api_key,
                max_tokens=settings.llm_max_tokens
            )

    @staticmethod
    def is_cursor_mode() -> bool:
        """
        Cursor ëª¨ë“œ ì—¬ë¶€ í™•ì¸

        Returns:
        --------
        True: Cursor ëª¨ë“œ (Cursor LLM ì‚¬ìš©)
        False: External LLM ëª¨ë“œ (API í˜¸ì¶œ)
        """
        return settings.llm_mode.lower() == "cursor"

    @staticmethod
    def is_external_mode() -> bool:
        """
        External LLM ëª¨ë“œ ì—¬ë¶€ í™•ì¸

        Returns:
        --------
        True: External LLM ëª¨ë“œ (API í˜¸ì¶œ)
        False: Cursor ëª¨ë“œ (Cursor LLM ì‚¬ìš©)
        """
        return settings.llm_mode.lower() != "cursor"

    @staticmethod
    def get_mode_info() -> Dict[str, Any]:
        """
        í˜„ì¬ ëª¨ë“œ ì •ë³´ ë°˜í™˜

        Returns:
        --------
        Dict with keys:
            - mode: 'cursor' or model name (e.g., 'gpt-4o-mini')
            - uses_api: bool
            - cost: str (ë¹„ìš© ì„¤ëª…)
            - automation: bool (ìë™í™” ê°€ëŠ¥ ì—¬ë¶€)
        """
        mode = settings.llm_mode.lower()

        if mode == "cursor":
            return {
                "mode": "cursor",
                "uses_api": False,
                "cost": "$0 (Cursor êµ¬ë… í¬í•¨)",
                "automation": False,
                "description": "RAG ê²€ìƒ‰ë§Œ ìˆ˜í–‰ â†’ Cursor LLMì´ ë¶„ì„"
            }
        else:
            return {
                "mode": mode,
                "uses_api": True,
                "cost": f"í† í°ë‹¹ ê³¼ê¸ˆ (ëª¨ë¸: {settings.llm_model})",
                "automation": True,
                "description": "RAG ê²€ìƒ‰ + API í˜¸ì¶œ â†’ ì™„ì„±ëœ ê²°ê³¼"
            }


class CursorModeMixin:
    """
    Cursor ëª¨ë“œ í—¬í¼ Mixin

    Agentê°€ Cursor/External LLM ëª¨ë“œë¥¼ ì‰½ê²Œ ì²˜ë¦¬í•˜ë„ë¡ ë•ëŠ” ìœ í‹¸ë¦¬í‹°

    ì‚¬ìš© ì˜ˆì‹œ:
    ---------
    ```python
    class ExplorerRAG(CursorModeMixin):
        def generate_hypothesis(self, rag_results):
            if self.is_cursor():
                return self.prepare_cursor_output(
                    rag_results,
                    instruction="ìœ„ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°íšŒ ê°€ì„¤ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
                )
            else:
                # External LLM: API í˜¸ì¶œ
                return self._call_api(rag_results)
    ```
    """

    def is_cursor(self) -> bool:
        """Cursor ëª¨ë“œ ì—¬ë¶€"""
        return LLMProvider.is_cursor_mode()

    def is_external(self) -> bool:
        """External LLM ëª¨ë“œ ì—¬ë¶€"""
        return LLMProvider.is_external_mode()

    def prepare_cursor_output(
        self,
        rag_results: Any,
        instruction: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Cursor ëª¨ë“œìš© ì¶œë ¥ ì¤€ë¹„

        RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ Cursor LLMì´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ í¬ë§·íŒ…

        Parameters:
        -----------
        rag_results: RAG ê²€ìƒ‰ ê²°ê³¼
        instruction: Cursor LLMì—ê²Œ ì „ë‹¬í•  ì§€ì‹œì‚¬í•­
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
        --------
        Dict with keys:
            - mode: 'cursor'
            - rag_results: ê²€ìƒ‰ ê²°ê³¼
            - instruction: LLM ì§€ì‹œì‚¬í•­
            - metadata: ë©”íƒ€ë°ì´í„°
        """
        return {
            "mode": "cursor",
            "rag_results": rag_results,
            "instruction": instruction,
            "metadata": metadata or {},
            "next_step": "Cursor Composer/Chatì—ì„œ ìœ„ ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”."
        }


# ========================================
# ì‚¬ìš© ê°€ì´ë“œ
# ========================================

"""
Agent ìˆ˜ì • ê°€ì´ë“œ:
-----------------

1. LLM ì´ˆê¸°í™” ìˆ˜ì •
-------------------
# Before (í•­ìƒ ChatOpenAI)
self.llm = ChatOpenAI(
    model=settings.llm_model,
    temperature=settings.llm_temperature,
    openai_api_key=settings.openai_api_key
)

# After (ëª¨ë“œì— ë”°ë¼)
from umis_rag.core.llm_provider import LLMProvider

self.llm = LLMProvider.create_llm()
self.mode = settings.umis_mode


2. LLM í˜¸ì¶œ ë©”ì„œë“œ ìˆ˜ì •
-----------------------
# Before (ë¬´ì¡°ê±´ API í˜¸ì¶œ)
def generate_hypothesis(self, patterns, cases):
    prompt = ChatPromptTemplate.from_messages([...])
    chain = prompt | self.llm | StrOutputParser()
    return chain.invoke({...})

# After (ëª¨ë“œ ë¶„ê¸°)
def generate_hypothesis(self, patterns, cases):
    rag_results = self._prepare_rag_context(patterns, cases)

    if self.mode == "native":
        # Native: RAG ê²°ê³¼ë§Œ ë°˜í™˜
        return {
            'mode': 'native',
            'rag_results': rag_results,
            'instruction': 'ìœ„ íŒ¨í„´ê³¼ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°íšŒ ê°€ì„¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.'
        }
    else:
        # External: API í˜¸ì¶œ
        prompt = ChatPromptTemplate.from_messages([...])
        chain = prompt | self.llm | StrOutputParser()
        return chain.invoke({'context': rag_results})


3. ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© ì˜ˆì‹œ
--------------------
# Native ëª¨ë“œ (.env: UMIS_MODE=native)
python scripts/test_explorer.py

# ì¶œë ¥:
# {
#   'mode': 'native',
#   'rag_results': [...íŒ¨í„´ ê²€ìƒ‰ ê²°ê³¼...],
#   'instruction': 'ìœ„ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì„¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”.'
# }
#
# â†’ Cursor Composerì—ì„œ:
#   "@Explorer ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìŒì•… ìŠ¤íŠ¸ë¦¬ë° ì‹œì¥ ê¸°íšŒ 3ê°œ ì œì‹œ"

# External ëª¨ë“œ (.env: UMIS_MODE=external)
python scripts/test_explorer.py

# ì¶œë ¥:
# ê°€ì„¤ 1: êµ¬ë… ëª¨ë¸ ê¸°ë°˜ ìŒì•… í”Œë«í¼
# ...ì™„ì„±ëœ ê°€ì„¤...


4. ë¹„ìš© ë¹„êµ
-----------
Native Mode:
    - RAG ì„ë² ë”©: $0.0001
    - LLM í˜¸ì¶œ: $0 (Cursor)
    - í•©ê³„: $0

External Mode:
    - RAG ì„ë² ë”©: $0.0001
    - LLM í˜¸ì¶œ: $0.10
    - í•©ê³„: $0.10

100íšŒ ë¶„ì„:
    - Native: $0
    - External: $10
"""

