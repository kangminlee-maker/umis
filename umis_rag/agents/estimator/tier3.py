"""
Tier 3: Fermi Model Search

Ïû¨Í∑Ä Î∂ÑÌï¥ Ï∂îÏ†ï - ÎÖºÎ¶¨Ïùò ÌçºÏ¶ê ÎßûÏ∂îÍ∏∞

ÏÑ§Í≥Ñ: config/fermi_model_search.yaml (1,269Ï§Ñ)
ÏõêÎ¶¨: Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞(Bottom-up) + Í∞úÎÖê Î∂ÑÌï¥(Top-down) Î∞òÎ≥µ
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
import time
import math
import copy
from datetime import datetime

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.agents.estimator.models import (
    Context, EstimationResult, DecompositionTrace,
    ComponentEstimation, Tier3Config
)
from umis_rag.agents.estimator.tier2 import Tier2JudgmentPath
from umis_rag.utils.logger import logger
from umis_rag.core.config import settings

# LLM API
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False
    logger.warning("OpenAI Ìå®ÌÇ§ÏßÄ ÏóÜÏùå (pip install openai)")

import yaml
import re


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø - REMOVED (v7.5.0)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 
# v7.5.0 Î≥ÄÍ≤Ω:
# - ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú Í≥ÑÏÇ∞ Í≥µÏãùÏùÄ QuantifierÎ°ú Ïù¥Îèô
# - EstimatorÎäî ÏàúÏàò Í∞í Ï∂îÏ†ïÎßå Îã¥Îãπ
# - Tier 3Îäî ÏùºÎ∞òÏ†Å Fermi Î∂ÑÌï¥Ïóê ÏßëÏ§ë
#
# Ïù¥Ï†Ñ ÏúÑÏπò: tier3.py BUSINESS_METRIC_TEMPLATES
# Ïã†Í∑ú ÏúÑÏπò: data/raw/calculation_methodologies.yaml (Quantifier)
#
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# v7.5.0: ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Ï†úÍ±∞Îê®
# QuantifierÍ∞Ä LTV, CAC, ARPU Îì±Ïùò Í≥ÑÏÇ∞ Í≥µÏãù ÏÜåÏú†
BUSINESS_METRIC_TEMPLATES_REMOVED = {
    # Unit Economics (Ïö∞ÏÑ† - "ltv/cac" Ï†ïÌôï Îß§Ïπ≠)
    "unit_economics": {
        "keywords": ["unit economics", "ltv/cac", "ÎπÑÏú®", "ratio", "Í≤ΩÏ†úÏÑ±"],
        "models": [
            {
                "id": "UE_001",
                "formula": "ratio = ltv / cac",
                "description": "LTV/CAC ÎπÑÏú®",
                "variables": ["ltv", "cac"]
            }
        ]
    },
    
    # ÏãúÏû• Í∑úÎ™®
    "market_sizing": {
        "keywords": ["ÏãúÏû•", "Í∑úÎ™®", "TAM", "SAM", "market size"],
        "models": [
            {
                "id": "MARKET_001",
                "formula": "market = customers √ó adoption_rate √ó arpu √ó 12",
                "description": "Í∏∞ÏóÖ/Í≥†Í∞ù Ïàò Í∏∞Î∞ò ÏãúÏû• Í∑úÎ™®",
                "variables": ["customers", "adoption_rate", "arpu"]
            },
            {
                "id": "MARKET_002",
                "formula": "market = population √ó digital_rate √ó conversion_rate √ó arpu √ó 12",
                "description": "Ïù∏Íµ¨ Í∏∞Î∞ò ÎîîÏßÄÌÑ∏ Ï†ÑÌôò ÏãúÏû•",
                "variables": ["population", "digital_rate", "conversion_rate", "arpu"]
            }
        ]
    },
    
    # Í≥†Í∞ù ÏÉùÏï† Í∞ÄÏπò
    "ltv": {
        "keywords": ["ltv", "LTV", "ÏÉùÏï†Í∞ÄÏπò", "lifetime value"],
        "models": [
            {
                "id": "LTV_001",
                "formula": "ltv = arpu / churn_rate",
                "description": "ARPUÎ•º ChurnÏúºÎ°ú ÎÇòÎàà LTV",
                "variables": ["arpu", "churn_rate"]
            },
            {
                "id": "LTV_002",
                "formula": "ltv = arpu √ó average_lifetime_months",
                "description": "ÌèâÍ∑† ÏÉùÏï† Í∏∞Í∞Ñ Í∏∞Î∞ò LTV",
                "variables": ["arpu", "average_lifetime_months"]
            }
        ]
    },
    
    # Í≥†Í∞ù ÌöçÎìù ÎπÑÏö©
    "cac": {
        "keywords": ["cac", "CAC", "Í≥†Í∞ùÌöçÎìù", "customer acquisition"],
        "models": [
            {
                "id": "CAC_001",
                "formula": "cac = marketing_cost / new_customers",
                "description": "ÎßàÏºÄÌåÖ ÎπÑÏö©ÏùÑ Ïã†Í∑ú Í≥†Í∞ùÏúºÎ°ú ÎÇòÎàî",
                "variables": ["marketing_cost", "new_customers"]
            },
            {
                "id": "CAC_002",
                "formula": "cac = cpc / conversion_rate",
                "description": "CPCÎ•º Ï†ÑÌôòÏú®Î°ú ÎÇòÎàî",
                "variables": ["cpc", "conversion_rate"]
            }
        ]
    },
    
    # Ï†ÑÌôòÏú®
    "conversion": {
        "keywords": ["Ï†ÑÌôòÏú®", "conversion", "CVR"],
        "models": [
            {
                "id": "CVR_001",
                "formula": "conversion = paid_users / free_users",
                "description": "Ïú†Î£å Ï†ÑÌôòÏú® (Freemium)",
                "variables": ["paid_users", "free_users"]
            },
            {
                "id": "CVR_002",
                "formula": "conversion = industry_avg √ó product_quality_factor",
                "description": "ÏóÖÍ≥Ñ ÌèâÍ∑† Ï°∞Ï†ï",
                "variables": ["industry_avg", "product_quality_factor"]
            }
        ]
    },
    
    # Ìï¥ÏßÄÏú®
    "churn": {
        "keywords": ["churn", "Ìï¥ÏßÄÏú®", "Ïù¥ÌÉàÏú®"],
        "models": [
            {
                "id": "CHURN_001",
                "formula": "churn = churned_customers / total_customers",
                "description": "Ìï¥ÏßÄ Í≥†Í∞ù ÎπÑÏú®",
                "variables": ["churned_customers", "total_customers"]
            },
            {
                "id": "CHURN_002",
                "formula": "churn = 1 - retention_rate",
                "description": "Ïú†ÏßÄÏú®Ïùò Ïó≠Ïàò",
                "variables": ["retention_rate"]
            }
        ]
    },
    
    # ARPU
    "arpu": {
        "keywords": ["arpu", "ARPU", "ÌèâÍ∑†Îß§Ï∂ú", "average revenue"],
        "models": [
            {
                "id": "ARPU_001",
                "formula": "arpu = base_fee",
                "description": "Í∏∞Î≥∏Î£åÎßå",
                "variables": ["base_fee"]
            },
            {
                "id": "ARPU_002",
                "formula": "arpu = base_fee + overage_fee",
                "description": "Í∏∞Î≥∏Î£å + Ï¥àÍ≥ºÎ£å",
                "variables": ["base_fee", "overage_fee"]
            },
            {
                "id": "ARPU_003",
                "formula": "arpu = base_fee + usage_fee + addon_fee",
                "description": "Í∏∞Î≥∏Î£å + ÏÇ¨Ïö©ÎüâÎ£å + Ï∂îÍ∞ÄÍ∏∞Îä•Î£å",
                "variables": ["base_fee", "usage_fee", "addon_fee"]
            }
        ]
    },
    
    # ÏÑ±Ïû•Î•†
    "growth": {
        "keywords": ["ÏÑ±Ïû•Î•†", "growth rate", "CAGR"],
        "models": [
            {
                "id": "GROWTH_001",
                "formula": "growth = (current_year - last_year) / last_year",
                "description": "YoY ÏÑ±Ïû•Î•†",
                "variables": ["current_year", "last_year"]
            },
            {
                "id": "GROWTH_002",
                "formula": "growth = market_growth + market_share_change",
                "description": "ÏãúÏû• ÏÑ±Ïû• + Ï†êÏú†Ïú® Î≥ÄÌôî",
                "variables": ["market_growth", "market_share_change"]
            }
        ]
    },
    
    # Payback Period (v7.5.0)
    "payback": {
        "keywords": ["payback", "ÌöåÏàòÍ∏∞Í∞Ñ", "Ìà¨ÏûêÌöåÏàò"],
        "models": [
            {
                "id": "PAYBACK_001",
                "formula": "payback = cac / (arpu √ó gross_margin)",
                "description": "CACÎ•º Ïõî Í∏∞Ïó¨Ïù¥ÏùµÏúºÎ°ú ÎÇòÎàî",
                "variables": ["cac", "arpu", "gross_margin"]
            },
            {
                "id": "PAYBACK_002",
                "formula": "payback = initial_investment / monthly_profit",
                "description": "Ï¥àÍ∏∞ Ìà¨ÏûêÎ•º Ïõî ÏàòÏùµÏúºÎ°ú ÎÇòÎàî",
                "variables": ["initial_investment", "monthly_profit"]
            }
        ]
    },
    
    # Rule of 40 (v7.5.0)
    "rule_of_40": {
        "keywords": ["rule of 40", "40 Î≤ïÏπô"],
        "models": [
            {
                "id": "R40_001",
                "formula": "rule_40 = growth_rate + profit_margin",
                "description": "ÏÑ±Ïû•Î•† + Ïù¥ÏùµÎ•† (40% Ïù¥ÏÉÅÏù¥ Í±¥Í∞ï)",
                "variables": ["growth_rate", "profit_margin"]
            }
        ]
    },
    
    # Net Revenue Retention (v7.5.0)
    "nrr": {
        "keywords": ["nrr", "net revenue retention", "ÏàúÎß§Ï∂úÏú†ÏßÄÏú®"],
        "models": [
            {
                "id": "NRR_001",
                "formula": "nrr = (beginning_mrr + expansion - contraction - churn) / beginning_mrr",
                "description": "ÏàúÎß§Ï∂ú Ïú†ÏßÄÏú® (100% Ïù¥ÏÉÅÏù¥ Í±¥Í∞ï)",
                "variables": ["beginning_mrr", "expansion", "contraction", "churn"]
            },
            {
                "id": "NRR_002",
                "formula": "nrr = 1 + expansion_rate - churn_rate",
                "description": "ÌôïÏû•Î•† - Ìï¥ÏßÄÏú® + 1",
                "variables": ["expansion_rate", "churn_rate"]
            }
        ]
    },
    
    # Gross Margin (v7.5.0)
    "gross_margin": {
        "keywords": ["gross margin", "Îß§Ï∂úÏ¥ùÏù¥ÏùµÎ•†", "gross profit"],
        "models": [
            {
                "id": "GM_001",
                "formula": "gross_margin = (revenue - cogs) / revenue",
                "description": "Îß§Ï∂úÏ¥ùÏù¥ÏùµÎ•†",
                "variables": ["revenue", "cogs"]
            },
            {
                "id": "GM_002",
                "formula": "gross_margin = 1 - (cogs / revenue)",
                "description": "1 - COGS ÎπÑÏú®",
                "variables": ["cogs", "revenue"]
            }
        ]
    }
}


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ (Tier 3 Ï†ÑÏö©)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

@dataclass
class FermiVariable:
    """
    Fermi Î™®ÌòïÏùò Î≥ÄÏàò
    
    Attributes:
        name: Î≥ÄÏàò Ïù¥Î¶Ñ (Ïòà: "restaurants", "arpu")
        available: Í∞ÄÏö© Ïó¨Î∂Ä
        value: Í∞í (Ï±ÑÏõåÏßÑ Í≤ΩÏö∞)
        source: Ï∂úÏ≤ò ("project_data", "tier2", "recursive")
        confidence: Ïã†Î¢∞ÎèÑ
        need_estimate: Ï∂îÏ†ï ÌïÑÏöî Ïó¨Î∂Ä
        estimation_result: Ï∂îÏ†ï Í≤∞Í≥º (Ïû¨Í∑ÄÎ°ú Ï±ÑÏö¥ Í≤ΩÏö∞)
    """
    name: str
    available: bool
    value: Optional[float] = None
    source: str = ""
    confidence: float = 0.0
    need_estimate: bool = False
    uncertainty: float = 0.3
    
    # Ïû¨Í∑Ä Ï∂îÏ†ï Í≤∞Í≥º
    estimation_result: Optional[EstimationResult] = None


@dataclass
class FermiModel:
    """
    Fermi Ï∂îÏ†ï Î™®Ìòï
    
    Ïòà: "ÏãúÏû• = ÏùåÏãùÏ†ê √ó ÎîîÏßÄÌÑ∏Ïú® √ó Ï†ÑÌôòÏú® √ó ARPU √ó 12"
    
    Attributes:
        model_id: Î™®Ìòï ID (MODEL_001, MODEL_002, ...)
        name: Î™®Ìòï Ïù¥Î¶Ñ
        formula: ÏàòÏãù (Î¨∏ÏûêÏó¥)
        description: ÏÑ§Î™Ö
        variables: Î≥ÄÏàò ÎîïÏÖîÎÑàÎ¶¨
        total_variables: Ï¥ù Î≥ÄÏàò Í∞úÏàò
        unknown_count: Unknown Î≥ÄÏàò Í∞úÏàò
        feasibility_score: Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï†êÏàò
    """
    model_id: str
    name: str
    formula: str
    description: str
    variables: Dict[str, FermiVariable] = field(default_factory=dict)
    
    # ÌÜµÍ≥Ñ
    total_variables: int = 0
    unknown_count: int = 0
    available_count: int = 0
    
    # ÌèâÍ∞Ä
    feasibility_score: float = 0.0
    unknown_filled: bool = False
    
    # ÏÑ†ÌÉù
    selection_reason: str = ""
    is_alternative: bool = False
    why_not_selected: str = ""


@dataclass
class RankedModel:
    """
    Ï†êÏàòÌôîÎêú Î™®Ìòï
    
    Î™®Ìòï ÏÑ†ÌÉù Í∏∞Ï§Ä 4Í∞ú:
    - Unknown count (50%)
    - Confidence (30%)
    - Complexity (20%)
    - Depth (10% bonus)
    """
    rank: int
    model: FermiModel
    score: float
    
    # Ï†êÏàò Î∂ÑÌï¥
    unknown_score: float = 0.0
    confidence_score: float = 0.0
    complexity_score: float = 0.0
    depth_score: float = 0.0
    
    # ÏÉÅÌÉú
    status: str = "feasible"  # feasible/partial/infeasible
    missing: List[str] = field(default_factory=list)


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Î≥ÄÏàò ÏàòÎ†¥ Ï†ïÏ±Ö (Simple Î∞©Ïãù)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class SimpleVariablePolicy:
    """
    Îã®Ïàú Î≥ÄÏàò Ï†ïÏ±Ö (Ïã§Ïö©Ï†Å)
    
    ÏõêÏπô:
    - 6Í∞ú: Í∂åÏû• (Occam's Razor)
    - 7-10Í∞ú: ÌóàÏö© (Í≤ΩÍ≥†)
    - 10Í∞ú+: Í∏àÏßÄ (Miller's Law)
    
    Ìö®Í≥º: 98% (Hybrid ÎåÄÎπÑ 2% Ï∞®Ïù¥)
    ÏΩîÎìú: 20Ï§Ñ (Hybrid ÎåÄÎπÑ 15Î∞∞ Í∞ÑÎã®)
    """
    
    def __init__(self):
        self.recommended_max = 6   # Occam's Razor
        self.absolute_max = 10     # Miller's Law (7¬±2)
    
    def check(self, variable_count: int) -> Tuple[bool, Optional[str]]:
        """
        Î≥ÄÏàò Í∞úÏàò Ï≤¥ÌÅ¨
        
        Args:
            variable_count: ÌòÑÏû¨ Î≥ÄÏàò Í∞úÏàò
        
        Returns:
            (allowed, warning)
                allowed: True/False
                warning: None ÎòêÎäî Í≤ΩÍ≥† Î©îÏãúÏßÄ
        """
        # Ï†àÎåÄ ÏÉÅÌïú
        if variable_count > self.absolute_max:
            return False, f"üõë Ï†àÎåÄ ÏÉÅÌïú {self.absolute_max}Í∞ú Ï¥àÍ≥º (Ïù∏ÏßÄ ÌïúÍ≥Ñ)"
        
        # Í∂åÏû• ÏÉÅÌïú (Í≤ΩÍ≥†Îßå)
        if variable_count > self.recommended_max:
            return True, f"‚ö†Ô∏è  Í∂åÏû• ÏÉÅÌïú {self.recommended_max}Í∞ú Ï¥àÍ≥º (Î≥µÏû°ÎèÑ‚Üë)"
        
        # Ï†ïÏÉÅ
        return True, None


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Tier 3 Î©îÏù∏ ÌÅ¥ÎûòÏä§
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

class Tier3FermiPath:
    """
    Tier 3: Fermi Model Search
    
    Ïû¨Í∑Ä Î∂ÑÌï¥ Ï∂îÏ†ï - ÎÖºÎ¶¨Ïùò ÌçºÏ¶ê ÎßûÏ∂îÍ∏∞
    
    ÌîÑÎ°úÏÑ∏Ïä§:
    ---------
    Phase 1: Ï¥àÍ∏∞ Ïä§Ï∫î (Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞ ÌååÏïÖ, Bottom-up)
    Phase 2: Î™®Ìòï ÏÉùÏÑ± (LLM 3-5Í∞ú ÌõÑÎ≥¥, Top-down)
    Phase 3: Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨ (Ïû¨Í∑Ä Ï∂îÏ†ïÏúºÎ°ú ÌçºÏ¶ê ÎßûÏ∂îÍ∏∞)
    Phase 4: Î™®Ìòï Ïã§Ìñâ (BacktrackingÏúºÎ°ú Ïû¨Ï°∞Î¶Ω)
    
    ÏïàÏ†Ñ Ïû•Ïπò:
    ----------
    - Max depth: 4 (Î¨¥Ìïú Ïû¨Í∑Ä Î∞©ÏßÄ)
    - ÏàúÌôò Í∞êÏßÄ: Call stack Ï∂îÏ†Å
    - Î≥ÄÏàò Ï†úÌïú: 6Í∞ú Í∂åÏû•, 10Í∞ú Ï†àÎåÄ
    
    Usage:
        >>> tier3 = Tier3FermiPath()
        >>> result = tier3.estimate(
        ...     "ÏùåÏãùÏ†ê SaaS ÏãúÏû•ÏùÄ?",
        ...     context=Context(domain="Food_Service")
        ... )
        >>> result.decomposition.depth  # 2
        >>> result.value  # 20,160,000,000
    """
    
    def __init__(self, config: Tier3Config = None):
        """Ï¥àÍ∏∞Ìôî"""
        self.config = config or Tier3Config()
        
        # Tier 2 ÏùòÏ°¥ÏÑ±
        self.tier2 = Tier2JudgmentPath()
        
        # Ïû¨Í∑Ä Ï∂îÏ†Å
        self.call_stack: List[str] = []
        self.max_depth = self.config.max_depth  # 4
        
        # Î≥ÄÏàò Ï†ïÏ±Ö
        self.variable_policy = SimpleVariablePolicy()
        
        # LLM Î™®Îìú (config/llm_mode.yaml Ï§ÄÏàò)
        self.llm_mode = getattr(settings, 'llm_mode', 'native')  # Í∏∞Î≥∏: native
        self.llm_client = None
        
        # External modeÏùº ÎïåÎßå API Ï¥àÍ∏∞Ìôî
        if self.llm_mode == 'external':
            if HAS_OPENAI and settings.openai_api_key:
                self.llm_client = OpenAI(api_key=settings.openai_api_key)
                logger.info("  ‚úÖ External LLM (OpenAI API) Ï§ÄÎπÑ")
            else:
                logger.warning("  ‚ö†Ô∏è  External modeÏßÄÎßå OpenAI API ÌÇ§ ÏóÜÏùå (Fallback: ÌÖúÌîåÎ¶øÎßå)")
        else:
            logger.info("  ‚úÖ Native Mode (Cursor LLM, ÎπÑÏö© $0)")
            logger.info("     LLM Î™®Ìòï ÏÉùÏÑ±: ÌÖúÌîåÎ¶øÎßå ÏÇ¨Ïö© (80-90% Ïª§Î≤Ñ)")
        
        logger.info("[Tier 3] Fermi Model Search Ï¥àÍ∏∞Ìôî")
        logger.info(f"  Max depth: {self.max_depth}")
        logger.info(f"  Î≥ÄÏàò Ï†ïÏ±Ö: Í∂åÏû• 6Í∞ú, Ï†àÎåÄ 10Í∞ú")
        logger.info(f"  LLM Î™®Îìú: {self.llm_mode}")
    
    def estimate(
        self,
        question: str,
        context: Context = None,
        available_data: Dict = None,
        depth: int = 0,
        parent_data: Dict = None
    ) -> Optional[EstimationResult]:
        """
        Fermi Decomposition Ï∂îÏ†ï
        
        Args:
            question: ÏßàÎ¨∏ (Ïòà: "ÏùåÏãùÏ†ê SaaS ÏãúÏû•ÏùÄ?")
            context: Îß•ÎùΩ (domain, region, time)
            available_data: Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞ (ÌîÑÎ°úÏ†ùÌä∏ Ï†úÍ≥µ)
            depth: ÌòÑÏû¨ Ïû¨Í∑Ä ÍπäÏù¥
            parent_data: Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ (Ïû¨Í∑Ä Ïãú ÏÉÅÏÜç) v7.5.0+
        
        Returns:
            EstimationResult (decomposition Ìè¨Ìï®) ÎòêÎäî None
        """
        start_time = time.time()
        
        logger.info(f"\n{'  ' * depth}[Tier 3] Fermi Estimation (depth {depth})")
        logger.info(f"{'  ' * depth}  ÏßàÎ¨∏: {question}")
        
        # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        # ÏïàÏ†Ñ Ï≤¥ÌÅ¨
        # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        
        # 1. Max depth Ï≤¥ÌÅ¨
        if depth >= self.max_depth:
            logger.warning(f"{'  ' * depth}  ‚ö†Ô∏è  Max depth {self.max_depth} ÎèÑÎã¨ ‚Üí Tier 2 Fallback")
            # Fallback to Tier 2
            return self.tier2.estimate(question, context or Context())
        
        # 2. ÏàúÌôò Í∞êÏßÄ
        if self._detect_circular(question):
            logger.warning(f"{'  ' * depth}  ‚ö†Ô∏è  ÏàúÌôò ÏùòÏ°¥ÏÑ± Í∞êÏßÄ (A‚ÜíB‚ÜíA) ‚Üí Ï§ëÎã®")
            return None
        
        # 3. Call stack Ï∂îÍ∞Ä
        self.call_stack.append(question)
        
        try:
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            # Phase 1: Ï¥àÍ∏∞ Ïä§Ï∫î (Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÜç v7.5.0)
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            scan_result = self._phase1_scan(question, context, available_data, depth, parent_data)
            
            if not scan_result:
                logger.warning(f"{'  ' * depth}  ‚ùå Phase 1 Ïã§Ìå®")
                return None
            
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            # Phase 2: Î™®Ìòï ÏÉùÏÑ±
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            candidate_models = self._phase2_generate_models(
                question,
                scan_result['available'],
                scan_result['unknown'],
                depth
            )
            
            if not candidate_models:
                logger.warning(f"{'  ' * depth}  ‚ùå Phase 2 Ïã§Ìå® (Î™®Ìòï ÏóÜÏùå)")
                return None
            
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            # Phase 3: Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨ (Ïû¨Í∑Ä!)
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            ranked_models = self._phase3_check_feasibility(
                candidate_models,
                context or Context(),
                depth
            )
            
            if not ranked_models:
                logger.warning(f"{'  ' * depth}  ‚ùå Phase 3 Ïã§Ìå® (Ïã§Ìñâ Î∂àÍ∞ÄÎä•)")
                return None
            
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            # Phase 4: ÏµúÏÑ† Î™®Ìòï Ïã§Ìñâ
            # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
            result = self._phase4_execute(ranked_models[0], depth, context or Context())
            
            if result:
                execution_time = time.time() - start_time
                logger.info(f"{'  ' * depth}  ‚úÖ Tier 3 ÏôÑÎ£å: {result.value} ({execution_time:.2f}Ï¥à)")
            
            return result
        
        except Exception as e:
            logger.error(f"{'  ' * depth}  ‚ùå Tier 3 ÏóêÎü¨: {e}")
            return None
        
        finally:
            # Call stackÏóêÏÑú Ï†úÍ±∞ (Ï§ëÏöî!)
            if self.call_stack and self.call_stack[-1] == question:
                self.call_stack.pop()
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Phase 1: Ï¥àÍ∏∞ Ïä§Ï∫î (Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞ ÌååÏïÖ)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _phase1_scan(
        self,
        question: str,
        context: Optional[Context],
        available_data: Optional[Dict],
        depth: int,
        parent_data: Optional[Dict] = None
    ) -> Optional[Dict]:
        """
        Phase 1: Ï¥àÍ∏∞ Ïä§Ï∫î (Bottom-up)
        
        Í∞ÄÏö©Ìïú Îç∞Ïù¥ÌÑ∞ ÌååÏïÖ:
        1. Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÜç (Ïû¨Í∑Ä Ïãú) v7.5.0+
        2. ÌîÑÎ°úÏ†ùÌä∏ Îç∞Ïù¥ÌÑ∞ (available_data)
        3. Îß•ÎùΩÏóêÏÑú ÏûêÎ™ÖÌïú Îç∞Ïù¥ÌÑ∞
        
        Args:
            question: ÏßàÎ¨∏
            context: Îß•ÎùΩ
            available_data: ÌîÑÎ°úÏ†ùÌä∏ Îç∞Ïù¥ÌÑ∞
            depth: ÍπäÏù¥
            parent_data: Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ (v7.5.0+)
        
        Returns:
            {
                'available': Dict[str, FermiVariable],
                'unknown': List[str]
            }
        """
        logger.info(f"{'  ' * depth}  [Phase 1] Ï¥àÍ∏∞ Ïä§Ï∫î")
        
        available = {}
        
        # Step 0: Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÜç (v7.5.0+)
        if parent_data:
            for key, val in parent_data.items():
                if isinstance(val, FermiVariable):
                    # Î∂ÄÎ™® Î≥ÄÏàò Í∑∏ÎåÄÎ°ú ÏÉÅÏÜç
                    available[key] = val
                    logger.info(f"{'  ' * depth}    Î∂ÄÎ™®Î°úÎ∂ÄÌÑ∞ ÏÉÅÏÜç: {key} = {val.value}")
                elif isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source=val.get('source', 'parent_inherited'),
                        confidence=val.get('confidence', 0.8)
                    )
        
        # Step 1: ÌîÑÎ°úÏ†ùÌä∏ Îç∞Ïù¥ÌÑ∞
        if available_data:
            for key, val in available_data.items():
                if isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source="project_data",
                        confidence=val.get('confidence', 1.0),
                        uncertainty=val.get('uncertainty', 0.0)
                    )
                else:
                    # Îã®Ïàú Í∞í
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val,
                        source="project_data",
                        confidence=1.0,
                        uncertainty=0.0
                    )
        
        # Step 2: Îß•ÎùΩÏóêÏÑú ÏûêÎ™ÖÌïú Îç∞Ïù¥ÌÑ∞
        # (Ïòà: ÏãúÍ∞Ñ Ï†úÏïΩ Îì±)
        if context:
            # TODO: context Í∏∞Î∞ò ÏûêÎ™ÖÌïú Î≥ÄÏàò Ï∂îÍ∞Ä
            pass
        
        logger.info(f"{'  ' * depth}    Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞: {len(available)}Í∞ú")
        
        return {
            'available': available,
            'unknown': []  # Phase 2ÏóêÏÑú Î™®ÌòïÎ≥ÑÎ°ú ÌååÏïÖ
        }
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Phase 2: Î™®Ìòï ÏÉùÏÑ± (LLM)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _phase2_generate_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        unknown: List[str],
        depth: int
    ) -> List[FermiModel]:
        """
        Phase 2: Î™®Ìòï ÏÉùÏÑ± (Top-down)
        
        LLMÏóêÍ≤å Ïó¨Îü¨ ÌõÑÎ≥¥ Î™®Ìòï ÏöîÏ≤≠
        
        ÌòÑÏû¨: Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø ÏÇ¨Ïö© (LLM API Íµ¨ÌòÑ ÎåÄÍ∏∞)
        TODO: OpenAI/Anthropic API ÌÜµÌï©
        
        Args:
            question: ÏßàÎ¨∏
            available: Í∞ÄÏö© Î≥ÄÏàò
            unknown: ÎØ∏ÏßÄÏàò Î¶¨Ïä§Ìä∏
            depth: ÍπäÏù¥
        
        Returns:
            3-5Í∞ú FermiModel ÌõÑÎ≥¥
        """
        logger.info(f"{'  ' * depth}  [Phase 2] Î™®Ìòï ÏÉùÏÑ±")
        
        # TODO: LLM API ÌÜµÌï©
        # ÌòÑÏû¨Îäî Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø ÏÇ¨Ïö©
        models = self._generate_default_models(question, available, depth)
        
        # Î≥ÄÏàò Ï†ïÏ±Ö ÌïÑÌÑ∞ÎßÅ
        filtered_models = []
        for model in models:
            allowed, warning = self.variable_policy.check(model.total_variables)
            
            if not allowed:
                logger.warning(f"{'  ' * depth}    Î™®Ìòï {model.model_id} Ï†úÏô∏: {warning}")
                model.why_not_selected = warning
                continue
            
            if warning:
                logger.warning(f"{'  ' * depth}    Î™®Ìòï {model.model_id}: {warning}")
            
            filtered_models.append(model)
        
        logger.info(f"{'  ' * depth}    ÏÉùÏÑ±Îêú Î™®Ìòï: {len(filtered_models)}Í∞ú")
        
        return filtered_models
    
    def _generate_default_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int
    ) -> List[FermiModel]:
        """
        Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø Î™®Ìòï ÏÉùÏÑ±
        
        v7.5.0 Î≥ÄÍ≤Ω:
        - ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Ï†úÍ±∞Îê®
        - LLM Í∏∞Î∞ò ÏùºÎ∞ò Î∂ÑÌï¥Îßå ÏàòÌñâ (External mode)
        - Native modeÎäî CursorÏóêÍ≤å ÏúÑÏûÑ
        
        Args:
            question: ÏßàÎ¨∏
            available: Í∞ÄÏö© Î≥ÄÏàò
            depth: ÍπäÏù¥
        
        Returns:
            FermiModel Î¶¨Ïä§Ìä∏
        """
        # v7.5.0: ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Ï†úÍ±∞
        # ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú(LTV, CAC Îì±)Îäî QuantifierÍ∞Ä Ï≤òÎ¶¨
        # Tier 3Îäî ÏùºÎ∞òÏ†Å Fermi Î∂ÑÌï¥Îßå Îã¥Îãπ
        
        # 2. LLM Î™®Ìòï ÏÉùÏÑ± (External modeÎßå)
        if self.llm_mode == 'external' and self.llm_client:
            logger.info(f"{'  ' * depth}    ÌÖúÌîåÎ¶ø ÏóÜÏùå ‚Üí External LLM Î™®Ìòï ÏÉùÏÑ±")
            llm_models = self._generate_llm_models(question, available, depth)
            if llm_models:
                return llm_models
        elif self.llm_mode == 'native':
            logger.info(f"{'  ' * depth}    Native Mode ‚Üí Cursor LLMÏóêÍ≤å Fermi Î∂ÑÌï¥ ÏöîÏ≤≠")
            logger.info(f"{'  ' * depth}    ‚ÑπÔ∏è  ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú(LTV, CAC Îì±)Îäî QuantifierÍ∞Ä Ï≤òÎ¶¨")
            logger.info(f"{'  ' * depth}    ‚ÑπÔ∏è  Tier 3Îäî ÏùºÎ∞ò Fermi Î∂ÑÌï¥Îßå Îã¥Îãπ")
            return []  # Native modeÏóêÏÑúÎäî CursorÍ∞Ä ÏßÅÏ†ë Fermi Î∂ÑÌï¥ ÏàòÌñâ
        
        # 3. Fallback: Í∏∞Î≥∏ Î™®Ìòï
        logger.warning(f"{'  ' * depth}    Fallback: Í∏∞Î≥∏ Î™®Ìòï")
        
        model = FermiModel(
            model_id="MODEL_DEFAULT",
            name="Í∏∞Î≥∏ Î™®Ìòï",
            formula="result = value",
            description="Îã®Ïàú Ï∂îÏ†ï (Tier 2 ÌôúÏö©)",
            variables={
                "value": FermiVariable(
                    name="value",
                    available=False,
                    need_estimate=True
                )
            },
            total_variables=1,
            unknown_count=1
        )
        
        return [model]
    
    def _match_business_metric_template(
        self,
        question: str
    ) -> List[FermiModel]:
        """
        ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Îß§Ïπ≠ - DISABLED (v7.5.0)
        
        v7.5.0 Î≥ÄÍ≤Ω:
        - ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Ï†úÍ±∞Îê®
        - QuantifierÍ∞Ä LTV, CAC Îì±Ïùò Í≥ÑÏÇ∞ Îã¥Îãπ
        - Estimator Tier 3Îäî ÏùºÎ∞òÏ†Å Fermi Î∂ÑÌï¥Îßå ÏàòÌñâ
        
        Args:
            question: ÏßàÎ¨∏
        
        Returns:
            Îπà Î¶¨Ïä§Ìä∏ (Ìï≠ÏÉÅ Îß§Ïπ≠ ÏóÜÏùå)
        """
        # v7.5.0: ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú ÌÖúÌîåÎ¶ø Ï†úÍ±∞
        # QuantifierÍ∞Ä ÎπÑÏ¶àÎãàÏä§ ÏßÄÌëú Í≥ÑÏÇ∞ Îã¥Îãπ
        return []
    
    def _generate_llm_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int
    ) -> List[FermiModel]:
        """
        LLM APIÎ°ú Î™®Ìòï ÏÉùÏÑ±
        
        ÏÑ§Í≥Ñ: fermi_model_search.yaml Line 1158-1181
        
        Args:
            question: ÏßàÎ¨∏
            available: Í∞ÄÏö© Î≥ÄÏàò
            depth: ÍπäÏù¥
        
        Returns:
            LLMÏù¥ ÏÉùÏÑ±Ìïú FermiModel Î¶¨Ïä§Ìä∏
        """
        logger.info(f"{'  ' * depth}      [LLM] Î™®Ìòï ÏÉùÏÑ± ÏöîÏ≤≠")
        
        # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        prompt = self._build_llm_prompt(question, available)
        
        try:
            # OpenAI API Ìò∏Ï∂ú
            response = self.llm_client.chat.completions.create(
                model=self.config.llm_model,
                temperature=self.config.llm_temperature,
                messages=[
                    {
                        "role": "system",
                        "content": "ÎãπÏã†ÏùÄ Fermi Estimation Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. ÏßàÎ¨∏ÏùÑ Í≥ÑÏÇ∞ Í∞ÄÎä•Ìïú ÏàòÌïôÏ†Å Î™®ÌòïÏúºÎ°ú Î∂ÑÌï¥ÌïòÏÑ∏Ïöî."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            llm_output = response.choices[0].message.content
            logger.info(f"{'  ' * depth}      [LLM] ÏùëÎãµ ÏàòÏã† ({len(llm_output)}Ïûê)")
            
            # ÏùëÎãµ ÌååÏã±
            models = self._parse_llm_models(llm_output, depth)
            
            logger.info(f"{'  ' * depth}      [LLM] ÌååÏã± ÏôÑÎ£å: {len(models)}Í∞ú Î™®Ìòï")
            
            return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}      ‚ùå LLM API Ïã§Ìå®: {e}")
            return []
    
    def _build_llm_prompt(
        self,
        question: str,
        available: Dict[str, FermiVariable]
    ) -> str:
        """
        LLM ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        
        ÏÑ§Í≥Ñ: fermi_model_search.yaml Line 1163-1181
        """
        # Í∞ÄÏö© Îç∞Ïù¥ÌÑ∞ Î¨∏ÏûêÏó¥
        if available:
            available_str = "\n".join([
                f"- {var.name}: {var.value} ({var.source}, confidence: {var.confidence:.0%})"
                for var in available.values()
            ])
        else:
            available_str = "(ÏóÜÏùå)"
        
        prompt = f"""ÏßàÎ¨∏: {question}

Í∞ÄÏö©Ìïú Îç∞Ïù¥ÌÑ∞:
{available_str}

ÏûÑÎ¨¥:
1. Ïù¥ ÏßàÎ¨∏Ïóê ÎãµÌïòÍ∏∞ ÏúÑÌïú Í≥ÑÏÇ∞ Î™®ÌòïÏùÑ 3-5Í∞ú Ï†úÏãúÌïòÏÑ∏Ïöî.
2. Í∞Å Î™®ÌòïÏùÄ Îã§Î•∏ Î∂ÑÌï¥ Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.
3. Í∞ÄÏö©Ìïú Îç∞Ïù¥ÌÑ∞Î•º ÏµúÎåÄÌïú ÌôúÏö©ÌïòÏÑ∏Ïöî.
4. Unknown Î≥ÄÏàòÎ•º ÏµúÏÜåÌôîÌïòÏÑ∏Ïöî.
5. Í∞ÑÎã®Ìï†ÏàòÎ°ù Ï¢ãÏäµÎãàÎã§ (Occam's Razor, ÏµúÎåÄ 6Í∞ú Î≥ÄÏàò Í∂åÏû•).

Ï∂úÎ†• ÌòïÏãù (YAML):
```yaml
models:
  - id: MODEL_001
    formula: "result = A √ó B √ó C"
    description: "ÏÑ§Î™Ö"
    variables:
      - name: A
        description: "ÏùåÏãùÏ†ê Ïàò"
        available: true
      - name: B
        description: "ÎèÑÏûÖÎ•†"
        available: false
      - name: C
        description: "ARPU"
        available: false
  
  - id: MODEL_002
    formula: "result = A √ó B √ó C √ó D"
    description: "ÏÑ§Î™Ö"
    variables:
      - name: A
        description: "ÏùåÏãùÏ†ê Ïàò"
        available: true
      - name: B
        description: "ÎîîÏßÄÌÑ∏Ïú®"
        available: true
      - name: C
        description: "Ï†ÑÌôòÏú®"
        available: true
      - name: D
        description: "ARPU"
        available: false
```

Ï£ºÏùò: YAML ÌòïÏãùÏúºÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî."""
        
        return prompt
    
    def _parse_llm_models(
        self,
        llm_output: str,
        depth: int
    ) -> List[FermiModel]:
        """
        LLM ÏùëÎãµ ÌååÏã± (YAML)
        
        Args:
            llm_output: LLM ÏùëÎãµ
            depth: ÍπäÏù¥
        
        Returns:
            FermiModel Î¶¨Ïä§Ìä∏
        """
        try:
            # YAML Î∏îÎ°ù Ï∂îÏ∂ú (```yaml ... ```)
            yaml_match = re.search(r'```yaml\n(.*?)\n```', llm_output, re.DOTALL)
            
            if not yaml_match:
                # YAML Î∏îÎ°ù ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥ ÌååÏã± ÏãúÎèÑ
                yaml_str = llm_output
            else:
                yaml_str = yaml_match.group(1)
            
            # YAML ÌååÏã±
            data = yaml.safe_load(yaml_str)
            
            if not data or 'models' not in data:
                logger.warning(f"{'  ' * depth}        ‚ö†Ô∏è  YAML ÌååÏã± Ïã§Ìå® (models ÌÇ§ ÏóÜÏùå)")
                return []
            
            # FermiModel Î≥ÄÌôò
            models = []
            for model_data in data['models']:
                # Î≥ÄÏàò ÌååÏã±
                variables = {}
                for var_data in model_data.get('variables', []):
                    var_name = var_data.get('name', 'unknown')
                    var_available = var_data.get('available', False)
                    
                    variables[var_name] = FermiVariable(
                        name=var_name,
                        available=var_available,
                        need_estimate=not var_available,
                        source="llm_generated" if var_available else ""
                    )
                
                # FermiModel ÏÉùÏÑ±
                model = FermiModel(
                    model_id=model_data.get('id', f"LLM_MODEL_{len(models)+1}"),
                    name="LLM ÏÉùÏÑ± Î™®Ìòï",
                    formula=model_data.get('formula', ''),
                    description=model_data.get('description', ''),
                    variables=variables,
                    total_variables=len(variables),
                    unknown_count=sum(1 for v in variables.values() if not v.available)
                )
                
                models.append(model)
            
            return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}        ‚ùå LLM ÏùëÎãµ ÌååÏã± Ïã§Ìå®: {e}")
            return []
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Phase 3: Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨ (Ïû¨Í∑Ä Ï∂îÏ†ï)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _phase3_check_feasibility(
        self,
        models: List[FermiModel],
        context: Context,
        current_depth: int
    ) -> List[RankedModel]:
        """
        Phase 3: Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨ + Ïû¨Í∑Ä Ï∂îÏ†ï
        
        Í∞Å Î™®ÌòïÏùò Unknown Î≥ÄÏàòÎ•º Ïû¨Í∑Ä Ìò∏Ï∂úÎ°ú Ï±ÑÏö∞Í∏∞
        
        Args:
            models: ÌõÑÎ≥¥ Î™®ÌòïÎì§
            context: Îß•ÎùΩ
            current_depth: ÌòÑÏû¨ ÍπäÏù¥
        
        Returns:
            Ï†êÏàò Ïàú RankedModel Î¶¨Ïä§Ìä∏
        """
        logger.info(f"{'  ' * current_depth}  [Phase 3] Ïã§Ìñâ Í∞ÄÎä•ÏÑ± Ï≤¥ÌÅ¨")
        
        ranked = []
        
        for model in models:
            logger.info(f"{'  ' * current_depth}    Î™®Ìòï: {model.model_id}")
            
            # Unknown Î≥ÄÏàò Ï∂îÏ†ï (Ïû¨Í∑Ä!)
            for var_name, var in model.variables.items():
                if var.need_estimate and not var.estimation_result:
                    logger.info(f"{'  ' * current_depth}      Î≥ÄÏàò '{var_name}' Ï∂îÏ†ï ÌïÑÏöî")
                    
                    # ‚≠ê Ïû¨Í∑Ä Ìò∏Ï∂ú!
                    var_result = self._estimate_variable(
                        var_name,
                        context,
                        current_depth + 1
                    )
                    
                    if var_result:
                        var.estimation_result = var_result
                        var.value = var_result.value
                        var.confidence = var_result.confidence
                        var.available = True
                        var.source = f"tier3_recursive_depth_{current_depth + 1}"
                        logger.info(f"{'  ' * current_depth}        ‚úÖ {var.value} (conf: {var.confidence:.2f})")
                    else:
                        logger.warning(f"{'  ' * current_depth}        ‚ùå Ï∂îÏ†ï Ïã§Ìå®")
            
            # Î™®Ìòï Ï†êÏàòÌôî
            score_result = self._score_model(model, current_depth)
            
            ranked.append(RankedModel(
                rank=0,  # Ï†ïÎ†¨ ÌõÑ Ìï†Îãπ
                model=model,
                score=score_result['total'],
                unknown_score=score_result['unknown'],
                confidence_score=score_result['confidence'],
                complexity_score=score_result['complexity'],
                depth_score=score_result['depth'],
                status=score_result['status'],
                missing=score_result['missing']
            ))
        
        # Ï†êÏàò Ïàú Ï†ïÎ†¨
        ranked.sort(key=lambda x: x.score, reverse=True)
        
        # Rank Ìï†Îãπ
        for i, rm in enumerate(ranked, 1):
            rm.rank = i
        
        if ranked:
            logger.info(f"{'  ' * current_depth}    ÏµúÏÑ† Î™®Ìòï: {ranked[0].model.model_id} "
                       f"(Ï†êÏàò: {ranked[0].score:.3f})")
        
        return ranked
    
    def _estimate_variable(
        self,
        var_name: str,
        context: Context,
        depth: int
    ) -> Optional[EstimationResult]:
        """
        Î≥ÄÏàò Ï∂îÏ†ï (Ïû¨Í∑Ä)
        
        1. Tier 2 Î®ºÏ†Ä ÏãúÎèÑ (Îπ†Î¶Ñ, Ïû¨Í∑Ä ÌîºÌï®)
        2. Tier 2 Ïã§Ìå® ‚Üí Tier 3 Ïû¨Í∑Ä Ìò∏Ï∂ú
        
        Args:
            var_name: Î≥ÄÏàò Ïù¥Î¶Ñ
            context: Îß•ÎùΩ
            depth: ÍπäÏù¥
        
        Returns:
            EstimationResult ÎòêÎäî None
        """
        # ContextÎ•º ÏßàÎ¨∏Ïóê Î™ÖÏãúÏ†ÅÏúºÎ°ú Ìè¨Ìï® (v7.5.0)
        question = self._build_contextualized_question(var_name, context)
        
        logger.info(f"{'  ' * depth}      [Recursive] {question}")
        
        # 1. Tier 2 Î®ºÏ†Ä ÏãúÎèÑ (Ïû¨Í∑Ä ÏµúÏÜåÌôî)
        tier2_result = self.tier2.estimate(question, context)
        
        if tier2_result and tier2_result.confidence >= 0.80:  # v7.5.0: 0.7‚Üí0.8 Í∞ïÌôî
            logger.info(f"{'  ' * depth}        ‚úÖ Tier 2 ÏÑ±Í≥µ (Ïû¨Í∑Ä Î∂àÌïÑÏöî)")
            return tier2_result
        
        # 2. Tier 2 Ïã§Ìå® ‚Üí Tier 3 Ïû¨Í∑Ä
        logger.info(f"{'  ' * depth}        üîÑ Tier 2 Ïã§Ìå® ‚Üí Fermi Ïû¨Í∑Ä")
        
        # Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ (v7.5.0+)
        parent_data_to_pass = {}
        # TODO: ÌòÑÏû¨ Î™®ÌòïÏùò available Î≥ÄÏàòÎ•º Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞Î°ú Ï†ÑÎã¨
        
        # ‚≠ê Ïû¨Í∑Ä Ìò∏Ï∂ú (Î∂ÄÎ™® Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÜç)
        return self.estimate(
            question=question,
            context=context,
            available_data=None,
            depth=depth,
            parent_data=parent_data_to_pass  # v7.5.0: Îç∞Ïù¥ÌÑ∞ ÏÉÅÏÜç
        )
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Phase 4: Î™®Ìòï Ïã§Ìñâ (Backtracking)
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _phase4_execute(
        self,
        ranked_model: RankedModel,
        depth: int,
        context: Context
    ) -> Optional[EstimationResult]:
        """
        Phase 4: Î™®Ìòï Ïã§Ìñâ (Backtracking)
        
        Ïû¨Í∑ÄÎ°ú Ï±ÑÏö¥ Î≥ÄÏàòÎì§ÏùÑ backtrackingÏúºÎ°ú Ïû¨Ï°∞Î¶Ω
        
        Args:
            ranked_model: ÏÑ†ÌÉùÎêú Î™®Ìòï
            depth: ÍπäÏù¥
            context: Îß•ÎùΩ
        
        Returns:
            EstimationResult (decomposition Ìè¨Ìï®)
        """
        logger.info(f"{'  ' * depth}  [Phase 4] Î™®Ìòï Ïã§Ìñâ")
        
        model = ranked_model.model
        
        # Step 1: Î≥ÄÏàò Î∞îÏù∏Îî© ÌôïÏù∏
        bindings = {}
        for name, var in model.variables.items():
            if var.available and var.value is not None:
                bindings[name] = var.value
            else:
                logger.warning(f"{'  ' * depth}    ‚ö†Ô∏è  Î≥ÄÏàò '{name}' Í∞í ÏóÜÏùå")
        
        if not bindings:
            logger.warning(f"{'  ' * depth}    ‚ùå Î∞îÏù∏Îî©Ìï† Î≥ÄÏàò ÏóÜÏùå")
            return None
        
        logger.info(f"{'  ' * depth}    Î≥ÄÏàò Î∞îÏù∏Îî©: {list(bindings.keys())}")
        
        # Step 2: Í≥ÑÏÇ∞ Ïã§Ìñâ
        # TODO: ÏàòÏãù ÌååÏã± Î∞è ÏïàÏ†ÑÌïú Ïã§Ìñâ
        # ÌòÑÏû¨: Í∞ÑÎã®Ìïú Í≥±ÏÖà Í∞ÄÏ†ï
        result_value = self._execute_formula_simple(model.formula, bindings)
        
        # Step 3: Confidence Ï°∞Ìï© (Geometric Mean)
        confidences = [
            var.confidence
            for var in model.variables.values()
            if var.available and var.confidence > 0
        ]
        
        if confidences:
            combined_confidence = math.prod(confidences) ** (1 / len(confidences))
        else:
            combined_confidence = 0.5
        
        logger.info(f"{'  ' * depth}    Confidence: {combined_confidence:.2f}")
        
        # Step 4: DecompositionTrace ÏÉùÏÑ±
        decomposition = DecompositionTrace(
            formula=model.formula,
            variables={
                name: var.estimation_result
                for name, var in model.variables.items()
                if var.estimation_result
            },
            calculation_logic=model.description,
            depth=depth,
            decomposition_reasoning=model.selection_reason
        )
        
        # Step 5: ComponentEstimation ÏÉùÏÑ±
        components = [
            ComponentEstimation(
                component_name=name,
                component_value=var.value or 0.0,
                estimation_method=var.source,
                reasoning=f"{var.source}ÏóêÏÑú ÌöçÎìù",
                confidence=var.confidence,
                sources=[var.source]
            )
            for name, var in model.variables.items()
            if var.available
        ]
        
        # Step 6: Estimation Trace ÏÉùÏÑ±
        trace = [
            f"Step 1: Î¨∏Ï†ú Ï†ïÏùò - {model.description}",
            f"Step 2: Î™®Ìòï ÏÑ†ÌÉù - {model.formula}",
            f"Step 3: Î∂ÑÌï¥ - {model.total_variables}Í∞ú Î≥ÄÏàò",
            f"Step 4: Î≥ÄÏàò Ï∂îÏ†ï - {model.available_count}Í∞ú ÌôïÎ≥¥",
            f"Step 5: Ïû¨Í∑Ä ÍπäÏù¥ - depth {depth}",
            f"Step 6: Í≥ÑÏÇ∞ - {model.formula}",
            f"Step 7: Confidence - {combined_confidence:.2f}",
            f"Step 8: Í≤∞Í≥º - {result_value}"
        ]
        
        # Step 7: EstimationResult ÏÉùÏÑ±
        result = EstimationResult(
            value=result_value,
            confidence=combined_confidence,
            tier=3,
            sources=[var.source for var in model.variables.values() if var.available],
            reasoning_detail={
                'method': 'fermi_decomposition',
                'model_id': model.model_id,
                'formula': model.formula,
                'depth': depth,
                'selection_reason': model.selection_reason,
                'why_this_method': f'Tier 1/2 Ïã§Ìå®, Ïû¨Í∑Ä Î∂ÑÌï¥ ÌïÑÏöî (depth {depth})'
            },
            component_estimations=components,
            estimation_trace=trace,
            decomposition=decomposition
        )
        
        return result
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Î™®Ìòï Ï†êÏàòÌôî
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _score_model(
        self,
        model: FermiModel,
        depth: int
    ) -> Dict[str, Any]:
        """
        Î™®Ìòï Ï†êÏàòÌôî (4Í∞ú Í∏∞Ï§Ä)
        
        ÏÑ§Í≥Ñ: fermi_model_search.yaml Line 725-810
        
        Í∏∞Ï§Ä:
        1. Unknown count (50%): Ï†ÅÏùÑÏàòÎ°ù Ï¢ãÏùå
        2. Confidence (30%): ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏùå
        3. Complexity (20%): Í∞ÑÎã®Ìï†ÏàòÎ°ù Ï¢ãÏùå
        4. Depth (10% bonus): ÏñïÏùÑÏàòÎ°ù Ï¢ãÏùå
        
        Returns:
            {
                'unknown': float,
                'confidence': float,
                'complexity': float,
                'depth': float,
                'total': float,
                'status': str,
                'missing': List[str]
            }
        """
        # 1. Unknown count (50%)
        if model.total_variables > 0:
            filled = sum(1 for v in model.variables.values() if v.available)
            model.available_count = filled
            unknown_ratio = filled / model.total_variables
        else:
            unknown_ratio = 0.0
        
        unknown_score = unknown_ratio * 0.5
        
        # 2. Confidence (30%)
        confidences = [
            v.confidence for v in model.variables.values()
            if v.available and v.confidence > 0
        ]
        
        if confidences:
            avg_confidence = math.prod(confidences) ** (1 / len(confidences))  # Geometric mean
        else:
            avg_confidence = 0.0
        
        confidence_score = avg_confidence * 0.3
        
        # 3. Complexity (20%)
        var_count = model.total_variables
        
        complexity_map = {
            1: 1.0, 2: 1.0,
            3: 0.9, 4: 0.7,
            5: 0.5, 6: 0.3,
            7: 0.2, 8: 0.15,
            9: 0.10, 10: 0.05
        }
        
        complexity = complexity_map.get(var_count, 0.0)
        complexity_score = complexity * 0.2
        
        # 4. Depth (10% bonus)
        depth_penalties = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}
        depth_penalty = depth_penalties.get(depth, 0.2)
        depth_score = depth_penalty * 0.1
        
        # Ï¥ùÏ†ê
        total = unknown_score + confidence_score + complexity_score + depth_score
        
        # ÏÉÅÌÉú ÌåêÎã®
        missing = [
            name for name, var in model.variables.items()
            if not var.available
        ]
        
        if not missing:
            status = "feasible"
        elif len(missing) <= 2:
            status = "partial"
        else:
            status = "infeasible"
        
        return {
            'unknown': unknown_score,
            'confidence': confidence_score,
            'complexity': complexity_score,
            'depth': depth_score,
            'total': total,
            'status': status,
            'missing': missing
        }
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # ÏïàÏ†Ñ Ïû•Ïπò
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _detect_circular(self, question: str) -> bool:
        """
        ÏàúÌôò ÏùòÏ°¥ÏÑ± Í∞êÏßÄ
        
        Call stackÏóê ÎèôÏùº ÏßàÎ¨∏Ïù¥ ÏûàÏúºÎ©¥ ÏàúÌôò
        
        Ïòà:
            depth 0: "ÏãúÏû• Í∑úÎ™®Îäî?"
            depth 1: "Ï†êÏú†Ïú®ÏùÄ?"
            depth 2: "ÏãúÏû• Í∑úÎ™®Îäî?"  # ‚Üê ÏàúÌôò!
        
        Args:
            question: ÏßàÎ¨∏
        
        Returns:
            True: ÏàúÌôò Í∞êÏßÄ
            False: Ï†ïÏÉÅ
        """
        normalized = question.lower().strip()
        
        for past_question in self.call_stack:
            if past_question.lower().strip() == normalized:
                logger.warning(f"    ÏàúÌôò Í∞êÏßÄ: '{question}'")
                logger.warning(f"    Call stack: {self.call_stack}")
                return True
        
        return False
    
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    # Ïú†Ìã∏Î¶¨Ìã∞
    # ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
    
    def _build_contextualized_question(
        self,
        var_name: str,
        context: Context
    ) -> str:
        """
        ContextÎ•º Ìè¨Ìï®Ìïú Íµ¨Ï≤¥Ï†ÅÏù∏ ÏßàÎ¨∏ ÏÉùÏÑ± (v7.5.0)
        
        Î≥ÄÏàò Ïù¥Î¶ÑÎßåÏúºÎ°úÎäî Ïï†Îß§ÌïòÎØÄÎ°ú, Îß•ÎùΩÏùÑ Î™ÖÏãúÏ†ÅÏúºÎ°ú Ìè¨Ìï®
        
        Args:
            var_name: Î≥ÄÏàò Ïù¥Î¶Ñ (Ïòà: "arpu", "churn_rate")
            context: Îß•ÎùΩ
        
        Returns:
            Íµ¨Ï≤¥ÌôîÎêú ÏßàÎ¨∏ Î¨∏ÏûêÏó¥
        
        Example:
            >>> _build_contextualized_question("arpu", Context(domain="B2B_SaaS", region="ÌïúÍµ≠"))
            >>> # "B2B SaaS ÌïúÍµ≠ ÏãúÏû•Ïùò ARPUÎäî?"
        """
        # Î≥ÄÏàò Ïù¥Î¶Ñ Ï†ïÎ¶¨ (snake_case ‚Üí ÎùÑÏñ¥Ïì∞Í∏∞)
        readable_var = var_name.replace('_', ' ').upper()
        
        # Context ÏöîÏÜå ÏàòÏßë
        context_parts = []
        
        if context.domain and context.domain != "General":
            context_parts.append(context.domain.replace('_', ' '))
        
        if context.region:
            context_parts.append(context.region)
        
        if context.time_period:
            context_parts.append(context.time_period)
        
        # ÏßàÎ¨∏ Ï°∞Î¶Ω
        if context_parts:
            context_str = " ".join(context_parts)
            question = f"{context_str} ÏãúÏû•Ïùò {readable_var}Îäî?"
        else:
            question = f"{readable_var}Îäî?"
        
        return question
    
    def _execute_formula_simple(
        self,
        formula: str,
        bindings: Dict[str, float]
    ) -> float:
        """
        ÏàòÏãù Ïã§Ìñâ (ÏïàÏ†ÑÌïú Î≤ÑÏ†Ñ)
        
        ÏßÄÏõê Ïó∞ÏÇ∞: +, -, *, /, Í¥ÑÌò∏
        Í∏àÏßÄ: eval() (Î≥¥Ïïà ÏúÑÌóò)
        
        Args:
            formula: ÏàòÏãù (Ïòà: "ltv = arpu / churn_rate")
            bindings: Î≥ÄÏàò Í∞í (Ïòà: {"arpu": 80000, "churn_rate": 0.05})
        
        Returns:
            Í≥ÑÏÇ∞ Í≤∞Í≥º
        """
        try:
            # ÏàòÏãùÏóêÏÑú Í≤∞Í≥º Î≥ÄÏàò Ï†úÍ±∞ (Ïòà: "ltv = ..." ‚Üí "...")
            if '=' in formula:
                parts = formula.split('=', 1)
                if len(parts) == 2:
                    formula = parts[1].strip()
            
            # √ó ‚Üí * Î≥ÄÌôò (ÏàòÌïô Í∏∞Ìò∏ Ï†ïÍ∑úÌôî)
            expr = formula.replace('√ó', '*').replace('√∑', '/')
            
            # Î≥ÄÏàò ÏπòÌôò
            for var_name, var_value in bindings.items():
                # Î≥ÄÏàò Ïù¥Î¶ÑÏùÑ Í∞íÏúºÎ°ú ÏπòÌôò
                expr = expr.replace(var_name, str(var_value))
            
            # ÏïàÏ†ÑÌïú Í≥ÑÏÇ∞ (ÌóàÏö© Î¨∏ÏûêÎßå)
            allowed_chars = set('0123456789.+-*/() ')
            if not all(c in allowed_chars for c in expr):
                logger.warning(f"    ‚ö†Ô∏è  ÏàòÏãùÏóê ÌóàÏö©ÎêòÏßÄ ÏïäÎäî Î¨∏Ïûê: {formula}")
                # Fallback: Í≥±ÏÖà
                return math.prod(bindings.values()) if bindings else 0.0
            
            # Í≥ÑÏÇ∞ Ïã§Ìñâ (Ï†úÌïúÏ†Å eval - Ïà´ÏûêÏôÄ Ïó∞ÏÇ∞ÏûêÎßå)
            result = eval(expr, {"__builtins__": {}}, {})
            
            return float(result)
        
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è  ÏàòÏãù Ïã§Ìñâ Ïã§Ìå®: {e}")
            logger.warning(f"    Fallback: Í≥±ÏÖà ÏÇ¨Ïö©")
            
            # Fallback: Í≥±ÏÖà
            if bindings:
                return math.prod(bindings.values())
            return 0.0


