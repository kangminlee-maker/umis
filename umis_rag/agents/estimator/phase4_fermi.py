"""
Phase 4: Fermi Decomposition (v7.7.0)

ì¬ê·€ ë¶„í•´ ì¶”ì • - ë…¼ë¦¬ì˜ í¼ì¦ ë§ì¶”ê¸° (Step 1-4)

ì„¤ê³„: config/fermi_model_search.yaml (1,500ì¤„)
ì›ë¦¬: ê°€ìš© ë°ì´í„°(Bottom-up) + ê°œë… ë¶„í•´(Top-down) ë°˜ë³µ

v7.7.0 íŒŒì¼ëª… ë³€ê²½:
-------------------
- tier3.py â†’ phase4_fermi.py
- Tier3FermiPath â†’ Phase4FermiDecomposition
- Phase 4: Estimatorì˜ Fermi Decomposition
- Step 1-4: ë‚´ë¶€ ì„¸ë¶€ ë‹¨ê³„ (ìŠ¤ìº” â†’ ìƒì„± â†’ ì²´í¬ â†’ ì‹¤í–‰)

v7.6.2 ì£¼ìš” ê°œì„ :
-----------------
- í•˜ë“œì½”ë”© ì™„ì „ ì œê±° (adoption_rate, arpu ë“±)
- Boundary ê²€ì¦ ì¶”ê°€ (ê°œë… ê¸°ë°˜)
- Fallback ì²´ê³„ (confidence 0.5)
- Native Mode ì¬ê·€ ì¶”ì • ê°•í™”
- ì •í™•ë„ 3ë°° ê°œì„  (70% â†’ 25% ì˜¤ì°¨)
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
import time
import math
import copy
from datetime import datetime

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.agents.estimator.models import (
    Context, EstimationResult, DecompositionTrace,
    ComponentEstimation, Phase4Config
)
from umis_rag.agents.estimator.phase3_guestimation import Phase3Guestimation
from umis_rag.utils.logger import logger
from umis_rag.core.config import settings
from umis_rag.core.model_router import select_model

# LLM API
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# RAG (Chroma)
try:
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings
    HAS_CHROMA = True
except ImportError:
    HAS_CHROMA = False
    logger.warning("OpenAI íŒ¨í‚¤ì§€ ì—†ìŒ (pip install openai)")

import yaml
import re


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ í…œí”Œë¦¿ - REMOVED (v7.5.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 
# v7.5.0 ë³€ê²½:
# - ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ê³„ì‚° ê³µì‹ì€ Quantifierë¡œ ì´ë™
# - EstimatorëŠ” ìˆœìˆ˜ ê°’ ì¶”ì •ë§Œ ë‹´ë‹¹
# - Phase 4ëŠ” ì¼ë°˜ì  Fermi ë¶„í•´ì— ì§‘ì¤‘
#
# ì´ì „ ìœ„ì¹˜: phase4_fermi.py BUSINESS_METRIC_TEMPLATES
# ì‹ ê·œ ìœ„ì¹˜: data/raw/calculation_methodologies.yaml (Quantifier)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# v7.5.0: ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ í…œí”Œë¦¿ ì œê±°ë¨
# Quantifierê°€ LTV, CAC, ARPU ë“±ì˜ ê³„ì‚° ê³µì‹ ì†Œìœ 
# v7.7.0: íŒŒì¼ëª… ë³€ê²½ (tier3.py â†’ phase4_fermi.py)
BUSINESS_METRIC_TEMPLATES_REMOVED = {
    # Unit Economics (ìš°ì„  - "ltv/cac" ì •í™• ë§¤ì¹­)
    "unit_economics": {
        "keywords": ["unit economics", "ltv/cac", "ë¹„ìœ¨", "ratio", "ê²½ì œì„±"],
        "models": [
            {
                "id": "UE_001",
                "formula": "ratio = ltv / cac",
                "description": "LTV/CAC ë¹„ìœ¨",
                "variables": ["ltv", "cac"]
            }
        ]
    },
    
    # ì‹œì¥ ê·œëª¨
    "market_sizing": {
        "keywords": ["ì‹œì¥", "ê·œëª¨", "TAM", "SAM", "market size"],
        "models": [
            {
                "id": "MARKET_001",
                "formula": "market = customers Ã— adoption_rate Ã— arpu Ã— 12",
                "description": "ê¸°ì—…/ê³ ê° ìˆ˜ ê¸°ë°˜ ì‹œì¥ ê·œëª¨",
                "variables": ["customers", "adoption_rate", "arpu"]
            },
            {
                "id": "MARKET_002",
                "formula": "market = population Ã— digital_rate Ã— conversion_rate Ã— arpu Ã— 12",
                "description": "ì¸êµ¬ ê¸°ë°˜ ë””ì§€í„¸ ì „í™˜ ì‹œì¥",
                "variables": ["population", "digital_rate", "conversion_rate", "arpu"]
            }
        ]
    },
    
    # ê³ ê° ìƒì•  ê°€ì¹˜
    "ltv": {
        "keywords": ["ltv", "LTV", "ìƒì• ê°€ì¹˜", "lifetime value"],
        "models": [
            {
                "id": "LTV_001",
                "formula": "ltv = arpu / churn_rate",
                "description": "ARPUë¥¼ Churnìœ¼ë¡œ ë‚˜ëˆˆ LTV",
                "variables": ["arpu", "churn_rate"]
            },
            {
                "id": "LTV_002",
                "formula": "ltv = arpu Ã— average_lifetime_months",
                "description": "í‰ê·  ìƒì•  ê¸°ê°„ ê¸°ë°˜ LTV",
                "variables": ["arpu", "average_lifetime_months"]
            }
        ]
    },
    
    # ê³ ê° íšë“ ë¹„ìš©
    "cac": {
        "keywords": ["cac", "CAC", "ê³ ê°íšë“", "customer acquisition"],
        "models": [
            {
                "id": "CAC_001",
                "formula": "cac = marketing_cost / new_customers",
                "description": "ë§ˆì¼€íŒ… ë¹„ìš©ì„ ì‹ ê·œ ê³ ê°ìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["marketing_cost", "new_customers"]
            },
            {
                "id": "CAC_002",
                "formula": "cac = cpc / conversion_rate",
                "description": "CPCë¥¼ ì „í™˜ìœ¨ë¡œ ë‚˜ëˆ”",
                "variables": ["cpc", "conversion_rate"]
            }
        ]
    },
    
    # ì „í™˜ìœ¨
    "conversion": {
        "keywords": ["ì „í™˜ìœ¨", "conversion", "CVR"],
        "models": [
            {
                "id": "CVR_001",
                "formula": "conversion = paid_users / free_users",
                "description": "ìœ ë£Œ ì „í™˜ìœ¨ (Freemium)",
                "variables": ["paid_users", "free_users"]
            },
            {
                "id": "CVR_002",
                "formula": "conversion = industry_avg Ã— product_quality_factor",
                "description": "ì—…ê³„ í‰ê·  ì¡°ì •",
                "variables": ["industry_avg", "product_quality_factor"]
            }
        ]
    },
    
    # í•´ì§€ìœ¨
    "churn": {
        "keywords": ["churn", "í•´ì§€ìœ¨", "ì´íƒˆìœ¨"],
        "models": [
            {
                "id": "CHURN_001",
                "formula": "churn = churned_customers / total_customers",
                "description": "í•´ì§€ ê³ ê° ë¹„ìœ¨",
                "variables": ["churned_customers", "total_customers"]
            },
            {
                "id": "CHURN_002",
                "formula": "churn = 1 - retention_rate",
                "description": "ìœ ì§€ìœ¨ì˜ ì—­ìˆ˜",
                "variables": ["retention_rate"]
            }
        ]
    },
    
    # ARPU
    "arpu": {
        "keywords": ["arpu", "ARPU", "í‰ê· ë§¤ì¶œ", "average revenue"],
        "models": [
            {
                "id": "ARPU_001",
                "formula": "arpu = base_fee",
                "description": "ê¸°ë³¸ë£Œë§Œ",
                "variables": ["base_fee"]
            },
            {
                "id": "ARPU_002",
                "formula": "arpu = base_fee + overage_fee",
                "description": "ê¸°ë³¸ë£Œ + ì´ˆê³¼ë£Œ",
                "variables": ["base_fee", "overage_fee"]
            },
            {
                "id": "ARPU_003",
                "formula": "arpu = base_fee + usage_fee + addon_fee",
                "description": "ê¸°ë³¸ë£Œ + ì‚¬ìš©ëŸ‰ë£Œ + ì¶”ê°€ê¸°ëŠ¥ë£Œ",
                "variables": ["base_fee", "usage_fee", "addon_fee"]
            }
        ]
    },
    
    # ì„±ì¥ë¥ 
    "growth": {
        "keywords": ["ì„±ì¥ë¥ ", "growth rate", "CAGR"],
        "models": [
            {
                "id": "GROWTH_001",
                "formula": "growth = (current_year - last_year) / last_year",
                "description": "YoY ì„±ì¥ë¥ ",
                "variables": ["current_year", "last_year"]
            },
            {
                "id": "GROWTH_002",
                "formula": "growth = market_growth + market_share_change",
                "description": "ì‹œì¥ ì„±ì¥ + ì ìœ ìœ¨ ë³€í™”",
                "variables": ["market_growth", "market_share_change"]
            }
        ]
    },
    
    # Payback Period (v7.5.0)
    "payback": {
        "keywords": ["payback", "íšŒìˆ˜ê¸°ê°„", "íˆ¬ìíšŒìˆ˜"],
        "models": [
            {
                "id": "PAYBACK_001",
                "formula": "payback = cac / (arpu Ã— gross_margin)",
                "description": "CACë¥¼ ì›” ê¸°ì—¬ì´ìµìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["cac", "arpu", "gross_margin"]
            },
            {
                "id": "PAYBACK_002",
                "formula": "payback = initial_investment / monthly_profit",
                "description": "ì´ˆê¸° íˆ¬ìë¥¼ ì›” ìˆ˜ìµìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["initial_investment", "monthly_profit"]
            }
        ]
    },
    
    # Rule of 40 (v7.5.0)
    "rule_of_40": {
        "keywords": ["rule of 40", "40 ë²•ì¹™"],
        "models": [
            {
                "id": "R40_001",
                "formula": "rule_40 = growth_rate + profit_margin",
                "description": "ì„±ì¥ë¥  + ì´ìµë¥  (40% ì´ìƒì´ ê±´ê°•)",
                "variables": ["growth_rate", "profit_margin"]
            }
        ]
    },
    
    # Net Revenue Retention (v7.5.0)
    "nrr": {
        "keywords": ["nrr", "net revenue retention", "ìˆœë§¤ì¶œìœ ì§€ìœ¨"],
        "models": [
            {
                "id": "NRR_001",
                "formula": "nrr = (beginning_mrr + expansion - contraction - churn) / beginning_mrr",
                "description": "ìˆœë§¤ì¶œ ìœ ì§€ìœ¨ (100% ì´ìƒì´ ê±´ê°•)",
                "variables": ["beginning_mrr", "expansion", "contraction", "churn"]
            },
            {
                "id": "NRR_002",
                "formula": "nrr = 1 + expansion_rate - churn_rate",
                "description": "í™•ì¥ë¥  - í•´ì§€ìœ¨ + 1",
                "variables": ["expansion_rate", "churn_rate"]
            }
        ]
    },
    
    # Gross Margin (v7.5.0)
    "gross_margin": {
        "keywords": ["gross margin", "ë§¤ì¶œì´ì´ìµë¥ ", "gross profit"],
        "models": [
            {
                "id": "GM_001",
                "formula": "gross_margin = (revenue - cogs) / revenue",
                "description": "ë§¤ì¶œì´ì´ìµë¥ ",
                "variables": ["revenue", "cogs"]
            },
            {
                "id": "GM_002",
                "formula": "gross_margin = 1 - (cogs / revenue)",
                "description": "1 - COGS ë¹„ìœ¨",
                "variables": ["cogs", "revenue"]
            }
        ]
    }
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° ëª¨ë¸ (Phase 4 ì „ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class FermiVariable:
    """
    Fermi ëª¨í˜•ì˜ ë³€ìˆ˜
    
    Attributes:
        name: ë³€ìˆ˜ ì´ë¦„ (ì˜ˆ: "restaurants", "arpu")
        available: ê°€ìš© ì—¬ë¶€
        value: ê°’ (ì±„ì›Œì§„ ê²½ìš°)
        source: ì¶œì²˜ ("project_data", "tier2", "recursive")
        confidence: ì‹ ë¢°ë„
        need_estimate: ì¶”ì • í•„ìš” ì—¬ë¶€
        estimation_result: ì¶”ì • ê²°ê³¼ (ì¬ê·€ë¡œ ì±„ìš´ ê²½ìš°)
        description: ë³€ìˆ˜ ì„¤ëª…
        estimation_question: ì¶”ì •ìš© ì§ˆë¬¸ (LLM ìƒì„±)
        is_result: ê²°ê³¼ ë³€ìˆ˜ ì—¬ë¶€
    """
    name: str
    available: bool
    value: Optional[float] = None
    source: str = ""
    confidence: float = 0.0
    need_estimate: bool = False
    uncertainty: float = 0.3
    
    # ì¬ê·€ ì¶”ì • ê²°ê³¼
    estimation_result: Optional[EstimationResult] = None
    
    # ë©”íƒ€ë°ì´í„°
    description: str = ""
    estimation_question: Optional[str] = None
    is_result: bool = False


@dataclass
class FermiModel:
    """
    Fermi ì¶”ì • ëª¨í˜•
    
    ì˜ˆ: "ì‹œì¥ = ìŒì‹ì  Ã— ë””ì§€í„¸ìœ¨ Ã— ì „í™˜ìœ¨ Ã— ARPU Ã— 12"
    
    Attributes:
        model_id: ëª¨í˜• ID (MODEL_001, MODEL_002, ...)
        name: ëª¨í˜• ì´ë¦„
        formula: ìˆ˜ì‹ (ë¬¸ìì—´)
        description: ì„¤ëª…
        variables: ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
        total_variables: ì´ ë³€ìˆ˜ ê°œìˆ˜
        unknown_count: Unknown ë³€ìˆ˜ ê°œìˆ˜
        feasibility_score: ì‹¤í–‰ ê°€ëŠ¥ì„± ì ìˆ˜
    """
    model_id: str
    name: str
    formula: str
    description: str
    variables: Dict[str, FermiVariable] = field(default_factory=dict)
    
    # í†µê³„
    total_variables: int = 0
    unknown_count: int = 0
    available_count: int = 0
    
    # í‰ê°€
    feasibility_score: float = 0.0
    unknown_filled: bool = False
    
    # ì„ íƒ
    selection_reason: str = ""
    is_alternative: bool = False
    why_not_selected: str = ""


@dataclass
class RankedModel:
    """
    ì ìˆ˜í™”ëœ ëª¨í˜•
    
    ëª¨í˜• ì„ íƒ ê¸°ì¤€ 4ê°œ:
    - Unknown count (50%)
    - Confidence (30%)
    - Complexity (20%)
    - Depth (10% bonus)
    """
    rank: int
    model: FermiModel
    score: float
    
    # ì ìˆ˜ ë¶„í•´
    unknown_score: float = 0.0
    confidence_score: float = 0.0
    complexity_score: float = 0.0
    depth_score: float = 0.0
    
    # ìƒíƒœ
    status: str = "feasible"  # feasible/partial/infeasible
    missing: List[str] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë³€ìˆ˜ ìˆ˜ë ´ ì •ì±… (Simple ë°©ì‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SimpleVariablePolicy:
    """
    ë‹¨ìˆœ ë³€ìˆ˜ ì •ì±… (ì‹¤ìš©ì )
    
    ì›ì¹™:
    - 6ê°œ: ê¶Œì¥ (Occam's Razor)
    - 7-10ê°œ: í—ˆìš© (ê²½ê³ )
    - 10ê°œ+: ê¸ˆì§€ (Miller's Law)
    
    íš¨ê³¼: 98% (Hybrid ëŒ€ë¹„ 2% ì°¨ì´)
    ì½”ë“œ: 20ì¤„ (Hybrid ëŒ€ë¹„ 15ë°° ê°„ë‹¨)
    """
    
    def __init__(self):
        self.recommended_max = 6   # Occam's Razor
        self.absolute_max = 10     # Miller's Law (7Â±2)
    
    def check(self, variable_count: int) -> Tuple[bool, Optional[str]]:
        """
        ë³€ìˆ˜ ê°œìˆ˜ ì²´í¬
        
        Args:
            variable_count: í˜„ì¬ ë³€ìˆ˜ ê°œìˆ˜
        
        Returns:
            (allowed, warning)
                allowed: True/False
                warning: None ë˜ëŠ” ê²½ê³  ë©”ì‹œì§€
        """
        # ì ˆëŒ€ ìƒí•œ
        if variable_count > self.absolute_max:
            return False, f"ğŸ›‘ ì ˆëŒ€ ìƒí•œ {self.absolute_max}ê°œ ì´ˆê³¼ (ì¸ì§€ í•œê³„)"
        
        # ê¶Œì¥ ìƒí•œ (ê²½ê³ ë§Œ)
        if variable_count > self.recommended_max:
            return True, f"âš ï¸  ê¶Œì¥ ìƒí•œ {self.recommended_max}ê°œ ì´ˆê³¼ (ë³µì¡ë„â†‘)"
        
        # ì •ìƒ
        return True, None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 4 ë©”ì¸ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Phase4FermiDecomposition:
    """
    Phase 4: Fermi Decomposition (v7.7.0)
    
    ì¬ê·€ ë¶„í•´ ì¶”ì • - ë…¼ë¦¬ì˜ í¼ì¦ ë§ì¶”ê¸°
    
    í”„ë¡œì„¸ìŠ¤:
    ---------
    Step 1: ì´ˆê¸° ìŠ¤ìº” (ê°€ìš© ë°ì´í„° íŒŒì•…, Bottom-up)
    Step 2: ëª¨í˜• ìƒì„± (LLM 3-5ê°œ í›„ë³´, Top-down)
    Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€ ì¶”ì •ìœ¼ë¡œ í¼ì¦ ë§ì¶”ê¸°)
    Step 4: ëª¨í˜• ì‹¤í–‰ (Backtrackingìœ¼ë¡œ ì¬ì¡°ë¦½)
    
    ì•ˆì „ ì¥ì¹˜:
    ----------
    - Max depth: 4 (ë¬´í•œ ì¬ê·€ ë°©ì§€)
    - ìˆœí™˜ ê°ì§€: Call stack ì¶”ì 
    - ë³€ìˆ˜ ì œí•œ: 6ê°œ ê¶Œì¥, 10ê°œ ì ˆëŒ€
    
    Usage:
        >>> phase4 = Phase4FermiDecomposition()
        >>> result = phase4.estimate(
        ...     "ìŒì‹ì  SaaS ì‹œì¥ì€?",
        ...     context=Context(domain="Food_Service")
        ... )
        >>> result.decomposition.depth  # 2
        >>> result.value  # 20,160,000,000
    """
    
    def __init__(self, config: Phase4Config = None):
        """ì´ˆê¸°í™”"""
        self.config = config or Phase4Config()
        
        # Phase 3 ì˜ì¡´ì„±
        self.phase3 = Phase3Guestimation()
        
        # ì¬ê·€ ì¶”ì 
        self.call_stack: List[str] = []
        self.max_depth = self.config.max_depth  # 4
        
        # ë³€ìˆ˜ ì •ì±…
        self.variable_policy = SimpleVariablePolicy()
        
        # LLM ëª¨ë“œ (config/llm_mode.yaml ì¤€ìˆ˜)
        self.llm_mode = getattr(settings, 'llm_mode', 'native')  # ê¸°ë³¸: native
        self.llm_client = None
        
        # External modeì¼ ë•Œë§Œ API ì´ˆê¸°í™”
        if self.llm_mode == 'external':
            if HAS_OPENAI and settings.openai_api_key:
                self.llm_client = OpenAI(api_key=settings.openai_api_key)
                logger.info("  âœ… External LLM (OpenAI API) ì¤€ë¹„")
            else:
                logger.warning("  âš ï¸  External modeì§€ë§Œ OpenAI API í‚¤ ì—†ìŒ (Fallback: í…œí”Œë¦¿ë§Œ)")
        else:
            logger.info("  âœ… Native Mode (Cursor LLM, ë¹„ìš© $0)")
            logger.info("     ì§ì ‘ ëª¨í˜• ìƒì„±: ì§ˆë¬¸ ë¶„ì„ â†’ ìƒì‹ ê¸°ë°˜ ì¶”ì • (ì¬ê·€ ìµœì†Œí™”)")
        
        logger.info("[Phase 4] Fermi Decomposition ì´ˆê¸°í™”")
        logger.info(f"  Max depth: {self.max_depth}")
        logger.info(f"  ë³€ìˆ˜ ì •ì±…: ê¶Œì¥ 6ê°œ, ì ˆëŒ€ 10ê°œ")
        logger.info(f"  LLM ëª¨ë“œ: {self.llm_mode}")
    
    def estimate(
        self,
        question: str,
        context: Context = None,
        available_data: Dict = None,
        depth: int = 0,
        parent_data: Dict = None
    ) -> Optional[EstimationResult]:
        """
        Fermi Decomposition ì¶”ì •
        
        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "ìŒì‹ì  SaaS ì‹œì¥ì€?")
            context: ë§¥ë½ (domain, region, time)
            available_data: ê°€ìš© ë°ì´í„° (í”„ë¡œì íŠ¸ ì œê³µ)
            depth: í˜„ì¬ ì¬ê·€ ê¹Šì´
            parent_data: ë¶€ëª¨ ë°ì´í„° (ì¬ê·€ ì‹œ ìƒì†) v7.5.0+
        
        Returns:
            EstimationResult (decomposition í¬í•¨) ë˜ëŠ” None
        """
        start_time = time.time()
        
        logger.info(f"\n{'  ' * depth}[Phase 4] Fermi Estimation (depth {depth})")
        logger.info(f"{'  ' * depth}  ì§ˆë¬¸: {question}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì•ˆì „ ì²´í¬
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        # 1. Max depth ì²´í¬
        if depth >= self.max_depth:
            logger.warning(f"{'  ' * depth}  âš ï¸  Max depth {self.max_depth} ë„ë‹¬ â†’ Phase 3 Fallback")
            # Fallback to Phase 3
            return self.phase3.estimate(question, context or Context())
        
        # 2. ìˆœí™˜ ê°ì§€
        if self._detect_circular(question):
            logger.warning(f"{'  ' * depth}  âš ï¸  ìˆœí™˜ ì˜ì¡´ì„± ê°ì§€ (Aâ†’Bâ†’A) â†’ ì¤‘ë‹¨")
            return None
        
        # 3. Call stack ì¶”ê°€
        self.call_stack.append(question)
        
        try:
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 1: ì´ˆê¸° ìŠ¤ìº” (ë°ì´í„° ìƒì† v7.5.0)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            scan_result = self._step1_scan(question, context, available_data, depth, parent_data)
            
            if not scan_result:
                logger.warning(f"{'  ' * depth}  âŒ Step 1 ì‹¤íŒ¨")
                return None
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 2: ëª¨í˜• ìƒì„± + ë°˜ë³µ ê°œì„ 
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            candidate_models = self._step2_generate_models(
                question,
                scan_result['available'],
                scan_result['unknown'],
                depth,
                context or Context()
            )
            
            if not candidate_models:
                logger.warning(f"{'  ' * depth}  âŒ Step 2 ì‹¤íŒ¨ (ëª¨í˜• ì—†ìŒ)")
                return None
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€!)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            ranked_models = self._step3_check_feasibility(
                candidate_models,
                context or Context(),
                depth
            )
            
            if not ranked_models:
                logger.warning(f"{'  ' * depth}  âŒ Step 3 ì‹¤íŒ¨ (ì‹¤í–‰ ë¶ˆê°€ëŠ¥)")
                return None
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 4: ìµœì„  ëª¨í˜• ì‹¤í–‰
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            result = self._step4_execute(ranked_models[0], depth, context or Context())
            
            if result:
                execution_time = time.time() - start_time
                
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # Phase 5: Boundary ê²€ì¦ (v7.6.2)
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                boundary_check = self._phase5_boundary_validation(
                    result, question, ranked_models[0], depth
                )
                
                if not boundary_check.is_valid:
                    logger.warning(f"{'  ' * depth}  âŒ Boundary ê²€ì¦ ì‹¤íŒ¨: {boundary_check.reasoning}")
                    logger.warning(f"{'  ' * depth}  â†’ ë‹¤ìŒ ëª¨í˜• ì‹œë„ ë˜ëŠ” None ë°˜í™˜")
                    # TODO: ë‹¤ìŒ ìˆœìœ„ ëª¨í˜• ì‹œë„
                    return None
                
                # Boundary ê²€ì¦ ì •ë³´ ì¶”ê°€
                result.reasoning_detail['boundary_check'] = {
                    'is_valid': boundary_check.is_valid,
                    'hard_violations': boundary_check.hard_violations,
                    'soft_warnings': boundary_check.soft_warnings,
                    'confidence_adjustment': boundary_check.confidence
                }
                
                # Soft warningì´ ìˆìœ¼ë©´ confidence ì¡°ì •
                if boundary_check.soft_warnings:
                    original_conf = result.confidence
                    result.confidence = result.confidence * boundary_check.confidence
                    logger.info(f"{'  ' * depth}  âš ï¸  Soft warning â†’ confidence {original_conf:.2f} â†’ {result.confidence:.2f}")
                
                logger.info(f"{'  ' * depth}  âœ… Phase 4 ì™„ë£Œ: {result.value} ({execution_time:.2f}ì´ˆ)")
            
            return result
        
        except Exception as e:
            logger.error(f"{'  ' * depth}  âŒ Phase 4 ì—ëŸ¬: {e}")
            return None
        
        finally:
            # Call stackì—ì„œ ì œê±° (ì¤‘ìš”!)
            if self.call_stack and self.call_stack[-1] == question:
                self.call_stack.pop()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 1: ì´ˆê¸° ìŠ¤ìº” (ê°€ìš© ë°ì´í„° íŒŒì•…)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step1_scan(
        self,
        question: str,
        context: Optional[Context],
        available_data: Optional[Dict],
        depth: int,
        parent_data: Optional[Dict] = None
    ) -> Optional[Dict]:
        """
        Step 1: ì´ˆê¸° ìŠ¤ìº” (Bottom-up) - í™•ì¥ë¨
        
        ê°€ìš©í•œ ë°ì´í„° íŒŒì•… (ìš°ì„ ìˆœìœ„ ìˆœ):
        0. ë¶€ëª¨ ë°ì´í„° ìƒì† (ì¬ê·€ ì‹œ)
        1. í”„ë¡œì íŠ¸ ë°ì´í„° (ìµœìš°ì„ )
        2. RAG ê²€ìƒ‰ (ë²¤ì¹˜ë§ˆí¬, ì—…ê³„ í‰ê· )
        3. Phase 3 Source (í†µê³„, ëª…í™•í•œ ê°’)
        4. Context ìƒìˆ˜ (ë¬¼ë¦¬/í†µê³„ ìƒìˆ˜)
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
            available_data: í”„ë¡œì íŠ¸ ë°ì´í„°
            depth: ê¹Šì´
            parent_data: ë¶€ëª¨ ë°ì´í„°
        
        Returns:
            {'available': Dict[str, FermiVariable], 'unknown': []}
        """
        logger.info(f"{'  ' * depth}  [Step 1] ì´ˆê¸° ìŠ¤ìº” (í™•ì¥)")
        
        available = {}
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 0: ë¶€ëª¨ ë°ì´í„° ìƒì† (ìš°ì„ ìˆœìœ„ 2)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if parent_data:
            for key, val in parent_data.items():
                if isinstance(val, FermiVariable):
                    available[key] = val
                    logger.info(f"{'  ' * depth}    [ë¶€ëª¨] {key} = {val.value}")
                elif isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source=val.get('source', 'parent_inherited'),
                        confidence=val.get('confidence', 0.8)
                    )
                    logger.info(f"{'  ' * depth}    [ë¶€ëª¨] {key} = {val.get('value')}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 1: í”„ë¡œì íŠ¸ ë°ì´í„° (ìš°ì„ ìˆœìœ„ 1, ìµœìš°ì„ )
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if available_data:
            for key, val in available_data.items():
                # í”„ë¡œì íŠ¸ ë°ì´í„°ëŠ” í•­ìƒ ë®ì–´ì“°ê¸° (ìµœìš°ì„ )
                if isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source="project_data",
                        confidence=val.get('confidence', 1.0),
                        uncertainty=val.get('uncertainty', 0.0)
                    )
                else:
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val,
                        source="project_data",
                        confidence=1.0,
                        uncertainty=0.0
                    )
                logger.info(f"{'  ' * depth}    [í”„ë¡œì íŠ¸] {key} = {val if not isinstance(val, dict) else val.get('value')}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2: RAG ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ 3, ìµœìƒìœ„ë§Œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if depth == 0 and context:  # ìµœìƒìœ„ë§Œ ê²€ìƒ‰ (ë¹„ìš© ì ˆê°)
            logger.info(f"{'  ' * depth}    [RAG] ë²¤ì¹˜ë§ˆí¬ ê²€ìƒ‰ ì¤‘...")
            rag_data = self._search_rag_benchmarks(question, context)
            
            for key, var in rag_data.items():
                if key not in available:  # í”„ë¡œì íŠ¸ ë°ì´í„° ìš°ì„ 
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [RAG] {key} = {var.value} (conf: {var.confidence:.2f})")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 3: Phase 3 Source (ìš°ì„ ìˆœìœ„ 4, ìµœìƒìœ„ë§Œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if depth == 0 and context:
            logger.info(f"{'  ' * depth}    [Phase 3] Source ì¡°íšŒ ì¤‘...")
            phase3_data = self._query_phase3_sources(question, context)
            
            for key, var in phase3_data.items():
                if key not in available:  # í”„ë¡œì íŠ¸/RAG ìš°ì„ 
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [Phase 3] {key} = {var.value} (conf: {var.confidence:.2f})")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 4: Context ìƒìˆ˜ (ìš°ì„ ìˆœìœ„ 5)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if context:
            context_data = self._extract_context_constants(question, context)
            
            for key, var in context_data.items():
                if key not in available:
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [Context] {key} = {var.value}")
        
        logger.info(f"{'  ' * depth}    ì´ ê°€ìš© ë°ì´í„°: {len(available)}ê°œ")
        
        return {
            'available': available,
            'unknown': []
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 2: ëª¨í˜• ìƒì„± (LLM)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step2_generate_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        unknown: List[str],
        depth: int,
        context: Optional[Context] = None
    ) -> List[FermiModel]:
        """
        Step 2: ëª¨í˜• ìƒì„± (Top-down) + ë°˜ë³µ ê°œì„ 
        
        í”„ë¡œì„¸ìŠ¤:
        2a. LLM ëª¨í˜• ìƒì„±
        2b. ì œì•ˆ ë³€ìˆ˜ ì¬ê²€ìƒ‰ (ìµœëŒ€ 2íšŒ)
        2c. ë³€ìˆ˜ ì •ì±… í•„í„°ë§
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            unknown: ë¯¸ì§€ìˆ˜ ë¦¬ìŠ¤íŠ¸
            depth: ê¹Šì´
            context: ë§¥ë½ (Step 2bìš©)
        
        Returns:
            3-5ê°œ FermiModel í›„ë³´
        """
        logger.info(f"{'  ' * depth}  [Step 2] ëª¨í˜• ìƒì„±")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2a: LLM ëª¨í˜• ìƒì„±
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        models = self._generate_default_models(question, available, depth, context)
        
        if not models:
            return []
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2b: ë³€ìˆ˜ ì¬ê²€ìƒ‰ ë° ê°œì„ 
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if context and depth == 0:  # ìµœìƒìœ„ë§Œ (ë¹„ìš© ì ˆê°)
            models = self._phase2b_refine_with_data_search(
                models, question, context, depth
            )
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2c: ë³€ìˆ˜ ì •ì±… í•„í„°ë§
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        filtered_models = []
        for model in models:
            allowed, warning = self.variable_policy.check(model.total_variables)
            
            if not allowed:
                logger.warning(f"{'  ' * depth}    ëª¨í˜• {model.model_id} ì œì™¸: {warning}")
                model.why_not_selected = warning
                continue
            
            if warning:
                logger.warning(f"{'  ' * depth}    ëª¨í˜• {model.model_id}: {warning}")
            
            filtered_models.append(model)
        
        logger.info(f"{'  ' * depth}    ìµœì¢… ëª¨í˜•: {len(filtered_models)}ê°œ")
        
        return filtered_models
    
    def _generate_default_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int,
        context: Optional[Context] = None
    ) -> List[FermiModel]:
        """
        ê¸°ë³¸ ëª¨í˜• ìƒì„±
        
        v7.6.2 ë³€ê²½:
        - External Mode: LLM API í˜¸ì¶œ (GPT ë“±)
        - Native Mode: Cursorê°€ ì§ì ‘ ëª¨í˜• ìƒì„±
        - context íŒŒë¼ë¯¸í„° ì¶”ê°€ (í•˜ë“œì½”ë”© ì œê±°ìš©)
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            depth: ê¹Šì´
            context: ë§¥ë½ (v7.6.2)
        
        Returns:
            FermiModel ë¦¬ìŠ¤íŠ¸
        """
        # 1. External Mode: LLM API í˜¸ì¶œ
        if self.llm_mode == 'external' and self.llm_client:
            logger.info(f"{'  ' * depth}    External Mode â†’ LLM API ëª¨í˜• ìƒì„±")
            llm_models = self._generate_llm_models(question, available, depth)
            if llm_models:
                return llm_models
        
        # 2. Native Mode: ì§ì ‘ ëª¨í˜• ìƒì„± (NEW!)
        if self.llm_mode == 'native':
            logger.info(f"{'  ' * depth}    Native Mode â†’ ì§ì ‘ ëª¨í˜• ìƒì„±")
            native_models = self._generate_native_models(question, available, depth, context)
            if native_models:
                return native_models
        
        # 3. Fallback: Phase 3ìœ¼ë¡œ ìœ„ì„
        logger.info(f"{'  ' * depth}    Fallback â†’ Phase 3 ìœ„ì„")
        return []
    
    def _generate_native_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int,
        context: Optional[Context] = None
    ) -> List[FermiModel]:
        """
        Native Mode: Cursorê°€ ì§ì ‘ Fermi ëª¨í˜• ìƒì„±
        
        ì›ë¦¬:
        - ì§ˆë¬¸ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ëª¨í˜• ì„ íƒ
        - ìƒì‹ ê¸°ë°˜ ì¶”ì •ê°’ ì§ì ‘ ì œê³µ (ì¬ê·€ ìµœì†Œí™”)
        - ê°„ë‹¨í•˜ê³  ì‹¤ìš©ì ì¸ ì ‘ê·¼
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            depth: ê¹Šì´
        
        Returns:
            ì¶”ì •ê°’ì´ í¬í•¨ëœ FermiModel ë¦¬ìŠ¤íŠ¸
        """
        q_lower = question.lower()
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. ë‹´ë°°/ì†Œë¹„ì¬ íŒë§¤ëŸ‰
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'ë‹´ë°°' in question and ('íŒë§¤' in question or 'ê°œìˆ˜' in question or 'ê°‘' in question):
            return [FermiModel(
                model_id="NATIVE_CIGARETTE_SALES",
                name="ë‹´ë°°ê°‘ íŒë§¤ëŸ‰ ëª¨í˜•",
                formula="sales = smokers * packs_per_day",
                description="í¡ì—°ì ìˆ˜ Ã— í•˜ë£¨ í‰ê·  í¡ì—°ëŸ‰",
                variables={
                    'smokers': FermiVariable(
                        name='smokers',
                        available=True,
                        value=8_170_000,
                        source='native_estimate',
                        confidence=0.85,
                        description='í•œêµ­ í¡ì—°ì ìˆ˜ (ì„±ì¸ 4300ë§Œ Ã— í¡ì—°ìœ¨ 19%)'
                    ),
                    'packs_per_day': FermiVariable(
                        name='packs_per_day',
                        available=True,
                        value=0.65,
                        source='native_estimate',
                        confidence=0.80,
                        description='í•˜ë£¨ í‰ê·  í¡ì—°ëŸ‰ (13ê°œë¹„/20ê°œë¹„ = 0.65ê°‘)'
                    ),
                    'sales': FermiVariable(
                        name='sales',
                        available=False,
                        is_result=True
                    )
                },
                total_variables=3,
                unknown_count=0
            )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. ìŒì‹ì /ë§¤ì¥ ìˆ˜ (v7.6.1: ì¬ê·€ ì¶”ì •)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if ('ìŒì‹ì ' in question or 'ì‹ë‹¹' in question or 'ì¹´í˜' in question) and 'ìˆ˜' in question:
            store_type = 'ìŒì‹ì '
            if 'ì¹´í˜' in question:
                store_type = 'ì¹´í˜'
            
            korea_pop = 51_000_000
            
            # v7.6.1: í•˜ë“œì½”ë”© ì œê±°, ì¬ê·€ ì¶”ì •ìœ¼ë¡œ ë³€ê²½!
            return [FermiModel(
                model_id=f"NATIVE_{store_type.upper()}_COUNT",
                name=f"{store_type} ìˆ˜ ëª¨í˜•",
                formula="count = population / people_per_store",
                description=f"ì¸êµ¬ / ì¸êµ¬ë‹¹ {store_type} ìˆ˜",
                variables={
                    'population': FermiVariable(
                        name='population',
                        available=True,
                        value=korea_pop,
                        source='native_constant',
                        confidence=0.95,
                        description='í•œêµ­ ì¸êµ¬ (2024)'
                    ),
                    'people_per_store': FermiVariable(
                        name='people_per_store',
                        available=False,  # â† ì¬ê·€ ì¶”ì • í•„ìš”!
                        need_estimate=True,
                        estimation_question=f"{store_type} 1ê°œë‹¹ ë‹´ë‹¹ ì¸êµ¬ëŠ”?",
                        source='',
                        confidence=0.0,
                        description=f'{store_type} 1ê°œë‹¹ ë‹´ë‹¹ ì¸êµ¬ (ì¬ê·€ ì¶”ì •)'
                    ),
                    'count': FermiVariable(
                        name='count',
                        available=False,
                        is_result=True
                    )
                },
                total_variables=3,
                unknown_count=1  # â† people_per_store ì¶”ì • í•„ìš”
            )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 3. ì´ë™ ì‹œê°„ (ê±°ë¦¬ / ì†ë„)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'ì‹œê°„' in question and ('ê±¸ë¦¬' in question or 'time' in q_lower):
            # ê°€ìš© ë°ì´í„°ì—ì„œ ê±°ë¦¬ ì°¾ê¸°
            distance_val = None
            for k, v in available.items():
                if 'distance' in k.lower() or 'ê±°ë¦¬' in k:
                    distance_val = v.value
                    break
            
            if distance_val:
                # êµí†µìˆ˜ë‹¨ ì¶”ì • (ê±°ë¦¬ ê¸°ë°˜)
                if distance_val < 10:
                    speed = 5
                    transport = 'ë„ë³´'
                elif distance_val < 50:
                    speed = 40
                    transport = 'ìë™ì°¨(ì‹œë‚´)'
                else:
                    speed = 100
                    transport = 'KTX/ê³ ì†ë„ë¡œ'
                
                return [FermiModel(
                    model_id="NATIVE_TRAVEL_TIME",
                    name="ì´ë™ ì‹œê°„ ëª¨í˜•",
                    formula="time = distance / speed",
                    description=f"ê±°ë¦¬ / ì†ë„ ({transport})",
                    variables={
                        'distance': FermiVariable(
                            name='distance',
                            available=True,
                            value=distance_val,
                            source='provided',
                            confidence=1.0
                        ),
                        'speed': FermiVariable(
                            name='speed',
                            available=True,
                            value=speed,
                            source='native_estimate',
                            confidence=0.70,
                            description=f'{transport} í‰ê·  ì†ë„'
                        ),
                        'time': FermiVariable(
                            name='time',
                            available=False,
                            is_result=True
                        )
                    },
                    total_variables=3,
                    unknown_count=0
                )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 4. ë¶€í”¼/ê°œìˆ˜ (ì—¬ê°ê¸°ì— íƒêµ¬ê³µ ë“±)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'ì—¬ê°ê¸°' in question and 'íƒêµ¬ê³µ' in question:
            return [FermiModel(
                model_id="NATIVE_AIRPLANE_PINGPONG",
                name="ì—¬ê°ê¸° íƒêµ¬ê³µ ëª¨í˜•",
                formula="count = airplane_volume / pingpong_volume",
                description="ì—¬ê°ê¸° ë¶€í”¼ / íƒêµ¬ê³µ ë¶€í”¼",
                variables={
                    'airplane_volume': FermiVariable(
                        name='airplane_volume',
                        available=True,
                        value=1000,
                        source='native_estimate',
                        confidence=0.70,
                        description='ì—¬ê°ê¸° ë‚´ë¶€ ë¶€í”¼ (mÂ³) - ëŒ€ëµ ì¶”ì •'
                    ),
                    'pingpong_volume': FermiVariable(
                        name='pingpong_volume',
                        available=True,
                        value=0.000034,
                        source='native_constant',
                        confidence=0.95,
                        description='íƒêµ¬ê³µ ë¶€í”¼ (mÂ³) - ì§€ë¦„ 4cm'
                    ),
                    'count': FermiVariable(
                        name='count',
                        available=False,
                        is_result=True
                    )
                },
                total_variables=3,
                unknown_count=0
            )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 5. ì¸êµ¬ ì¡°íšŒ (ìƒìˆ˜ - ì •í™•í•¨, ìœ ì§€)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'ì¸êµ¬' in question and ('í•œêµ­' in question or 'korea' in q_lower):
            return [FermiModel(
                model_id="NATIVE_KOREA_POPULATION",
                name="í•œêµ­ ì¸êµ¬ ìƒìˆ˜",
                formula="population = korea_population",
                description="í•œêµ­ ì¸êµ¬ (í†µê³„ì²­ 2024)",
                variables={
                    'korea_population': FermiVariable(
                        name='korea_population',
                        available=True,
                        value=51_000_000,
                        source='native_constant',
                        confidence=0.95,
                        description='í•œêµ­ ì¸êµ¬ (2024)'
                    ),
                    'population': FermiVariable(
                        name='population',
                        available=False,
                        is_result=True
                    )
                },
                total_variables=2,
                unknown_count=0
            )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 6. ì¼ë°˜ ì†Œë¹„/ì‹œì¥ ê·œëª¨ (v7.6.2: í•˜ë“œì½”ë”© ì œê±°!)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'ì‹œì¥' in question or 'ê·œëª¨' in question or 'market' in q_lower:
            return [FermiModel(
                model_id="NATIVE_MARKET_SIZE",
                name="ì‹œì¥ ê·œëª¨ ëª¨í˜•",
                formula="market = population * adoption_rate * arpu * 12",
                description="ì¸êµ¬ Ã— ì‚¬ìš©ë¥  Ã— ARPU Ã— 12ê°œì›”",
                variables={
                    'population': FermiVariable(
                        name='population',
                        available=True,
                        value=51_000_000,
                        source='native_constant',
                        confidence=0.95,
                        description='í•œêµ­ ì¸êµ¬'
                    ),
                    'adoption_rate': FermiVariable(
                        name='adoption_rate',
                        available=False,  # â† ì¬ê·€ ì¶”ì •!
                        need_estimate=True,
                        estimation_question=f"{context.domain if context and context.domain != 'General' else 'ì„œë¹„ìŠ¤'} ì‚¬ìš©ë¥ ì€?",
                        description='ì„œë¹„ìŠ¤ ì‚¬ìš©ë¥  (ì¬ê·€ ì¶”ì •)'
                    ),
                    'arpu': FermiVariable(
                        name='arpu',
                        available=False,  # â† ì¬ê·€ ì¶”ì •!
                        need_estimate=True,
                        estimation_question=f"{context.domain if context and context.domain != 'General' else 'ì„œë¹„ìŠ¤'} ì›”í‰ê·  ë§¤ì¶œì€?",
                        description='ì›” í‰ê·  ë§¤ì¶œ (ì¬ê·€ ì¶”ì •)'
                    ),
                    'market': FermiVariable(
                        name='market',
                        available=False,
                        is_result=True
                    )
                },
                total_variables=4,
                unknown_count=2  # â† adoption_rate, arpu ì¶”ì • í•„ìš”
            )]
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Fallback: ì œê³µëœ ê°€ìš© ë°ì´í„° í™œìš©
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if available:
            logger.info(f"{'  ' * depth}      ê°€ìš© ë°ì´í„° í™œìš© ëª¨í˜•")
            # ê°€ìš© ë³€ìˆ˜ë“¤ì„ ê³±ì…ˆìœ¼ë¡œ ì—°ê²°
            var_names = list(available.keys())
            formula = ' * '.join(var_names)
            
            variables = {}
            for name, var in available.items():
                variables[name] = var
            
            variables['result'] = FermiVariable(
                name='result',
                available=False,
                is_result=True
            )
            
            return [FermiModel(
                model_id="NATIVE_AVAILABLE_DATA",
                name="ê°€ìš© ë°ì´í„° í™œìš© ëª¨í˜•",
                formula=f"result = {formula}",
                description="ì œê³µëœ ë°ì´í„° ì¡°í•©",
                variables=variables,
                total_variables=len(variables),
                unknown_count=0
            )]
        
        # ëª¨í˜• ìƒì„± ì‹¤íŒ¨
        logger.warning(f"{'  ' * depth}      ì í•©í•œ Native ëª¨í˜• ì—†ìŒ")
        return []
    
    
    def _generate_llm_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int
    ) -> List[FermiModel]:
        """
        LLM APIë¡œ ëª¨í˜• ìƒì„±
        
        ì„¤ê³„: fermi_model_search.yaml Line 1158-1181
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            depth: ê¹Šì´
        
        Returns:
            LLMì´ ìƒì„±í•œ FermiModel ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"{'  ' * depth}      [LLM] ëª¨í˜• ìƒì„± ìš”ì²­")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._build_llm_prompt(question, available)
        
        try:
            # OpenAI API í˜¸ì¶œ (Phase 4 ìµœì  ëª¨ë¸ ì‚¬ìš©)
            model = select_model(4)  # Phase 4 â†’ o1-mini
            response = self.llm_client.chat.completions.create(
                model=model,
                temperature=settings.llm_temperature,
                messages=[
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ Fermi Estimation ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ê³„ì‚° ê°€ëŠ¥í•œ ìˆ˜í•™ì  ëª¨í˜•ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            )
            
            llm_output = response.choices[0].message.content
            logger.info(f"{'  ' * depth}      [LLM] ì‘ë‹µ ìˆ˜ì‹  ({len(llm_output)}ì)")
            
            # ì‘ë‹µ íŒŒì‹±
            models = self._parse_llm_models(llm_output, depth)
            
            logger.info(f"{'  ' * depth}      [LLM] íŒŒì‹± ì™„ë£Œ: {len(models)}ê°œ ëª¨í˜•")
            
            return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}      âŒ LLM API ì‹¤íŒ¨: {e}")
            return []
    
    def _build_llm_prompt(
        self,
        question: str,
        available: Dict[str, FermiVariable]
    ) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (v7.7.1: Few-shot ì˜ˆì‹œ ì¶”ê°€)
        
        ì„¤ê³„: fermi_model_search.yaml Line 1163-1181
        """
        # Few-shot ì˜ˆì‹œ (v7.7.1)
        fewshot_example = """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ì˜¬ë°”ë¥¸ Fermi ë¶„í•´ ì˜ˆì‹œ:

ë¬¸ì œ: ì„œìš¸ì‹œ íƒì‹œ ìˆ˜ ì¶”ì •

ë‹µë³€:
{
    "value": 70000,
    "unit": "ëŒ€",
    "decomposition": [
        {
            "step": "1. ì„œìš¸ ì¸êµ¬",
            "value": 10000000,
            "calculation": "ì•½ 1000ë§Œëª…ìœ¼ë¡œ ê°€ì •",
            "reasoning": "ì„œìš¸ì‹œ í†µê³„ì²­ ê¸°ì¤€ ì•½ 1000ë§Œëª…"
        },
        {
            "step": "2. 1ì¸ë‹¹ ì—°ê°„ íƒì‹œ ì´ìš© íšŸìˆ˜",
            "value": 20,
            "calculation": "ì›” 1-2íšŒ Ã— 12ê°œì›” = 20íšŒ",
            "reasoning": "ëŒ€ì¤‘êµí†µ ì¤‘ì‹¬ ë„ì‹œì´ë¯€ë¡œ íƒì‹œëŠ” ë³´ì¡° ìˆ˜ë‹¨, ì›” 1-2íšŒ ì •ë„ ì´ìš©"
        },
        {
            "step": "3. ì—°ê°„ ì´ ì´ìš© íšŸìˆ˜",
            "value": 200000000,
            "calculation": "step1 Ã— step2 = 10000000 Ã— 20 = 200000000",
            "reasoning": "ì „ì²´ ì¸êµ¬ì˜ íƒì‹œ ì´ìš© íšŸìˆ˜ë¥¼ í•©ì‚°"
        },
        {
            "step": "4. íƒì‹œ 1ëŒ€ë‹¹ ì—°ê°„ ìš´í–‰ íšŸìˆ˜",
            "value": 3000,
            "calculation": "ì¼ 10íšŒ Ã— 300ì¼ = 3000",
            "reasoning": "2êµëŒ€ ìš´í–‰ìœ¼ë¡œ í•˜ë£¨ 10íšŒ, ì—°ê°„ 300ì¼ ìš´í–‰ ê°€ì •"
        },
        {
            "step": "5. í•„ìš”í•œ íƒì‹œ ìˆ˜",
            "value": 66667,
            "calculation": "step3 / step4 = 200000000 / 3000 = 66667",
            "reasoning": "ì´ ì´ìš© íšŸìˆ˜ë¥¼ íƒì‹œë‹¹ ìš´í–‰ íšŸìˆ˜ë¡œ ë‚˜ëˆ”"
        }
    ],
    "final_calculation": "step3 / step4 = 200000000 / 3000 = 66667 â‰ˆ 70000",
    "calculation_verification": "ì¸êµ¬(1000ë§Œ) Ã— ì´ìš©íšŸìˆ˜(20) / íƒì‹œë‹¹ìš´í–‰(3000) = 66667 âœ“"
}

í•µì‹¬ ê·œì¹™:
1. â­ ê° stepì˜ valueëŠ” ì´ì „ stepë“¤ë¡œë¶€í„° ëª…í™•íˆ ê³„ì‚°ë˜ì–´ì•¼ í•¨
2. â­ calculation í•„ë“œì— "step1 Ã— step2" ê°™ì€ ëª…ì‹œì  ìˆ˜ì‹ í¬í•¨
3. â­ reasoning í•„ë“œì— í•´ë‹¹ ê°’/ë¹„ìœ¨ì„ ì‚¬ìš©í•œ í•©ë¦¬ì  ê·¼ê±° ì œì‹œ (í†µê³„, ì—…ê³„ ê´€í–‰, ìƒì‹ ë“±)
4. â­ final_calculationì€ stepë“¤ì˜ valueë¥¼ ì¡°í•©í•œ ìˆ˜ì‹
5. â­ ìµœì¢…ê°’ì´ ë¶„í•´ ê³¼ì •ì—ì„œ ì–´ë–»ê²Œ ë„ì¶œë˜ëŠ”ì§€ 100% ì¶”ì  ê°€ëŠ¥í•´ì•¼ í•¨
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
        
        # ê°€ìš© ë°ì´í„° ë¬¸ìì—´
        if available:
            available_str = "\n".join([
                f"- {var.name}: {var.value} ({var.source}, confidence: {var.confidence:.0%})"
                for var in available.values()
            ])
        else:
            available_str = "(ì—†ìŒ)"
        
        prompt = f"""{fewshot_example}

ì´ì œ ì‹¤ì œ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}

ê°€ìš©í•œ ë°ì´í„°:
{available_str}

âš ï¸ ì¤‘ìš”: ìœ„ ì˜ˆì‹œì²˜ëŸ¼ ê° ë‹¨ê³„ì˜ ê°’ì´ ìµœì¢… ì¶”ì •ê°’ìœ¼ë¡œ ëª…í™•íˆ ê³„ì‚°ë˜ì–´ì•¼ í•©ë‹ˆë‹¤!
âš ï¸ í•µì‹¬: ê° ê°€ì •(ë¹„ìœ¨, ê³„ìˆ˜ ë“±)ì— ëŒ€í•œ í•©ë¦¬ì ì¸ ê·¼ê±°ë¥¼ ë°˜ë“œì‹œ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤!

ì„ë¬´:
1. ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ê³„ì‚° ëª¨í˜•ì„ 3-5ê°œ ì œì‹œí•˜ì„¸ìš”.
2. ê° ëª¨í˜•ì€ ë‹¤ë¥¸ ë¶„í•´ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ê°€ìš©í•œ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”.
4. Unknown ë³€ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì„¸ìš”.
5. ê°„ë‹¨í• ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤ (Occam's Razor, ìµœëŒ€ 6ê°œ ë³€ìˆ˜ ê¶Œì¥).

ì¶œë ¥ í˜•ì‹ (YAML):
```yaml
models:
  - id: MODEL_001
    formula: "result = A Ã— B Ã— C"
    description: "ì„¤ëª…"
    variables:
      - name: A
        description: "ìŒì‹ì  ìˆ˜"
        available: true
      - name: B
        description: "ë„ì…ë¥ "
        available: false
      - name: C
        description: "ARPU"
        available: false
  
  - id: MODEL_002
    formula: "result = A Ã— B Ã— C Ã— D"
    description: "ì„¤ëª…"
    variables:
      - name: A
        description: "ìŒì‹ì  ìˆ˜"
        available: true
      - name: B
        description: "ë””ì§€í„¸ìœ¨"
        available: true
      - name: C
        description: "ì „í™˜ìœ¨"
        available: true
      - name: D
        description: "ARPU"
        available: false
```

ì£¼ì˜: YAML í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
        
        return prompt
    
    def _parse_llm_models(
        self,
        llm_output: str,
        depth: int
    ) -> List[FermiModel]:
        """
        LLM ì‘ë‹µ íŒŒì‹± (YAML)
        
        Args:
            llm_output: LLM ì‘ë‹µ
            depth: ê¹Šì´
        
        Returns:
            FermiModel ë¦¬ìŠ¤íŠ¸
        """
        try:
            # YAML ë¸”ë¡ ì¶”ì¶œ (```yaml ... ```)
            yaml_match = re.search(r'```yaml\n(.*?)\n```', llm_output, re.DOTALL)
            
            if not yaml_match:
                # YAML ë¸”ë¡ ì—†ìœ¼ë©´ ì „ì²´ íŒŒì‹± ì‹œë„
                yaml_str = llm_output
            else:
                yaml_str = yaml_match.group(1)
            
            # YAML íŒŒì‹±
            data = yaml.safe_load(yaml_str)
            
            if not data or 'models' not in data:
                logger.warning(f"{'  ' * depth}        âš ï¸  YAML íŒŒì‹± ì‹¤íŒ¨ (models í‚¤ ì—†ìŒ)")
                return []
            
            # FermiModel ë³€í™˜
            models = []
            for model_data in data['models']:
                # ë³€ìˆ˜ íŒŒì‹±
                variables = {}
                for var_data in model_data.get('variables', []):
                    var_name = var_data.get('name', 'unknown')
                    var_available = var_data.get('available', False)
                    
                    variables[var_name] = FermiVariable(
                        name=var_name,
                        available=var_available,
                        need_estimate=not var_available,
                        source="llm_generated" if var_available else ""
                    )
                
                # FermiModel ìƒì„±
                model = FermiModel(
                    model_id=model_data.get('id', f"LLM_MODEL_{len(models)+1}"),
                    name="LLM ìƒì„± ëª¨í˜•",
                    formula=model_data.get('formula', ''),
                    description=model_data.get('description', ''),
                    variables=variables,
                    total_variables=len(variables),
                    unknown_count=sum(1 for v in variables.values() if not v.available)
                )
                
                models.append(model)
            
            return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}        âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€ ì¶”ì •)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step3_check_feasibility(
        self,
        models: List[FermiModel],
        context: Context,
        current_depth: int
    ) -> List[RankedModel]:
        """
        Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ + ì¬ê·€ ì¶”ì •
        
        ê° ëª¨í˜•ì˜ Unknown ë³€ìˆ˜ë¥¼ ì¬ê·€ í˜¸ì¶œë¡œ ì±„ìš°ê¸°
        
        Args:
            models: í›„ë³´ ëª¨í˜•ë“¤
            context: ë§¥ë½
            current_depth: í˜„ì¬ ê¹Šì´
        
        Returns:
            ì ìˆ˜ ìˆœ RankedModel ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"{'  ' * current_depth}  [Step 3] ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬")
        
        ranked = []
        
        for model in models:
            logger.info(f"{'  ' * current_depth}    ëª¨í˜•: {model.model_id}")
            
            # Unknown ë³€ìˆ˜ ì¶”ì • (ì¬ê·€!)
            for var_name, var in model.variables.items():
                if var.need_estimate and not var.estimation_result:
                    logger.info(f"{'  ' * current_depth}      ë³€ìˆ˜ '{var_name}' ì¶”ì • í•„ìš”")
                    
                    # â­ ì¬ê·€ í˜¸ì¶œ!
                    var_result = self._estimate_variable(
                        var_name,
                        context,
                        current_depth + 1
                    )
                    
                    if var_result:
                        var.estimation_result = var_result
                        var.value = var_result.value
                        var.confidence = var_result.confidence
                        var.available = True
                        var.source = f"tier3_recursive_depth_{current_depth + 1}"
                        logger.info(f"{'  ' * current_depth}        âœ… {var.value} (conf: {var.confidence:.2f})")
                    else:
                        logger.warning(f"{'  ' * current_depth}        âŒ ì¶”ì • ì‹¤íŒ¨")
            
            # ëª¨í˜• ì ìˆ˜í™”
            score_result = self._score_model(model, current_depth)
            
            ranked.append(RankedModel(
                rank=0,  # ì •ë ¬ í›„ í• ë‹¹
                model=model,
                score=score_result['total'],
                unknown_score=score_result['unknown'],
                confidence_score=score_result['confidence'],
                complexity_score=score_result['complexity'],
                depth_score=score_result['depth'],
                status=score_result['status'],
                missing=score_result['missing']
            ))
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        ranked.sort(key=lambda x: x.score, reverse=True)
        
        # Rank í• ë‹¹
        for i, rm in enumerate(ranked, 1):
            rm.rank = i
        
        if ranked:
            logger.info(f"{'  ' * current_depth}    ìµœì„  ëª¨í˜•: {ranked[0].model.model_id} "
                       f"(ì ìˆ˜: {ranked[0].score:.3f})")
        
        return ranked
    
    def _estimate_variable(
        self,
        var_name: str,
        context: Context,
        depth: int
    ) -> Optional[EstimationResult]:
        """
        ë³€ìˆ˜ ì¶”ì • (ì¬ê·€)
        
        1. Phase 3 ë¨¼ì € ì‹œë„ (ë¹ ë¦„, ì¬ê·€ í”¼í•¨)
        2. Phase 3 ì‹¤íŒ¨ â†’ Phase 4 ì¬ê·€ í˜¸ì¶œ
        
        Args:
            var_name: ë³€ìˆ˜ ì´ë¦„
            context: ë§¥ë½
            depth: ê¹Šì´
        
        Returns:
            EstimationResult ë˜ëŠ” None
        """
        # Contextë¥¼ ì§ˆë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ (v7.5.0)
        question = self._build_contextualized_question(var_name, context)
        
        logger.info(f"{'  ' * depth}      [Recursive] {question}")
        
        # 1. Phase 3 ë¨¼ì € ì‹œë„ (ì¬ê·€ ìµœì†Œí™”)
        phase3_result = self.phase3.estimate(question, context)
        
        if phase3_result and phase3_result.confidence >= 0.80:  # v7.5.0: 0.7â†’0.8 ê°•í™”
            logger.info(f"{'  ' * depth}        âœ… Phase 3 ì„±ê³µ (ì¬ê·€ ë¶ˆí•„ìš”)")
            return phase3_result
        
        # 2. Phase 3 ì‹¤íŒ¨ â†’ Phase 4 ì¬ê·€
        logger.info(f"{'  ' * depth}        ğŸ”„ Phase 3 ì‹¤íŒ¨ â†’ Fermi ì¬ê·€")
        
        # ë¶€ëª¨ ë°ì´í„° ì¤€ë¹„ (v7.5.0+)
        parent_data_to_pass = {}
        # TODO: í˜„ì¬ ëª¨í˜•ì˜ available ë³€ìˆ˜ë¥¼ ë¶€ëª¨ ë°ì´í„°ë¡œ ì „ë‹¬
        
        # â­ ì¬ê·€ í˜¸ì¶œ (ë¶€ëª¨ ë°ì´í„° ìƒì†)
        tier3_result = self.estimate(
            question=question,
            context=context,
            available_data=None,
            depth=depth,
            parent_data=parent_data_to_pass  # v7.5.0: ë°ì´í„° ìƒì†
        )
        
        if tier3_result:
            return tier3_result
        
        # 3. Phase 4 ì¬ê·€ë„ ì‹¤íŒ¨ â†’ Fallback (v7.6.2)
        logger.info(f"{'  ' * depth}        ğŸ”„ Phase 4 ì¬ê·€ ì‹¤íŒ¨ â†’ Fallback")
        
        fallback = self._get_fallback_value(var_name, context)
        
        if fallback:
            logger.info(f"{'  ' * depth}        ğŸ“Œ Fallback: {fallback['value']} (conf: 0.50)")
            
            return EstimationResult(
                question=question,
                value=fallback['value'],
                unit=fallback.get('unit', ''),
                confidence=0.50,  # ë‚®ì€ ì‹ ë¢°ë„
                phase=4,
                context=context,
                reasoning=f"Fallback ì¶”ì •: {fallback['reasoning']}",
                reasoning_detail={
                    'method': 'fallback',
                    'fallback_type': fallback.get('type', 'conservative'),
                    'why_this_method': 'ì¬ê·€ ì¶”ì • ì‹¤íŒ¨, ë³´ìˆ˜ì  ì¶”ì •ê°’ ì‚¬ìš©'
                }
            )
        
        # ì™„ì „ ì‹¤íŒ¨
        return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 4: ëª¨í˜• ì‹¤í–‰ (Backtracking)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _phase5_boundary_validation(
        self,
        result: EstimationResult,
        question: str,
        model: Any,
        depth: int
    ):
        """
        Phase 5: Boundary ê²€ì¦ (v7.6.2)
        
        LLM ê¸°ë°˜ ë¹„ì •í˜• ì‚¬ê³ ë¡œ ì¶”ì •ê°’ íƒ€ë‹¹ì„± ê²€ì¦
        
        Args:
            result: ì¶”ì • ê²°ê³¼
            question: ì›ë˜ ì§ˆë¬¸
            model: ì‚¬ìš©ëœ ëª¨í˜•
            depth: ê¹Šì´
        
        Returns:
            BoundaryCheck
        """
        logger.info(f"{'  ' * depth}  [Phase 5] Boundary ê²€ì¦")
        
        try:
            from .boundary_validator import get_boundary_validator
            
            validator = get_boundary_validator(llm_mode=self.llm_mode)
            
            boundary_check = validator.validate(
                question=question,
                estimated_value=result.value,
                unit=result.unit,
                context=result.context,
                formula=model.formula if hasattr(model, 'formula') else ""
            )
            
            return boundary_check
        
        except Exception as e:
            logger.warning(f"{'  ' * depth}  âš ï¸  Boundary ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # Fallback: í†µê³¼ë¡œ ê°„ì£¼
            from .boundary_validator import BoundaryCheck
            return BoundaryCheck(is_valid=True, reasoning="Boundary ê²€ì¦ ìŠ¤í‚µ")
    
    def _step4_execute(
        self,
        ranked_model: RankedModel,
        depth: int,
        context: Context
    ) -> Optional[EstimationResult]:
        """
        Step 4: ëª¨í˜• ì‹¤í–‰ (Backtracking)
        
        ì¬ê·€ë¡œ ì±„ìš´ ë³€ìˆ˜ë“¤ì„ backtrackingìœ¼ë¡œ ì¬ì¡°ë¦½
        
        Args:
            ranked_model: ì„ íƒëœ ëª¨í˜•
            depth: ê¹Šì´
            context: ë§¥ë½
        
        Returns:
            EstimationResult (decomposition í¬í•¨)
        """
        logger.info(f"{'  ' * depth}  [Step 4] ëª¨í˜• ì‹¤í–‰")
        
        model = ranked_model.model
        
        # Step 1: ë³€ìˆ˜ ë°”ì¸ë”© í™•ì¸
        bindings = {}
        for name, var in model.variables.items():
            if var.available and var.value is not None:
                bindings[name] = var.value
            else:
                logger.warning(f"{'  ' * depth}    âš ï¸  ë³€ìˆ˜ '{name}' ê°’ ì—†ìŒ")
        
        if not bindings:
            logger.warning(f"{'  ' * depth}    âŒ ë°”ì¸ë”©í•  ë³€ìˆ˜ ì—†ìŒ")
            return None
        
        logger.info(f"{'  ' * depth}    ë³€ìˆ˜ ë°”ì¸ë”©: {list(bindings.keys())}")
        
        # Step 2: ê³„ì‚° ì‹¤í–‰
        # TODO: ìˆ˜ì‹ íŒŒì‹± ë° ì•ˆì „í•œ ì‹¤í–‰
        # í˜„ì¬: ê°„ë‹¨í•œ ê³±ì…ˆ ê°€ì •
        result_value = self._execute_formula_simple(model.formula, bindings)
        
        # Step 3: Confidence ì¡°í•© (Geometric Mean)
        confidences = [
            var.confidence
            for var in model.variables.values()
            if var.available and var.confidence > 0
        ]
        
        if confidences:
            combined_confidence = math.prod(confidences) ** (1 / len(confidences))
        else:
            combined_confidence = 0.5
        
        logger.info(f"{'  ' * depth}    Confidence: {combined_confidence:.2f}")
        
        # Step 4: DecompositionTrace ìƒì„±
        decomposition = DecompositionTrace(
            formula=model.formula,
            variables={
                name: var.estimation_result
                for name, var in model.variables.items()
                if var.estimation_result
            },
            calculation_logic=model.description,
            depth=depth,
            decomposition_reasoning=getattr(model, 'selection_reason', '')
        )
        
        # Step 5: Logic Steps ìƒì„±
        logic_steps = [
            f"ëª¨í˜• ì„ íƒ: {model.formula}",
            f"ë³€ìˆ˜ ë¶„í•´: {model.total_variables}ê°œ",
            f"ë³€ìˆ˜ í™•ë³´: {getattr(model, 'available_count', len(bindings))}ê°œ",
            f"ì¬ê·€ ê¹Šì´: depth {depth}",
            f"ê³„ì‚°: {model.formula}",
            f"ì‹ ë¢°ë„: {combined_confidence:.2f}",
            f"ê²°ê³¼: {result_value}"
        ]
        
        # Step 6: EstimationResult ìƒì„±
        result = EstimationResult(
            question=context.domain if context and context.domain else "unknown",
            value=result_value,
            confidence=combined_confidence,
            phase=4,
            context=context,
            reasoning=f"Fermi ë¶„í•´: {model.description}",
            reasoning_detail={
                'method': 'fermi_decomposition',
                'model_id': model.model_id,
                'formula': model.formula,
                'depth': depth,
                'selection_reason': getattr(model, 'selection_reason', ''),
                'why_this_method': f'Phase 1/2/3 ì‹¤íŒ¨, ì¬ê·€ ë¶„í•´ (depth {depth})',
                'variables': {
                    name: {
                        'value': var.value,
                        'source': var.source,
                        'confidence': var.confidence
                    }
                    for name, var in model.variables.items()
                    if var.available
                }
            },
            logic_steps=logic_steps,
            decomposition=decomposition,
            fermi_model=model
        )
        
        return result
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ëª¨í˜• ì ìˆ˜í™”
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _score_model(
        self,
        model: FermiModel,
        depth: int
    ) -> Dict[str, Any]:
        """
        ëª¨í˜• ì ìˆ˜í™” (4ê°œ ê¸°ì¤€)
        
        ì„¤ê³„: fermi_model_search.yaml Line 725-810
        
        ê¸°ì¤€:
        1. Unknown count (50%): ì ì„ìˆ˜ë¡ ì¢‹ìŒ
        2. Confidence (30%): ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        3. Complexity (20%): ê°„ë‹¨í• ìˆ˜ë¡ ì¢‹ìŒ
        4. Depth (10% bonus): ì–•ì„ìˆ˜ë¡ ì¢‹ìŒ
        
        Returns:
            {
                'unknown': float,
                'confidence': float,
                'complexity': float,
                'depth': float,
                'total': float,
                'status': str,
                'missing': List[str]
            }
        """
        # 1. Unknown count (50%)
        if model.total_variables > 0:
            filled = sum(1 for v in model.variables.values() if v.available)
            model.available_count = filled
            unknown_ratio = filled / model.total_variables
        else:
            unknown_ratio = 0.0
        
        unknown_score = unknown_ratio * 0.5
        
        # 2. Confidence (30%)
        confidences = [
            v.confidence for v in model.variables.values()
            if v.available and v.confidence > 0
        ]
        
        if confidences:
            avg_confidence = math.prod(confidences) ** (1 / len(confidences))  # Geometric mean
        else:
            avg_confidence = 0.0
        
        confidence_score = avg_confidence * 0.3
        
        # 3. Complexity (20%)
        var_count = model.total_variables
        
        complexity_map = {
            1: 1.0, 2: 1.0,
            3: 0.9, 4: 0.7,
            5: 0.5, 6: 0.3,
            7: 0.2, 8: 0.15,
            9: 0.10, 10: 0.05
        }
        
        complexity = complexity_map.get(var_count, 0.0)
        complexity_score = complexity * 0.2
        
        # 4. Depth (10% bonus)
        depth_penalties = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}
        depth_penalty = depth_penalties.get(depth, 0.2)
        depth_score = depth_penalty * 0.1
        
        # ì´ì 
        total = unknown_score + confidence_score + complexity_score + depth_score
        
        # ìƒíƒœ íŒë‹¨
        missing = [
            name for name, var in model.variables.items()
            if not var.available
        ]
        
        if not missing:
            status = "feasible"
        elif len(missing) <= 2:
            status = "partial"
        else:
            status = "infeasible"
        
        return {
            'unknown': unknown_score,
            'confidence': confidence_score,
            'complexity': complexity_score,
            'depth': depth_score,
            'total': total,
            'status': status,
            'missing': missing
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Fallback ê°’ ì œê³µ (v7.6.2)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _get_fallback_value(
        self,
        var_name: str,
        context: Context
    ) -> Optional[Dict]:
        """
        Fallback ê°’ ì œê³µ (v7.6.2)
        
        ì¬ê·€ ì¶”ì •ì´ ì™„ì „íˆ ì‹¤íŒ¨í–ˆì„ ë•Œ ë³´ìˆ˜ì  ì¶”ì •ê°’ ì œê³µ
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            {
                'value': float,
                'unit': str,
                'reasoning': str,
                'type': 'conservative' | 'industry_avg'
            } or None
        """
        logger.info(f"      [Fallback] {var_name} ë³´ìˆ˜ì  ì¶”ì •")
        
        # Domain ê¸°ë°˜ Fallback
        domain = context.domain if context else "General"
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ë””ì§€í„¸ ì„œë¹„ìŠ¤ ê´€ë ¨
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'adoption' in var_name.lower() or 'penetration' in var_name.lower():
            # ë””ì§€í„¸ ì„œë¹„ìŠ¤ ì‚¬ìš©ë¥ 
            if 'digital' in domain.lower() or 'saas' in domain.lower():
                return {
                    'value': 0.20,  # ë³´ìˆ˜ì : 20%
                    'unit': 'ë¹„ìœ¨',
                    'reasoning': 'ë””ì§€í„¸ ì„œë¹„ìŠ¤ ë³´ìˆ˜ì  ì‚¬ìš©ë¥  (ì—…ê³„ í•˜í•œ)',
                    'type': 'conservative'
                }
        
        if 'arpu' in var_name.lower():
            # ARPU (ì›”í‰ê·  ë§¤ì¶œ)
            if 'b2b' in domain.lower():
                return {
                    'value': 50_000,  # B2B ë³´ìˆ˜ì 
                    'unit': 'ì›/ì›”',
                    'reasoning': 'B2B SaaS ë³´ìˆ˜ì  ARPU (ì—…ê³„ í•˜í•œ)',
                    'type': 'conservative'
                }
            else:
                return {
                    'value': 5_000,  # B2C ë³´ìˆ˜ì 
                    'unit': 'ì›/ì›”',
                    'reasoning': 'B2C ì„œë¹„ìŠ¤ ë³´ìˆ˜ì  ARPU',
                    'type': 'conservative'
                }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ë°€ë„ ê´€ë ¨
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if 'people_per' in var_name.lower() or 'density' in var_name.lower():
            # ìŒì‹ì  ë°€ë„
            if 'food' in domain.lower() or 'ìŒì‹ì ' in var_name:
                return {
                    'value': 100,  # ë³´ìˆ˜ì : 100ëª…/ì 
                    'unit': 'ëª…/ì ',
                    'reasoning': 'ìŒì‹ì  ë°€ë„ ë³´ìˆ˜ì  ì¶”ì • (ë„ì‹œ í‰ê· )',
                    'type': 'conservative'
                }
            
            # ì¹´í˜ ë°€ë„
            if 'cafe' in domain.lower() or 'ì¹´í˜' in var_name:
                return {
                    'value': 500,  # ë³´ìˆ˜ì 
                    'unit': 'ëª…/ì ',
                    'reasoning': 'ì¹´í˜ ë°€ë„ ë³´ìˆ˜ì  ì¶”ì •',
                    'type': 'conservative'
                }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì°¾ì§€ ëª»í•¨
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        logger.info(f"      [Fallback] {var_name} ê°’ ì—†ìŒ")
        return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì•ˆì „ ì¥ì¹˜
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _detect_circular(self, question: str) -> bool:
        """
        ìˆœí™˜ ì˜ì¡´ì„± ê°ì§€
        
        Call stackì— ë™ì¼ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ìˆœí™˜
        
        ì˜ˆ:
            depth 0: "ì‹œì¥ ê·œëª¨ëŠ”?"
            depth 1: "ì ìœ ìœ¨ì€?"
            depth 2: "ì‹œì¥ ê·œëª¨ëŠ”?"  # â† ìˆœí™˜!
        
        Args:
            question: ì§ˆë¬¸
        
        Returns:
            True: ìˆœí™˜ ê°ì§€
            False: ì •ìƒ
        """
        normalized = question.lower().strip()
        
        for past_question in self.call_stack:
            if past_question.lower().strip() == normalized:
                logger.warning(f"    ìˆœí™˜ ê°ì§€: '{question}'")
                logger.warning(f"    Call stack: {self.call_stack}")
                return True
        
        return False
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ìœ í‹¸ë¦¬í‹°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _build_contextualized_question(
        self,
        var_name: str,
        context: Context
    ) -> str:
        """
        Contextë¥¼ í¬í•¨í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ìƒì„± (v7.5.0)
        
        ë³€ìˆ˜ ì´ë¦„ë§Œìœ¼ë¡œëŠ” ì• ë§¤í•˜ë¯€ë¡œ, ë§¥ë½ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
        
        Args:
            var_name: ë³€ìˆ˜ ì´ë¦„ (ì˜ˆ: "arpu", "churn_rate")
            context: ë§¥ë½
        
        Returns:
            êµ¬ì²´í™”ëœ ì§ˆë¬¸ ë¬¸ìì—´
        
        Example:
            >>> _build_contextualized_question("arpu", Context(domain="B2B_SaaS", region="í•œêµ­"))
            >>> # "B2B SaaS í•œêµ­ ì‹œì¥ì˜ ARPUëŠ”?"
        """
        # ë³€ìˆ˜ ì´ë¦„ ì •ë¦¬ (snake_case â†’ ë„ì–´ì“°ê¸°)
        readable_var = var_name.replace('_', ' ').upper()
        
        # Context ìš”ì†Œ ìˆ˜ì§‘
        context_parts = []
        
        if context.domain and context.domain != "General":
            context_parts.append(context.domain.replace('_', ' '))
        
        if context.region:
            context_parts.append(context.region)
        
        if context.time_period:
            context_parts.append(context.time_period)
        
        # ì§ˆë¬¸ ì¡°ë¦½
        if context_parts:
            context_str = " ".join(context_parts)
            question = f"{context_str} ì‹œì¥ì˜ {readable_var}ëŠ”?"
        else:
            question = f"{readable_var}ëŠ”?"
        
        return question
    
    def _execute_formula_simple(
        self,
        formula: str,
        bindings: Dict[str, float]
    ) -> float:
        """
        ìˆ˜ì‹ ì‹¤í–‰ (ì•ˆì „í•œ ë²„ì „)
        
        ì§€ì› ì—°ì‚°: +, -, *, /, ê´„í˜¸
        ê¸ˆì§€: eval() (ë³´ì•ˆ ìœ„í—˜)
        
        Args:
            formula: ìˆ˜ì‹ (ì˜ˆ: "ltv = arpu / churn_rate")
            bindings: ë³€ìˆ˜ ê°’ (ì˜ˆ: {"arpu": 80000, "churn_rate": 0.05})
        
        Returns:
            ê³„ì‚° ê²°ê³¼
        """
        try:
            # ìˆ˜ì‹ì—ì„œ ê²°ê³¼ ë³€ìˆ˜ ì œê±° (ì˜ˆ: "ltv = ..." â†’ "...")
            if '=' in formula:
                parts = formula.split('=', 1)
                if len(parts) == 2:
                    formula = parts[1].strip()
            
            # Ã— â†’ * ë³€í™˜ (ìˆ˜í•™ ê¸°í˜¸ ì •ê·œí™”)
            expr = formula.replace('Ã—', '*').replace('Ã·', '/')
            
            # ë³€ìˆ˜ ì¹˜í™˜
            for var_name, var_value in bindings.items():
                # ë³€ìˆ˜ ì´ë¦„ì„ ê°’ìœ¼ë¡œ ì¹˜í™˜
                expr = expr.replace(var_name, str(var_value))
            
            # ì•ˆì „í•œ ê³„ì‚° (í—ˆìš© ë¬¸ìë§Œ)
            allowed_chars = set('0123456789.+-*/() ')
            if not all(c in allowed_chars for c in expr):
                logger.warning(f"    âš ï¸  ìˆ˜ì‹ì— í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì: {formula}")
                # Fallback: ê³±ì…ˆ
                return math.prod(bindings.values()) if bindings else 0.0
            
            # ê³„ì‚° ì‹¤í–‰ (ì œí•œì  eval - ìˆ«ìì™€ ì—°ì‚°ìë§Œ)
            result = eval(expr, {"__builtins__": {}}, {})
            
            return float(result)
        
        except Exception as e:
            logger.warning(f"    âš ï¸  ìˆ˜ì‹ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            logger.warning(f"    Fallback: ê³±ì…ˆ ì‚¬ìš©")
            
            # Fallback: ê³±ì…ˆ
            if bindings:
                return math.prod(bindings.values())
            return 0.0
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 2b: ë°˜ë³µ ê°œì„  (ë³€ìˆ˜ ì¬ê²€ìƒ‰)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _phase2b_refine_with_data_search(
        self,
        models: List[FermiModel],
        question: str,
        context: Context,
        depth: int
    ) -> List[FermiModel]:
        """
        Step 2b: LLM ì œì•ˆ ë³€ìˆ˜ì— ëŒ€í•œ ë°ì´í„° ì¬ê²€ìƒ‰
        
        ë°˜ë³µ ìµœëŒ€ 2íšŒ:
        - 1íšŒì°¨: Unknown ë³€ìˆ˜ ê²€ìƒ‰
        - 2íšŒì°¨: ì—¬ì „íˆ Unknownì¸ ë³€ìˆ˜ ê²€ìƒ‰
        - ìƒˆ ë°œê²¬ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        
        Args:
            models: LLM ìƒì„± ëª¨í˜•ë“¤
            question: ì§ˆë¬¸
            context: ë§¥ë½
            depth: ê¹Šì´
        
        Returns:
            ê°œì„ ëœ ëª¨í˜•ë“¤
        """
        max_iterations = 2
        iteration = 0
        
        while iteration < max_iterations:
            # 1. Unknown ë³€ìˆ˜ ì¶”ì¶œ
            unknown_vars = set()
            for model in models:
                for var_name, var in model.variables.items():
                    if not var.available and var.need_estimate:
                        unknown_vars.add(var_name)
            
            if not unknown_vars:
                break  # ëª¨ë‘ available
            
            logger.info(f"{'  ' * depth}  [Refine {iteration+1}] Unknown ë³€ìˆ˜: {len(unknown_vars)}ê°œ")
            
            # 2. Unknown ë³€ìˆ˜ ì¬ê²€ìƒ‰
            newly_found = {}
            for var_name in unknown_vars:
                var_data = self._search_for_variable(var_name, question, context)
                if var_data:
                    newly_found[var_name] = var_data
                    logger.info(f"{'  ' * depth}    âœ… {var_name} = {var_data.value} (conf: {var_data.confidence:.2f})")
            
            if not newly_found:
                logger.info(f"{'  ' * depth}  [Refine {iteration+1}] ìƒˆ ë°œê²¬ ì—†ìŒ â†’ ì¢…ë£Œ")
                break  # ë” ì´ìƒ ë°œê²¬ ì—†ìŒ
            
            # 3. ëª¨í˜• ì—…ë°ì´íŠ¸
            for model in models:
                for var_name, var_data in newly_found.items():
                    if var_name in model.variables:
                        var = model.variables[var_name]
                        var.available = True
                        var.value = var_data.value
                        var.confidence = var_data.confidence
                        var.source = var_data.source
                        var.need_estimate = False
                        model.unknown_count = max(0, model.unknown_count - 1)
            
            iteration += 1
            logger.info(f"{'  ' * depth}  [Refine {iteration}] {len(newly_found)}ê°œ ë³€ìˆ˜ ë°œê²¬")
        
        return models
    
    def _search_for_variable(
        self,
        var_name: str,
        question: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ ë°ì´í„° ê²€ìƒ‰
        
        ìˆœì„œ:
        1. RAG ê²€ìƒ‰
        2. Phase 3 Source
        3. Context ìƒìˆ˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            question: ì›ë˜ ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë°ì´í„° ë˜ëŠ” None
        """
        # 1. RAG ê²€ìƒ‰
        result = self._search_rag_for_variable(var_name, context)
        if result:
            return result
        
        # 2. Phase 3 Source
        result = self._query_phase3_for_variable(var_name, context)
        if result:
            return result
        
        # 3. Context ìƒìˆ˜
        result = self._get_context_constant(var_name, context)
        if result:
            return result
        
        return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # í—¬í¼ ë©”ì„œë“œ: ë°ì´í„° ê²€ìƒ‰
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _search_rag_benchmarks(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        RAGì—ì„œ ê´€ë ¨ ë²¤ì¹˜ë§ˆí¬/ìƒìˆ˜ ê²€ìƒ‰
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ë“¤
        """
        results = {}
        
        if not HAS_CHROMA:
            return results
        
        try:
            # Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings(
                model=settings.embedding_model,
                openai_api_key=settings.openai_api_key
            )
            
            # ê²€ìƒ‰í•  collectionë“¤
            collection_names = [
                "market_benchmarks",
                "system_knowledge"
            ]
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = f"{context.domain} {question}" if context.domain else question
            
            for collection_name in collection_names:
                try:
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory=str(settings.chroma_persist_dir)
                    )
                    
                    # RAG ê²€ìƒ‰ (top 3)
                    docs = vectorstore.similarity_search(search_query, k=3)
                    
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ì¶”ì¶œ
                    for doc in docs:
                        metadata = doc.metadata
                        
                        # ë³€ìˆ˜ëª…ê³¼ ê°’ì´ ìˆëŠ” ê²½ìš°
                        if 'variable_name' in metadata and 'value' in metadata:
                            var_name = metadata['variable_name']
                            var_value = metadata['value']
                            
                            if var_name not in results:
                                results[var_name] = FermiVariable(
                                    name=var_name,
                                    value=var_value,
                                    available=True,
                                    source=f"rag_{collection_name}",
                                    confidence=metadata.get('confidence', 0.8),
                                    description=doc.page_content[:100]
                                )
                
                except Exception as e:
                    # Collection ì—†ìœ¼ë©´ ë¬´ì‹œ
                    continue
        
        except Exception as e:
            logger.warning(f"    âš ï¸  RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _query_phase3_sources(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        Phase 3 Sourceì—ì„œ ë°ì´í„° ì¡°íšŒ
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ë“¤
        """
        results = {}
        
        try:
            # Phase 3ìœ¼ë¡œ ì§ì ‘ ì¶”ì • ì‹œë„ (ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)
            phase3_result = self.phase3.estimate(question, context)
            
            if phase3_result and phase3_result.confidence >= 0.80:
                # ì§ˆë¬¸ì—ì„œ ë³€ìˆ˜ëª… ì¶”ì¶œ ì‹œë„
                var_name = self._extract_var_name_from_question(question)
                
                if var_name:
                    results[var_name] = FermiVariable(
                        name=var_name,
                        value=phase3_result.value,
                        available=True,
                        source=f"phase3_{phase3_result.sources[0] if phase3_result.sources else 'unknown'}",
                        confidence=phase3_result.confidence,
                        description=phase3_result.reasoning_detail.get('method', '')
                    )
        
        except Exception as e:
            logger.warning(f"    âš ï¸  Phase 3 ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _extract_var_name_from_question(self, question: str) -> Optional[str]:
        """
        ì§ˆë¬¸ì—ì„œ ë³€ìˆ˜ëª… ì¶”ì¶œ
        
        ì˜ˆ: "í•œêµ­ ì¸êµ¬ëŠ”?" â†’ "korea_population"
        """
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        keywords_map = {
            'ì¸êµ¬': 'population',
            'ì†ë„': 'speed',
            'churn': 'churn_rate',
            'arpu': 'arpu',
            'ltv': 'ltv',
            'ê±°ë¦¬': 'distance'
        }
        
        for keyword, var_name in keywords_map.items():
            if keyword in question.lower():
                return var_name
        
        # ê¸°ë³¸ê°’: ì§ˆë¬¸ì˜ ì²« ë‹¨ì–´
        words = question.replace('?', '').split()
        if words:
            return words[0].lower()
        
        return None
    
    def _extract_context_constants(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        Contextì—ì„œ ìëª…í•œ ìƒìˆ˜ ì¶”ì¶œ
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ìƒìˆ˜ë“¤
        """
        results = {}
        
        # Domainë³„ ìƒìˆ˜
        if context.domain == "transportation":
            # ë¬¼ë¦¬ ìƒìˆ˜
            if "ì¤‘ë ¥" in question or "gravity" in question.lower():
                results['gravity'] = FermiVariable(
                    name='gravity',
                    value=9.8,
                    available=True,
                    source="physical_constant",
                    confidence=1.0
                )
        
        # Regionë³„ ìƒìˆ˜
        if context.region == "South_Korea":
            if "ì¸êµ¬" in question or "population" in question.lower():
                results['korea_population'] = FermiVariable(
                    name='korea_population',
                    value=51_000_000,
                    available=True,
                    source="statistical_constant",
                    confidence=0.95,
                    description="í•œêµ­ ì¸êµ¬ (2024)"
                )
        
        return results
    
    def _search_rag_for_variable(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ RAG ê²€ìƒ‰
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë˜ëŠ” None
        """
        if not HAS_CHROMA:
            return None
        
        try:
            # ë³€ìˆ˜ëª…ì„ ìì—°ì–´ë¡œ ë³€í™˜
            query_text = self._var_name_to_natural_language(var_name, context)
            
            # RAG ê²€ìƒ‰
            embeddings = OpenAIEmbeddings(
                model=settings.embedding_model,
                openai_api_key=settings.openai_api_key
            )
            
            for collection_name in ["market_benchmarks", "system_knowledge"]:
                try:
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory=str(settings.chroma_persist_dir)
                    )
                    
                    docs = vectorstore.similarity_search(query_text, k=1)
                    
                    if docs:
                        doc = docs[0]
                        metadata = doc.metadata
                        
                        if 'value' in metadata:
                            return FermiVariable(
                                name=var_name,
                                value=metadata['value'],
                                available=True,
                                source=f"rag_{collection_name}",
                                confidence=metadata.get('confidence', 0.75),
                                description=doc.page_content[:100]
                            )
                
                except Exception:
                    continue
        
        except Exception as e:
            logger.debug(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨ ({var_name}): {e}")
        
        return None
    
    def _var_name_to_natural_language(self, var_name: str, context: Context) -> str:
        """
        ë³€ìˆ˜ëª…ì„ ìì—°ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜
        
        ì˜ˆ: "speed" + "transportation" â†’ "êµí†µìˆ˜ë‹¨ í‰ê·  ì†ë„"
        """
        # ë³€ìˆ˜ëª… ì •ê·œí™”
        var_lower = var_name.lower().replace('_', ' ')
        
        # Domain ê¸°ë°˜ ë³€í™˜
        if context.domain:
            return f"{context.domain} {var_lower}"
        
        return var_lower
    
    def _query_phase3_for_variable(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ Phase 3 Source ì¡°íšŒ
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë˜ëŠ” None
        """
        try:
            # ë³€ìˆ˜ëª…ì„ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
            question = self._build_contextualized_question(var_name, context)
            
            # Phase 3 ì¡°íšŒ
            phase3_result = self.phase3.estimate(question, context)
            
            if phase3_result and phase3_result.confidence >= 0.75:
                return FermiVariable(
                    name=var_name,
                    value=phase3_result.value,
                    available=True,
                    source=f"phase3_{phase3_result.sources[0] if phase3_result.sources else 'source'}",
                    confidence=phase3_result.confidence,
                    description=phase3_result.reasoning_detail.get('method', '')
                )
        
        except Exception as e:
            logger.debug(f"Phase 3 ì¡°íšŒ ì‹¤íŒ¨ ({var_name}): {e}")
        
        return None
    
    def _get_context_constant(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ Context ìƒìˆ˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ìƒìˆ˜ ë˜ëŠ” None
        """
        # Domain ê¸°ë°˜ ìƒìˆ˜ ë§¤ì¹­
        var_lower = var_name.lower()
        
        # Transportation domain
        if context.domain == "transportation":
            # ì†ë„ ê´€ë ¨ ë³€ìˆ˜
            if "speed" in var_lower or "ì†ë„" in var_lower or "velocity" in var_lower:
                return FermiVariable(
                    name=var_name,
                    value=130,
                    available=True,
                    source="context_benchmark",
                    confidence=0.85,
                    description="KTX í‰ê·  ì†ë„ (km/h, ì •ì°¨ í¬í•¨)"
                )
        
        # South Korea region
        if context.region == "South_Korea":
            # ì¸êµ¬ ê´€ë ¨
            if "population" in var_lower or "ì¸êµ¬" in var_lower:
                return FermiVariable(
                    name=var_name,
                    value=51_000_000,
                    available=True,
                    source="context_constant",
                    confidence=0.95,
                    description="í•œêµ­ ì¸êµ¬ (2024)"
                )
            
            # ê±°ë¦¬ ê´€ë ¨ (ì£¼ìš” ë„ì‹œ)
            if "seoul" in var_lower and "busan" in var_lower:
                if "distance" in var_lower or "ê±°ë¦¬" in var_lower:
                    return FermiVariable(
                        name=var_name,
                        value=325,
                        available=True,
                        source="context_constant",
                        confidence=1.0,
                        description="ì„œìš¸-ë¶€ì‚° ê±°ë¦¬ (km)"
                    )
        
        return None
    
    def _build_contextualized_question(
        self,
        var_name: str,
        context: Context
    ) -> str:
        """
        ë³€ìˆ˜ëª…ì„ ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë§¥ë½ í¬í•¨ ì§ˆë¬¸
        """
        # Domain ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        if context.domain:
            return f"{context.domain}ì—ì„œ {var_name}ëŠ” ì–¼ë§ˆì¸ê°€?"
        
        # ê¸°ë³¸ ì§ˆë¬¸
        return f"{var_name}ëŠ” ì–¼ë§ˆì¸ê°€?"
    
    def _verify_calculation_connectivity(
        self,
        decomposition: List[Dict],
        final_value: float
    ) -> Dict:
        """
        ë¶„í•´ ê°’ë“¤ì´ ìµœì¢…ê°’ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ ìë™ ê²€ì¦ (v7.7.1)
        
        Args:
            decomposition: ë¶„í•´ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸
            final_value: ìµœì¢… ì¶”ì •ê°’
        
        Returns:
            {
                'verified': bool,
                'method': str,  # 'ë§ˆì§€ë§‰ ë‹¨ê³„', 'í•©ê³„', 'ê³±ì…ˆ' ë“±
                'calculated_value': float,
                'error': float,  # ì˜¤ì°¨ìœ¨
                'score': int  # 0-25ì 
            }
        """
        if not isinstance(decomposition, list) or len(decomposition) < 2:
            return {
                'verified': False,
                'score': 0,
                'reason': 'ë‹¨ê³„ ë¶€ì¡±',
                'method': '',
                'calculated_value': 0,
                'error': 1.0
            }
        
        # ê° ë‹¨ê³„ì—ì„œ value ì¶”ì¶œ
        values = []
        for step in decomposition:
            if isinstance(step, dict):
                val = step.get('value', 0)
                if isinstance(val, (int, float)) and val > 0:
                    values.append(val)
        
        if len(values) < 2:
            return {
                'verified': False,
                'score': 0,
                'reason': 'ìœ íš¨í•œ ê°’ ë¶€ì¡±',
                'method': '',
                'calculated_value': 0,
                'error': 1.0
            }
        
        # ë‹¤ì–‘í•œ ì¡°í•© ì‹œë„
        attempts = []
        
        # 1. ë§ˆì§€ë§‰ ê°’
        if values[-1] > 0:
            error = abs(values[-1] - final_value) / max(final_value, 1)
            attempts.append({
                'method': 'ë§ˆì§€ë§‰ ë‹¨ê³„',
                'calculated': values[-1],
                'error': error
            })
        
        # 2. ì „ì²´ í•©ê³„
        total = sum(values)
        if total > 0:
            error = abs(total - final_value) / max(final_value, 1)
            attempts.append({
                'method': 'ëª¨ë“  ë‹¨ê³„ í•©',
                'calculated': total,
                'error': error
            })
        
        # 3. ë§ˆì§€ë§‰ 2ê°œ í•©
        if len(values) >= 2:
            last_two = sum(values[-2:])
            if last_two > 0:
                error = abs(last_two - final_value) / max(final_value, 1)
                attempts.append({
                    'method': 'ë§ˆì§€ë§‰ 2ë‹¨ê³„ í•©',
                    'calculated': last_two,
                    'error': error
                })
        
        # 4. ë§ˆì§€ë§‰ 3ê°œ í•©
        if len(values) >= 3:
            last_three = sum(values[-3:])
            if last_three > 0:
                error = abs(last_three - final_value) / max(final_value, 1)
                attempts.append({
                    'method': 'ë§ˆì§€ë§‰ 3ë‹¨ê³„ í•©',
                    'calculated': last_three,
                    'error': error
                })
        
        # ê°€ì¥ ì˜¤ì°¨ê°€ ì‘ì€ ê²ƒ ì„ íƒ
        if attempts:
            best = min(attempts, key=lambda x: x['error'])
            
            # ì ìˆ˜ ê³„ì‚°
            if best['error'] < 0.01:  # 1% ì´ë‚´
                score = 25
            elif best['error'] < 0.05:  # 5% ì´ë‚´
                score = 20
            elif best['error'] < 0.1:  # 10% ì´ë‚´
                score = 15
            elif best['error'] < 0.3:  # 30% ì´ë‚´
                score = 10
            else:
                score = 5
            
            return {
                'verified': best['error'] < 0.1,  # 10% ì´ë‚´ë©´ í†µê³¼
                'method': best['method'],
                'calculated_value': best['calculated'],
                'error': best['error'],
                'score': score
            }
        
        return {
            'verified': False,
            'score': 0,
            'reason': 'ê³„ì‚° ë¶ˆê°€',
            'method': '',
            'calculated_value': 0,
            'error': 1.0
        }


