"""
Validator RAG Agent Module

Validator (Rachel) ì—ì´ì „íŠ¸ì˜ RAG ê¸°ë°˜ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

í•µì‹¬ ê°œë…:
-----------
1. **Data Source Discovery**: ë°ì´í„° ì†ŒìŠ¤ ìë™ ê²€ìƒ‰ ë° ì¶”ì²œ
2. **Definition Validation**: ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ì°¸ì¡°
3. **Gap Analysis**: ì •ì˜ ë¶ˆì¼ì¹˜ ë¶„ì„ ê°€ì´ë“œ
4. **Creative Sourcing**: ì°½ì˜ì  ë°ì´í„° ì†Œì‹± ë°©ë²•
5. **Definite Data Search**: í™•ì • ë°ì´í„° ìš°ì„  ê²€ìƒ‰ (v7.6.0+)

Validatorì˜ í•µì‹¬ ì—­í• :
----------------------
1. í™•ì • ë°ì´í„° ê²€ìƒ‰ (Estimator Phase 2, v7.6.0+) â­ ìµœìš°ì„ !
2. ë°ì´í„° ì •ì˜ ê²€ì¦
3. ë‹¨ìœ„ ìë™ ë³€í™˜ (v7.6.1+)
4. Relevance ê²€ì¦ (v7.6.1+)
5. ì‹ ë¢°ë„ í‰ê°€

RAG Collections:
----------------
- data_sources_registry: ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ (24ê°œ, v7.6.0+)
- definition_validation_cases: ì •ì˜ ê²€ì¦ ì‚¬ë¡€ (100ê°œ)

v7.6.0+ ì£¼ìš” ë³€ê²½:
------------------
- search_definite_data(): Estimator ì¶”ì • ì „ í™•ì • ë°ì´í„° ê²€ìƒ‰
- ë‹¨ìœ„ ìë™ ë³€í™˜ (ê°‘/ë…„ â†’ ê°‘/ì¼ ë“±)
- Relevance ê²€ì¦ (GDP ì˜¤ë¥˜ ë°©ì§€)
- 94.7% ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±
"""

from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.core.config import settings
from umis_rag.utils.logger import logger

# v7.3.2: Estimator í†µí•© (ì¶”ì •ì¹˜ ê²€ì¦ìš©)
from umis_rag.agents.estimator import get_estimator_rag


class ValidatorRAG:
    """
    Validator (Rachel) RAG Agent
    
    ì—­í• :
    -----
    - ë°ì´í„° ì†ŒìŠ¤ ë°œê²¬
    - ì •ì˜ ê²€ì¦
    - ì‹ ë¢°ë„ í‰ê°€
    - Gap ë¶„ì„
    
    í•µì‹¬ ë©”ì„œë“œ:
    -----------
    - search_data_source(): ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
    - search_definition_case(): ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰
    - search_gap_analysis(): Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰
    
    í˜‘ì—…:
    -----
    - Quantifier: ëª¨ë“  ê³„ì‚°ì˜ ë°ì´í„° ì •ì˜ ê²€ì¦ (í•„ìˆ˜!)
    - Observer, Explorer: ë°ì´í„° ì¶œì²˜ í™•ì¸
    """
    
    def __init__(self):
        """Validator RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        logger.info("Validator RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™”")
        
        # v7.3.2: Estimator ì—°ê²° (êµì°¨ ê²€ì¦ìš©)
        self.estimator = None  # Lazy ì´ˆê¸°í™”
        
        # Embeddings
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            openai_api_key=settings.openai_api_key
        )
        
        # Vector Stores
        try:
            # 1. ë°ì´í„° ì†ŒìŠ¤
            self.source_store = Chroma(
                collection_name="data_sources_registry",
                embedding_function=self.embeddings,
                persist_directory=str(settings.chroma_persist_dir)
            )
            logger.info(f"  âœ… ë°ì´í„° ì†ŒìŠ¤: {self.source_store._collection.count()}ê°œ")
        except Exception as e:
            logger.warning(f"  âš ï¸  ë°ì´í„° ì†ŒìŠ¤ Collection ì—†ìŒ (êµ¬ì¶• í•„ìš”): {e}")
            self.source_store = None
        
        try:
            # 2. ì •ì˜ ê²€ì¦ ì‚¬ë¡€
            self.definition_store = Chroma(
                collection_name="definition_validation_cases",
                embedding_function=self.embeddings,
                persist_directory=str(settings.chroma_persist_dir)
            )
            logger.info(f"  âœ… ì •ì˜ ì‚¬ë¡€: {self.definition_store._collection.count()}ê°œ")
        except Exception as e:
            logger.warning(f"  âš ï¸  ì •ì˜ ê²€ì¦ Collection ì—†ìŒ (êµ¬ì¶• í•„ìš”): {e}")
            self.definition_store = None
    
    def search_data_source(
        self,
        data_type: str,
        top_k: int = 5
    ) -> List[tuple[Document, float]]:
        """
        ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–´ë””ì„œ êµ¬í• ì§€ ëª¨ë¥¼ ë•Œ
        
        ì˜ˆì‹œ:
        -----
        Input: "í•œêµ­ SaaS ì‹œì¥ ê·œëª¨"
        Output: [Gartner (85% ì‹ ë¢°ë„), IDC Korea, ...]
        
        Parameters:
        -----------
        data_type: ì°¾ëŠ” ë°ì´í„° ìœ í˜•
        top_k: ë°˜í™˜í•  ì†ŒìŠ¤ ìˆ˜
        
        Returns:
        --------
        List of (Document, similarity_score)
        """
        if not self.source_store:
            logger.warning("  âš ï¸  ë°ì´í„° ì†ŒìŠ¤ RAG ë¯¸êµ¬ì¶•")
            return []
        
        logger.info(f"[Validator] ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰")
        logger.info(f"  ë°ì´í„° ìœ í˜•: {data_type}")
        
        results = self.source_store.similarity_search_with_score(
            data_type,
            k=top_k
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì†ŒìŠ¤ ë°œê²¬")
        for doc, score in results:
            source_name = doc.metadata.get('source_name', 'Unknown')
            reliability = doc.metadata.get('reliability', 'N/A')
            logger.info(f"    - {source_name} (ì‹ ë¢°ë„: {reliability}, ìœ ì‚¬ë„: {score:.2f})")
        
        return results
    
    def search_definition_case(
        self,
        term: str,
        top_k: int = 3
    ) -> List[tuple[Document, float]]:
        """
        ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        ë°ì´í„° ì •ì˜ê°€ ì• ë§¤í•˜ê±°ë‚˜, ì‚°ì—…ë³„ ì°¨ì´ê°€ ìˆì„ ë•Œ
        
        ì˜ˆì‹œ:
        -----
        Input: "MAU (ì›”ê°„ í™œì„± ì‚¬ìš©ì)"
        Output: [Google ì •ì˜ vs Facebook ì •ì˜, Gap 20-30%, ...]
        
        Parameters:
        -----------
        term: ê²€ì¦í•  ìš©ì–´
        top_k: ë°˜í™˜í•  ì‚¬ë¡€ ìˆ˜
        
        Returns:
        --------
        List of (Document, similarity_score)
        """
        if not self.definition_store:
            logger.warning("  âš ï¸  ì •ì˜ ê²€ì¦ RAG ë¯¸êµ¬ì¶•")
            return []
        
        logger.info(f"[Validator] ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰")
        logger.info(f"  ìš©ì–´: {term}")
        
        results = self.definition_store.similarity_search_with_score(
            term,
            k=top_k
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì‚¬ë¡€ ë°œê²¬")
        for doc, score in results:
            case_term = doc.metadata.get('term', 'Unknown')
            gap_level = doc.metadata.get('gap_level', 'N/A')
            logger.info(f"    - {case_term} (Gap: {gap_level}, ìœ ì‚¬ë„: {score:.2f})")
        
        return results
    
    def search_gap_analysis(
        self,
        data_point: str,
        original_def: str,
        needed_def: str
    ) -> List[tuple[Document, float]]:
        """
        Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        ì›ë³¸ ì •ì˜ì™€ í•„ìš”í•œ ì •ì˜ê°€ ë‹¤ë¥¼ ë•Œ, ì¡°ì • ë°©ë²• ì°¾ê¸°
        
        ì˜ˆì‹œ:
        -----
        Input: 
          - data: "ë‚šì‹œì¸êµ¬ 750ë§Œ"
          - original: "ì—° 1íšŒ ì´ìƒ, ë°”ë‹¤ë‚šì‹œë§Œ"
          - needed: "ì›” 1íšŒ ì´ìƒ, ì „ì²´ ë‚šì‹œ"
        
        Output: [ìœ ì‚¬ Gap ì‚¬ë¡€, ì¡°ì • ë°©ë²•, ...]
        """
        if not self.definition_store:
            return []
        
        logger.info(f"[Validator] Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰")
        
        # Gap ì„¤ëª… ì¡°í•©
        gap_query = f"{data_point}: ì›ë³¸({original_def}) vs í•„ìš”({needed_def})"
        
        results = self.definition_store.similarity_search_with_score(
            gap_query,
            k=3,
            filter={"type": "gap_analysis"}
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì¡°ì • ê°€ì´ë“œ ë°œê²¬")
        
        return results
    
    def search_definite_data(
        self,
        question: str,
        context: Optional[Any] = None
    ) -> Optional[Dict[str, Any]]:
        """
        í™•ì • ë°ì´í„° ê²€ìƒ‰ (ì¶”ì • ì „ í•„ìˆ˜ í™•ì¸!)
        
        ì—­í• :
        -----
        - Estimator ì¶”ì • ì „ í™•ì • ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        - ê³µì‹ í†µê³„, ì •ë¶€ ë°ì´í„°, ë²¤ì¹˜ë§ˆí¬ ê²€ìƒ‰
        - ê°’ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜ (ì¶”ì • ë¶ˆí•„ìš”)
        
        ê²€ìƒ‰ ë²”ìœ„:
        ----------
        1. data_sources_registry (ê³µì‹ í†µê³„)
        2. ë©”íƒ€ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
        3. ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ (0.85+)
        
        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "í•œêµ­ ë‹´ë°° íŒë§¤ëŸ‰ì€?")
            context: ë§¥ë½ (domain, region ë“±)
        
        Returns:
            {
                'value': 87671233,
                'unit': 'ê°‘/ì¼',
                'source': 'ê¸°íšì¬ì •ë¶€',
                'confidence': 1.0,
                'definition': 'ì£¼ë¯¼ë“±ë¡ ê¸°ì¤€',
                'last_updated': '2023'
            } ë˜ëŠ” None
        
        Example:
            >>> validator = ValidatorRAG()
            >>> result = validator.search_definite_data("í•œêµ­ ì¸êµ¬ëŠ”?")
            >>> if result:
            ...     print(f"{result['value']}ëª… (ì¶œì²˜: {result['source']})")
        """
        if not self.source_store:
            logger.warning("  âš ï¸  data_sources_registry ì—†ìŒ (êµ¬ì¶• í•„ìš”)")
            return None
        
        logger.info(f"[Validator] í™•ì • ë°ì´í„° ê²€ìƒ‰: {question}")
        
        # Context ì •ë³´ ì¶”ì¶œ
        domain_str = ""
        if context and hasattr(context, 'domain'):
            domain_str = f"{context.domain} " if context.domain != "General" else ""
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_query = f"{domain_str}{question}".strip()
        logger.info(f"  ê²€ìƒ‰: {search_query}")
        
        # data_sources_registry ê²€ìƒ‰ (top 3)
        results = self.source_store.similarity_search_with_score(
            search_query,
            k=3
        )
        
        if not results:
            logger.info("  â†’ í™•ì • ë°ì´í„° ì—†ìŒ")
            return None
        
        # ë†’ì€ ìœ ì‚¬ë„ & ê°’ì´ ìˆëŠ” ê²ƒë§Œ
        for doc, score in results:
            logger.info(f"  í›„ë³´: {doc.metadata.get('source_name', 'Unknown')} (ìœ ì‚¬ë„: {score:.2f})")
            
            # v7.6.0: threshold 0.75
            if score > 0.75:
                metadata = doc.metadata
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
                if 'value' in metadata and metadata['value'] is not None:
                    # â­ v7.6.1: Relevance ê²€ì¦ ì¶”ê°€!
                    if not self._is_relevant(question, doc, context):
                        logger.warning(f"  âš ï¸  ìœ ì‚¬ë„ ë†’ì§€ë§Œ ê´€ë ¨ì„± ë‚®ìŒ â†’ ìŠ¤í‚µ")
                        continue
                    
                    logger.info(f"  âœ… í™•ì • ë°ì´í„° ë°œê²¬! (relevance ê²€ì¦ í†µê³¼)")
                    
                    # â­ v7.6.1: ë‹¨ìœ„ ë³€í™˜ ì¶”ê°€!
                    result_data = {
                        'value': metadata['value'],
                        'unit': metadata.get('unit', ''),
                        'source': metadata.get('source_name', 'Unknown'),
                        'confidence': 1.0,
                        'definition': metadata.get('definition', ''),
                        'last_updated': metadata.get('year', ''),
                        'access_method': metadata.get('access_method', ''),
                        'reliability': metadata.get('reliability', 'high'),
                        'document': doc.page_content
                    }
                    
                    # ë‹¨ìœ„ ë³€í™˜ ì‹œë„
                    converted = self._convert_unit_if_needed(question, result_data, doc)
                    if converted:
                        result_data = converted
                    
                    return result_data
        
        logger.info("  â†’ í™•ì • ë°ì´í„° ì—†ìŒ (ìœ ì‚¬ë„ ë‚®ê±°ë‚˜ ê°’ ì—†ìŒ)")
        return None
    
    def _is_relevant(
        self,
        question: str,
        doc: Any,
        context: Optional[Any] = None
    ) -> bool:
        """
        Relevance ê²€ì¦ (v7.6.1)
        
        ìœ ì‚¬ë„ê°€ ë†’ì•„ë„ ì‹¤ì œë¡œ ê´€ë ¨ ì—†ëŠ” ë°ì´í„° í•„í„°ë§
        ì˜ˆ: "ì‹œì¥ ê·œëª¨" â†’ GDP (X)
        
        ê²€ì¦ í•­ëª©:
        1. ë¹„í˜¸í™˜ ì¡°í•© ì²´í¬ (ì‹œì¥â‰ GDP ë“±)
        2. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
        3. Scale ê²€ì¦
        """
        metadata = doc.metadata
        doc_content = doc.page_content.lower()
        question_lower = question.lower()
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. ë¹„í˜¸í™˜ ì¡°í•© ì²´í¬
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        INCOMPATIBLE_PAIRS = [
            # (ì§ˆë¬¸ í‚¤ì›Œë“œ, ë°ì´í„° ì¹´í…Œê³ ë¦¬) = ë¹„í˜¸í™˜
            (['ì‹œì¥', 'ê·œëª¨'], ['gdp', 'êµ­ë‚´ì´ìƒì‚°']),
            (['ìˆ˜ì—…ë£Œ', 'í•™ì›'], ['ìµœì €ì„ê¸ˆ', 'ë²•ì •']),
            (['ìŒì‹ì ', 'ì¹´í˜'], ['ì¸êµ¬í†µê³„']),
            (['íŒë§¤ëŸ‰', 'ì†Œë¹„'], ['ì¸êµ¬', 'ê°€êµ¬']),
        ]
        
        data_point = metadata.get('data_point', '').lower()
        category = metadata.get('category', '').lower()
        
        for q_keywords, d_keywords in INCOMPATIBLE_PAIRS:
            # ì§ˆë¬¸ì— í‚¤ì›Œë“œ ìˆê³ 
            has_q = any(kw in question_lower for kw in q_keywords)
            # ë°ì´í„°ì— ë¹„í˜¸í™˜ í‚¤ì›Œë“œ ìˆìœ¼ë©´
            has_d = any(kw in data_point or kw in category or kw in doc_content for kw in d_keywords)
            
            if has_q and has_d:
                logger.info(f"    ë¹„í˜¸í™˜: {q_keywords} vs {d_keywords}")
                return False
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. í•µì‹¬ í‚¤ì›Œë“œ í•„ìˆ˜ ë§¤ì¹­
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì§ˆë¬¸ì˜ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ
        core_keywords = self._extract_core_keywords(question_lower)
        
        if core_keywords:
            # í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ ìµœì†Œ 1ê°œëŠ” ìˆì–´ì•¼
            matched = any(kw in doc_content for kw in core_keywords)
            
            if not matched:
                logger.info(f"    í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜: {core_keywords}")
                return False
        
        # í†µê³¼
        logger.info(f"    âœ… Relevance ê²€ì¦ í†µê³¼")
        return True
    
    def _extract_core_keywords(self, question: str) -> list:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ì£¼ìš” ëª…ì‚¬ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_map = {
            'ë‹´ë°°': ['ë‹´ë°°', 'í¡ì—°'],
            'ìŒì•…': ['ìŒì•…', 'ìŒì›'],
            'ìŠ¤íŠ¸ë¦¬ë°': ['ìŠ¤íŠ¸ë¦¬ë°', 'êµ¬ë…'],
            'ìŒì‹ì ': ['ìŒì‹ì ', 'ì‹ë‹¹', 'ë ˆìŠ¤í† ë‘'],
            'ì¹´í˜': ['ì¹´í˜', 'ì»¤í”¼'],
            'í•™ì›': ['í•™ì›', 'êµìœ¡'],
            'ìˆ˜ì—…ë£Œ': ['ìˆ˜ì—…ë£Œ', 'í•™ë¹„'],
        }
        
        keywords = []
        for key, variants in keyword_map.items():
            if any(v in question for v in variants):
                keywords.extend(variants)
        
        return keywords
    
    def _convert_unit_if_needed(
        self,
        question: str,
        result_data: dict,
        doc: Any
    ) -> Optional[dict]:
        """
        ë‹¨ìœ„ ë³€í™˜ (v7.6.1)
        
        ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ë¥¼ ì¶”ì¶œí•˜ê³ 
        í•„ìš” ì‹œ ìë™ ë³€í™˜
        
        ì˜ˆ: "í•˜ë£¨ì— íŒë§¤ë˜ëŠ”" â†’ ê°‘/ì¼ í•„ìš”
            ë°ì´í„°: 32,000,000,000 ê°‘/ë…„
            ë³€í™˜: 32,000,000,000 / 365 = 87,671,233 ê°‘/ì¼
        """
        current_unit = result_data.get('unit', '')
        
        # ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ ì¶”ì¶œ
        requested_unit = self._extract_requested_unit(question)
        
        if not requested_unit or not current_unit:
            return None
        
        # ë‹¨ìœ„ ë³€í™˜ í•„ìš” ì—¬ë¶€
        if current_unit == requested_unit:
            return None  # ë³€í™˜ ë¶ˆí•„ìš”
        
        # ë³€í™˜ ê·œì¹™
        CONVERSIONS = {
            ('ê°‘/ë…„', 'ê°‘/ì¼'): ('divide', 365),
            ('ì›/ë…„', 'ì›/ì›”'): ('divide', 12),
            ('ê°œ/ë…„', 'ê°œ/ì¼'): ('divide', 365),
            
            ('ê°‘/ì¼', 'ê°‘/ë…„'): ('multiply', 365),
            ('ì›/ì›”', 'ì›/ë…„'): ('multiply', 12),
        }
        
        conversion_key = (current_unit, requested_unit)
        
        if conversion_key in CONVERSIONS:
            operation, factor = CONVERSIONS[conversion_key]
            
            original_value = result_data['value']
            
            if operation == 'divide':
                converted_value = original_value / factor
            else:  # multiply
                converted_value = original_value * factor
            
            logger.info(f"  ğŸ”„ ë‹¨ìœ„ ë³€í™˜: {original_value:,.0f} {current_unit} â†’ {converted_value:,.0f} {requested_unit}")
            
            # ë³€í™˜ëœ ê²°ê³¼ ë°˜í™˜
            converted_data = result_data.copy()
            converted_data['value'] = converted_value
            converted_data['unit'] = requested_unit
            converted_data['original_value'] = original_value
            converted_data['original_unit'] = current_unit
            converted_data['conversion_applied'] = True
            converted_data['conversion_formula'] = f"{operation} {factor}"
            
            return converted_data
        
        # ë³€í™˜ ê·œì¹™ ì—†ìŒ
        return None
    
    def _extract_requested_unit(self, question: str) -> Optional[str]:
        """
        ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ ì¶”ì¶œ
        
        ì˜ˆ: "í•˜ë£¨ì— íŒë§¤ë˜ëŠ”" â†’ "ê°‘/ì¼"
            "ì—°ê°„ íŒë§¤ëŸ‰ì€" â†’ "ê°‘/ë…„"
            "ì›”í‰ê·  ë§¤ì¶œì€" â†’ "ì›/ì›”"
        """
        question_lower = question.lower()
        
        # ì‹œê°„ ë‹¨ìœ„
        if 'í•˜ë£¨' in question or 'ì¼ì¼' in question or 'ë§¤ì¼' in question:
            if 'ê°‘' in question:
                return 'ê°‘/ì¼'
            elif 'ê°œ' in question:
                return 'ê°œ/ì¼'
            else:
                return 'ì¼'
        
        if 'ì—°ê°„' in question or 'ë…„ê°„' in question or '1ë…„' in question:
            if 'ê°‘' in question:
                return 'ê°‘/ë…„'
            elif 'ì›' in question or 'ë§¤ì¶œ' in question:
                return 'ì›/ë…„'
        
        if 'ì›”' in question or 'í•œ ë‹¬' in question:
            if 'ì›' in question or 'ë§¤ì¶œ' in question:
                return 'ì›/ì›”'
        
        return None
    
    def validate_with_rag(
        self,
        data_point: str,
        claimed_value: Any,
        source_hint: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        RAG ê¸°ë°˜ ë°ì´í„° ê²€ì¦
        
        í”„ë¡œì„¸ìŠ¤:
        ---------
        1. ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ â†’ ì–´ë””ì„œ êµ¬í• ì§€
        2. ì •ì˜ ì‚¬ë¡€ ê²€ìƒ‰ â†’ ì •ì˜ í™•ì¸ ë°©ë²•
        3. ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸
        
        Returns:
        --------
        ê²€ì¦ ê²°ê³¼ + ì¶”ì²œ ì†ŒìŠ¤ + ì •ì˜ ì£¼ì˜ì‚¬í•­
        """
        logger.info(f"[Validator] RAG ê¸°ë°˜ ê²€ì¦: {data_point}")
        
        result = {
            'data_point': data_point,
            'value': claimed_value,
            'recommended_sources': [],
            'definition_warnings': [],
            'validation_status': 'pending'
        }
        
        # 1. ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
        sources = self.search_data_source(data_point, top_k=3)
        if sources:
            result['recommended_sources'] = [
                {
                    'name': doc.metadata.get('source_name'),
                    'reliability': doc.metadata.get('reliability'),
                    'access': doc.metadata.get('access_method'),
                    'confidence': score
                }
                for doc, score in sources
            ]
        
        # 2. ì •ì˜ ê²€ì¦ ì‚¬ë¡€
        definitions = self.search_definition_case(data_point, top_k=2)
        if definitions:
            result['definition_warnings'] = [
                {
                    'case': doc.metadata.get('term'),
                    'gap': doc.metadata.get('gap_description'),
                    'adjustment': doc.metadata.get('adjustment_method')
                }
                for doc, score in definitions
            ]
        
        # 3. ê²€ì¦ ìƒíƒœ
        if sources and sources[0][1] > 0.8:  # ë†’ì€ ìœ ì‚¬ë„
            result['validation_status'] = 'recommended'
        elif sources:
            result['validation_status'] = 'caution'
        else:
            result['validation_status'] = 'no_source_found'
        
        return result

    def load_kpi_library(self) -> Dict:
        """
        KPI ì •ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
        
        Returns:
            KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ dict
        """
        import yaml
        
        kpi_path = Path("data/raw/kpi_definitions.yaml")
        
        if not kpi_path.exists():
            logger.warning(f"KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ì—†ìŒ: {kpi_path}")
            return {}
        
        with open(kpi_path, 'r', encoding='utf-8') as f:
            library = yaml.safe_load(f)
        
        total = library.get('_meta', {}).get('total_kpis', 0)
        logger.info(f"âœ… KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ: {total}ê°œ")
        
        return library
    
    def validate_kpi_definition(
        self,
        metric_name: str,
        provided_definition: Dict
    ) -> Dict[str, Any]:
        """
        KPI ì •ì˜ ê²€ì¦ (s10 Industry KPI Library)
        
        Args:
            metric_name: KPI ì´ë¦„
            provided_definition: {
                'numerator': str,
                'denominator': str,
                'unit': str,
                'scope': {
                    'includes': [...],
                    'excludes': [...]
                }
            }
        
        Returns:
            {
                'status': 'match' | 'partial_match' | 'mismatch' | 'not_found',
                'kpi_id': str,
                'standard_definition': {...},
                'gaps': [...],
                'recommendation': str,
                'comparability_score': float (0-1)
            }
        """
        
        logger.info(f"[Validator] KPI ì •ì˜ ê²€ì¦: {metric_name}")
        
        # KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
        library = self.load_kpi_library()
        
        if not library:
            return {
                'status': 'library_not_found',
                'message': 'KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ì—†ìŒ',
                'recommendation': 'scripts/build_kpi_library.py ì‹¤í–‰ í•„ìš”'
            }
        
        # KPI ê²€ìƒ‰
        kpi = self._search_kpi(metric_name, library)
        
        if not kpi:
            return {
                'status': 'not_found',
                'message': f"KPI '{metric_name}'ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì—†ìŠµë‹ˆë‹¤",
                'recommendation': 'manual_review',
                'create_new': True
            }
        
        logger.info(f"  âœ… KPI ë°œê²¬: {kpi['kpi_id']}")
        
        # ì •ì˜ ë¹„êµ
        gaps = []
        
        # 1. ë¶„ì ë¹„êµ
        provided_numerator = provided_definition.get('numerator', '')
        standard_numerator = kpi.get('formula', {}).get('numerator', '')
        
        if provided_numerator and provided_numerator != standard_numerator:
            gaps.append({
                'field': 'numerator',
                'provided': provided_numerator,
                'standard': standard_numerator,
                'severity': 'high'
            })
            logger.warning(f"  âš ï¸  ë¶„ì ë¶ˆì¼ì¹˜")
        
        # 2. ë¶„ëª¨ ë¹„êµ
        provided_denominator = provided_definition.get('denominator', '')
        standard_denominator = kpi.get('formula', {}).get('denominator', '')
        
        if provided_denominator and standard_denominator != 'N/A' and provided_denominator != standard_denominator:
            gaps.append({
                'field': 'denominator',
                'provided': provided_denominator,
                'standard': standard_denominator,
                'severity': 'high'
            })
            logger.warning(f"  âš ï¸  ë¶„ëª¨ ë¶ˆì¼ì¹˜")
        
        # 3. ë‹¨ìœ„ ë¹„êµ
        provided_unit = provided_definition.get('unit', '')
        standard_unit = kpi.get('unit', '')
        
        if provided_unit and provided_unit != standard_unit:
            gaps.append({
                'field': 'unit',
                'provided': provided_unit,
                'standard': standard_unit,
                'severity': 'medium'
            })
            logger.warning(f"  âš ï¸  ë‹¨ìœ„ ë¶ˆì¼ì¹˜")
        
        # 4. Scope ë¹„êµ
        scope_gaps = self._compare_scope(
            provided_definition.get('scope', {}),
            kpi.get('scope', {})
        )
        gaps.extend(scope_gaps)
        
        # ìƒíƒœ ê²°ì •
        if len(gaps) == 0:
            status = 'match'
            logger.info(f"  âœ… ì™„ì „ ì¼ì¹˜")
        elif any(g['severity'] == 'high' for g in gaps):
            status = 'mismatch'
            logger.warning(f"  âŒ ë¶ˆì¼ì¹˜ (high severity)")
        else:
            status = 'partial_match'
            logger.info(f"  âš ï¸  ë¶€ë¶„ ì¼ì¹˜")
        
        # ë¹„êµ ê°€ëŠ¥ì„± ì ìˆ˜
        comparability_score = 1.0 - (len(gaps) * 0.2)
        comparability_score = max(0, comparability_score)
        
        # ê¶Œê³ ì‚¬í•­
        if status == 'match':
            recommendation = 'âœ… í‘œì¤€ ì •ì˜ì™€ ì¼ì¹˜. ë¹„êµ ê°€ëŠ¥'
        elif status == 'mismatch':
            recommendation = 'âŒ ì •ì˜ ë¶ˆì¼ì¹˜. ë¹„êµ ë¶ˆê°€ â†’ í‘œì¤€í™” í•„ìš”'
        else:
            recommendation = 'âš ï¸  ë¶€ë¶„ ì¼ì¹˜. ì£¼ì˜í•˜ì—¬ ë¹„êµ'
        
        logger.info(f"  ë¹„êµ ê°€ëŠ¥ì„±: {comparability_score*100:.0f}%")
        
        return {
            'status': status,
            'kpi_id': kpi['kpi_id'],
            'standard_definition': kpi,
            'gaps': gaps,
            'recommendation': recommendation,
            'comparability_score': comparability_score
        }
    
    def _search_kpi(self, metric_name: str, library: Dict) -> Optional[Dict]:
        """KPI ê²€ìƒ‰"""
        
        metric_lower = metric_name.lower()
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        for key in library:
            if key.endswith('_kpis'):
                for kpi in library[key]:
                    kpi_name_lower = kpi['metric_name'].lower()
                    
                    # ì •í™•í•œ ë§¤ì¹­
                    if kpi_name_lower == metric_lower:
                        return kpi
                    
                    # ë¶€ë¶„ ë§¤ì¹­
                    if metric_lower in kpi_name_lower or kpi_name_lower in metric_lower:
                        return kpi
        
        return None
    
    def _compare_scope(
        self,
        provided_scope: Dict,
        standard_scope: Dict
    ) -> List[Dict]:
        """Scope ë¹„êµ"""
        
        gaps = []
        
        # Includes ë¹„êµ
        provided_includes = set(provided_scope.get('includes', []))
        standard_includes = set(standard_scope.get('includes', []))
        
        missing_includes = standard_includes - provided_includes
        extra_includes = provided_includes - standard_includes
        
        if missing_includes:
            gaps.append({
                'field': 'scope.includes',
                'provided': list(provided_includes),
                'standard': list(standard_includes),
                'missing': list(missing_includes),
                'severity': 'medium'
            })
        
        # Excludes ë¹„êµ
        provided_excludes = set(provided_scope.get('excludes', []))
        standard_excludes = set(standard_scope.get('excludes', []))
        
        missing_excludes = standard_excludes - provided_excludes
        
        if missing_excludes:
            gaps.append({
                'field': 'scope.excludes',
                'provided': list(provided_excludes),
                'standard': list(standard_excludes),
                'missing': list(missing_excludes),
                'severity': 'high'  # ì œì™¸ í•­ëª© ì¤‘ìš”
            })
        
        return gaps


    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # v7.7.0: Estimator êµì°¨ ê²€ì¦ (5-Phase)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def validate_estimation(
        self,
        question: str,
        claimed_value: float,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        ì¶”ì •ê°’ì˜ í•©ë¦¬ì„± ê²€ì¦ (Estimator 5-Phase êµì°¨ ê²€ì¦)
        
        ì›ì¹™ (v7.7.0):
        -------------
        1. ì§ì ‘ ì¶”ì • ê¸ˆì§€ âŒ
        2. Estimatorì—ê²Œ êµì°¨ ê²€ì¦ ìš”ì²­ âœ… (Phase 0â†’1â†’2â†’3â†’4 ìë™)
        3. ë¹„êµ ë° íŒë‹¨
        
        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "B2B SaaS Churn RateëŠ”?")
            claimed_value: ì£¼ì¥ëœ ê°’ (ì˜ˆ: 0.08)
            context: ë§¥ë½ (domain, region ë“±)
        
        Returns:
            {
                'claimed_value': 0.08,
                'estimator_value': 0.06,
                'estimator_phase': 2,  # v7.7.0: Phase 0-4
                'estimator_confidence': 0.85,
                'estimator_reasoning': {...},
                'difference_pct': 0.33,
                'validation_result': 'caution'
            }
        
        Example:
            >>> validator = ValidatorRAG()
            >>> result = validator.validate_estimation(
            ...     "B2B SaaS Churn RateëŠ”?",
            ...     claimed_value=0.08,
            ...     context={'domain': 'B2B_SaaS'}
            ... )
            >>> print(result['validation_result'])  # 'caution'
        """
        logger.info(f"[Validator] ì¶”ì •ê°’ ê²€ì¦: {question} = {claimed_value}")
        
        # Estimator Lazy ì´ˆê¸°í™”
        if self.estimator is None:
            self.estimator = get_estimator_rag()
            logger.info("  âœ… Estimator ì—°ê²° (5-Phase)")
        
        # Estimatorì—ê²Œ êµì°¨ ê²€ì¦ ìš”ì²­ (Phase 0â†’1â†’2â†’3â†’4 ìë™ ì‹œë„)
        est_result = self.estimator.estimate(
            question=question,
            domain=context.get('domain') if context else None,
            region=context.get('region') if context else None
        )
        
        if not est_result:
            return {
                'validation': 'unable',
                'reason': 'Estimator ì¶”ì • ì‹¤íŒ¨'
            }
        
        # ë¹„êµ
        diff_pct = abs(claimed_value - est_result.value) / est_result.value if est_result.value else 0
        
        validation = {
            'claimed_value': claimed_value,
            'estimator_value': est_result.value,
            'estimator_confidence': est_result.confidence,
            'estimator_phase': est_result.phase,  # v7.7.0: tier â†’ phase
            
            # v7.7.0: ìƒì„¸ ê·¼ê±° í¬í•¨
            'estimator_reasoning': est_result.reasoning_detail,
            'estimator_components': est_result.component_estimations,
            'estimator_trace': est_result.estimation_trace,
            
            'difference_pct': diff_pct,
            
            'validation_result': (
                'pass' if diff_pct < 0.30 else
                'caution' if diff_pct < 0.50 else
                'fail'
            ),
            
            'recommendation': self._generate_recommendation(
                claimed_value, est_result, diff_pct
            )
        }
        
        logger.info(f"  ê²€ì¦: {validation['validation_result']} (ì°¨ì´ {diff_pct:.0%})")
        
        return validation
    
    def _generate_recommendation(
        self,
        claimed: float,
        est_result,
        diff_pct: float
    ) -> str:
        """ê²€ì¦ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
        
        lines = []
        lines.append(f"ì£¼ì¥ê°’: {claimed}")
        lines.append(f"Estimator ì¶”ì •: {est_result.value} (Phase {est_result.phase}, ì‹ ë¢°ë„ {est_result.confidence:.0%})")
        lines.append(f"ì°¨ì´: {diff_pct:.0%}")
        lines.append(f"")
        
        if diff_pct < 0.30:
            lines.append("âœ… ê²€ì¦ í†µê³¼: í•©ë¦¬ì  ë²”ìœ„")
        elif diff_pct < 0.50:
            lines.append("âš ï¸  ì£¼ì˜: ì°¨ì´ê°€ ë‹¤ì†Œ í¼")
            lines.append(f"Estimator ê·¼ê±° í™•ì¸ ê¶Œì¥:")
            if est_result.reasoning_detail:
                lines.append(f"  - ì „ëµ: {est_result.reasoning_detail.get('method')}")
                lines.append(f"  - ì¦ê±°: {est_result.reasoning_detail.get('evidence_count')}ê°œ")
        else:
            lines.append("âŒ ê²€ì¦ ì‹¤íŒ¨: ì°¨ì´ê°€ ë§¤ìš° í¼")
            lines.append(f"Estimator ì¶”ì • ì¬ê²€í†  í•„ìš”")
        
        return "\n".join(lines)


    def search_historical_data(
        self,
        market: str,
        years: range
    ) -> Dict[str, Any]:
        """
        ê³¼ê±° ë°ì´í„° íƒìƒ‰ ë° ìˆ˜ì§‘ (v7.8.0 ì‹ ê·œ)
        
        Observer Timeline ë¶„ì„ì„ ìœ„í•œ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘.
        Estimator í˜‘ì—…ìœ¼ë¡œ ëˆ„ë½ ë°ì´í„° ì¶”ì •.
        
        Args:
            market: ì‹œì¥ ì´ë¦„
            years: range(2015, 2026) â†’ 2015-2025
        
        Returns:
            {
                'market_size_by_year': {year: {value, source, reliability}, ...},
                'players_by_year': {year: {player: {share, source}, ...}, ...},
                'events': [Event, ...],
                'hhi_by_year': {year: hhi, ...},
                'player_count_by_year': {year: count, ...},
                'data_quality': {verified_ratio, avg_confidence, ...}
            }
        """
        logger.info(f"[Validator] ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘: {market} ({years.start}-{years.stop-1})")
        
        result = {
            'market_size_by_year': {},
            'players_by_year': {},
            'events': [],
            'hhi_by_year': {},
            'player_count_by_year': {},
            'data_gaps': {'missing_years': [], 'estimator_requests': []}
        }
        
        # Step 1: ê³µì‹ í†µê³„ ê²€ìƒ‰
        logger.info("  Step 1: ê³µì‹ í†µê³„ ê²€ìƒ‰")
        official_data = self._search_official_statistics(market, years)
        result['market_size_by_year'].update(official_data.get('market_size', {}))
        
        # Step 2: ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG)
        logger.info("  Step 2: ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG)")
        industry_data = self._search_industry_reports_rag(market, years)
        result['market_size_by_year'].update(industry_data.get('market_size', {}))
        
        # Step 3: ê³µì‹œ ë°ì´í„° (ìƒì¥ì‚¬)
        logger.info("  Step 3: ê³µì‹œ ë°ì´í„° ê²€ìƒ‰")
        public_data = self._search_public_filings(market, years)
        result['players_by_year'].update(public_data.get('players', {}))
        
        # Step 4: ë‰´ìŠ¤/ì‚¬ê±´
        logger.info("  Step 4: ì£¼ìš” ì‚¬ê±´ ê²€ìƒ‰")
        events = self._search_news_events(market, years)
        result['events'] = events
        
        # Step 5: Gap ì‹ë³„
        logger.info("  Step 5: ë°ì´í„° Gap ì‹ë³„")
        gaps = self._identify_data_gaps(result, years)
        result['data_gaps'] = gaps
        
        # Step 6: Estimator í˜‘ì—… (Gap ì±„ìš°ê¸°)
        if gaps['missing_years']:
            logger.info(f"  Step 6: Estimator í˜‘ì—… ({len(gaps['missing_years'])}ê°œ ëˆ„ë½ ì—°ë„)")
            result = self._fill_gaps_with_estimator(result, gaps)
        
        # Step 7: ë°ì´í„° í’ˆì§ˆ í‰ê°€
        result['data_quality'] = self._assess_data_quality(result, years)
        
        logger.info(f"  âœ… ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (í’ˆì§ˆ: {result['data_quality'].get('grade', 'N/A')})")
        
        return result
    
    def _search_official_statistics(self, market: str, years: range) -> Dict:
        """ê³µì‹ í†µê³„ ê²€ìƒ‰ (í†µê³„ì²­, í•œêµ­ì€í–‰ ë“±)"""
        # TODO: ì‹¤ì œ API ì—°ë™ ë˜ëŠ” ì›¹ ê²€ìƒ‰
        # í˜„ì¬ëŠ” placeholder
        logger.info("    (êµ¬í˜„ ì˜ˆì •: í†µê³„ì²­ API)")
        return {'market_size': {}}
    
    def _search_industry_reports_rag(self, market: str, years: range) -> Dict:
        """ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG í™œìš©)"""
        # data_sources_registryì—ì„œ ê²€ìƒ‰
        if self.source_store:
            results = self.source_store.similarity_search(
                f"{market} market size historical data",
                k=5
            )
            logger.info(f"    âœ… RAG: {len(results)}ê°œ ì†ŒìŠ¤ ë°œê²¬")
        
        # TODO: ì‹¤ì œ ë¦¬í¬íŠ¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        return {'market_size': {}}
    
    def _search_public_filings(self, market: str, years: range) -> Dict:
        """ê³µì‹œ ë°ì´í„° ê²€ìƒ‰ (DART API ë“±)"""
        # TODO: DART API ì—°ë™
        logger.info("    (êµ¬í˜„ ì˜ˆì •: DART API)")
        return {'players': {}}
    
    def _search_news_events(self, market: str, years: range) -> List[Dict]:
        """ë‰´ìŠ¤ì—ì„œ ì£¼ìš” ì‚¬ê±´ ì¶”ì¶œ"""
        # TODO: ë‰´ìŠ¤ ê²€ìƒ‰ ë° ì‚¬ê±´ ì¶”ì¶œ
        logger.info("    (êµ¬í˜„ ì˜ˆì •: ë‰´ìŠ¤ ê²€ìƒ‰)")
        return []
    
    def _identify_data_gaps(self, collected_data: Dict, years: range) -> Dict:
        """ë°ì´í„° Gap ì‹ë³„"""
        gaps = {'missing_years': [], 'estimator_requests': []}
        
        # ëˆ„ë½ ì—°ë„ íŒŒì•…
        for year in years:
            if year not in collected_data['market_size_by_year']:
                gaps['missing_years'].append(year)
                
                # Estimator ìš”ì²­ ì¤€ë¹„
                gaps['estimator_requests'].append({
                    'type': 'market_size_interpolation',
                    'year': year,
                    'market': collected_data.get('market'),
                    'known_data': collected_data['market_size_by_year']
                })
        
        logger.info(f"    Gap: {len(gaps['missing_years'])}ê°œ ëˆ„ë½ ì—°ë„")
        return gaps
    
    def _fill_gaps_with_estimator(self, data: Dict, gaps: Dict) -> Dict:
        """Estimator í˜‘ì—…ìœ¼ë¡œ Gap ì±„ìš°ê¸°"""
        try:
            from umis_rag.agents.estimator import get_estimator_rag
            estimator = get_estimator_rag()
            
            for request in gaps['estimator_requests']:
                if request['type'] == 'market_size_interpolation':
                    # ë³´ê°„ ìš”ì²­
                    # TODO: Estimator.estimate() í˜¸ì¶œ
                    logger.info(f"      Estimator: {request['year']}ë…„ ì¶”ì • ì¤‘...")
                    
                    # Placeholder
                    # result = estimator.estimate(...)
                    # data['market_size_by_year'][request['year']] = result
        
        except Exception as e:
            logger.warning(f"    âš ï¸ Estimator í˜‘ì—… ì‹¤íŒ¨: {e}")
        
        return data
    
    def _assess_data_quality(self, data: Dict, years: range) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        total_years = len(list(years))
        verified_years = sum(
            1 for y, d in data['market_size_by_year'].items()
            if d.get('reliability') == 'high'
        )
        estimated_years = sum(
            1 for y, d in data['market_size_by_year'].items()
            if d.get('reliability') == 'estimated'
        )
        
        verified_ratio = verified_years / total_years if total_years > 0 else 0
        
        # ë“±ê¸‰ íŒì •
        if verified_ratio >= 0.5:
            grade = 'A (High)'
        elif verified_ratio >= 0.3:
            grade = 'B (Medium)'
        else:
            grade = 'C (Low)'
        
        return {
            'total_years': total_years,
            'verified_years': verified_years,
            'estimated_years': estimated_years,
            'verified_ratio': verified_ratio,
            'grade': grade
        }


# Validator RAG ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_validator_rag_instance = None

def get_validator_rag() -> ValidatorRAG:
    """Validator RAG ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _validator_rag_instance
    if _validator_rag_instance is None:
        _validator_rag_instance = ValidatorRAG()
    return _validator_rag_instance

