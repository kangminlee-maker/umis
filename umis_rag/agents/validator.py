"""
Validator RAG Agent Module

Validator (Rachel) ì—ì´ì „íŠ¸ì˜ RAG ê¸°ë°˜ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

í•µì‹¬ ê°œë…:
-----------
1. **Data Source Discovery**: ë°ì´í„° ì†ŒìŠ¤ ìë™ ê²€ìƒ‰ ë° ì¶”ì²œ
2. **Definition Validation**: ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ì°¸ì¡°
3. **Gap Analysis**: ì •ì˜ ë¶ˆì¼ì¹˜ ë¶„ì„ ê°€ì´ë“œ
4. **Creative Sourcing**: ì°½ì˜ì  ë°ì´í„° ì†Œì‹± ë°©ë²•
5. **Definite Data Search**: í™•ì • ë°ì´í„° ìš°ì„  ê²€ìƒ‰ (v7.6.0+)

Validatorì˜ í•µì‹¬ ì—­í• :
----------------------
1. í™•ì • ë°ì´í„° ê²€ìƒ‰ (Estimator Phase 2, v7.6.0+) â­ ìµœìš°ì„ !
2. ë°ì´í„° ì •ì˜ ê²€ì¦
3. ë‹¨ìœ„ ìë™ ë³€í™˜ (v7.6.1+)
4. Relevance ê²€ì¦ (v7.6.1+)
5. ì‹ ë¢°ë„ í‰ê°€

RAG Collections:
----------------
- data_sources_registry: ë°ì´í„° ì†ŒìŠ¤ ëª©ë¡ (24ê°œ, v7.6.0+)
- definition_validation_cases: ì •ì˜ ê²€ì¦ ì‚¬ë¡€ (100ê°œ)

v7.6.0+ ì£¼ìš” ë³€ê²½:
------------------
- search_definite_data(): Estimator ì¶”ì • ì „ í™•ì • ë°ì´í„° ê²€ìƒ‰
- ë‹¨ìœ„ ìë™ ë³€í™˜ (ê°‘/ë…„ â†’ ê°‘/ì¼ ë“±)
- Relevance ê²€ì¦ (GDP ì˜¤ë¥˜ ë°©ì§€)
- 94.7% ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±
"""

from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.core.config import settings
from umis_rag.utils.logger import logger

# v7.3.2: Estimator í†µí•© (ì¶”ì •ì¹˜ ê²€ì¦ìš©)
from umis_rag.agents.estimator import get_estimator_rag


class ValidatorRAG:
    """
    Validator (Rachel) RAG Agent
    
    ì—­í• :
    -----
    - ë°ì´í„° ì†ŒìŠ¤ ë°œê²¬
    - ì •ì˜ ê²€ì¦
    - ì‹ ë¢°ë„ í‰ê°€
    - Gap ë¶„ì„
    
    í•µì‹¬ ë©”ì„œë“œ:
    -----------
    - search_data_source(): ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
    - search_definition_case(): ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰
    - search_gap_analysis(): Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰
    
    í˜‘ì—…:
    -----
    - Quantifier: ëª¨ë“  ê³„ì‚°ì˜ ë°ì´í„° ì •ì˜ ê²€ì¦ (í•„ìˆ˜!)
    - Observer, Explorer: ë°ì´í„° ì¶œì²˜ í™•ì¸
    """
    
    def __init__(self):
        """Validator RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        logger.info("Validator RAG ì—ì´ì „íŠ¸ ì´ˆê¸°í™”")
        
        # v7.3.2: Estimator ì—°ê²° (êµì°¨ ê²€ì¦ìš©)
        self.estimator = None  # Lazy ì´ˆê¸°í™”
        
        # v7.9.0: API ë°ì´í„° ì†ŒìŠ¤ (DART, KOSIS)
        self.dart_api_key = settings.dart_api_key
        self.kosis_api_key = settings.kosis_api_key
        
        # Embeddings
        self.embeddings = OpenAIEmbeddings(
            model=settings.embedding_model,
            openai_api_key=settings.openai_api_key
        )
        
        # Vector Stores
        try:
            # 1. ë°ì´í„° ì†ŒìŠ¤
            self.source_store = Chroma(
                collection_name="data_sources_registry",
                embedding_function=self.embeddings,
                persist_directory=str(settings.chroma_persist_dir)
            )
            logger.info(f"  âœ… ë°ì´í„° ì†ŒìŠ¤: {self.source_store._collection.count()}ê°œ")
        except Exception as e:
            logger.warning(f"  âš ï¸  ë°ì´í„° ì†ŒìŠ¤ Collection ì—†ìŒ (êµ¬ì¶• í•„ìš”): {e}")
            self.source_store = None
        
        try:
            # 2. ì •ì˜ ê²€ì¦ ì‚¬ë¡€
            self.definition_store = Chroma(
                collection_name="definition_validation_cases",
                embedding_function=self.embeddings,
                persist_directory=str(settings.chroma_persist_dir)
            )
            logger.info(f"  âœ… ì •ì˜ ì‚¬ë¡€: {self.definition_store._collection.count()}ê°œ")
        except Exception as e:
            logger.warning(f"  âš ï¸  ì •ì˜ ê²€ì¦ Collection ì—†ìŒ (êµ¬ì¶• í•„ìš”): {e}")
            self.definition_store = None
    
    def search_data_source(
        self,
        data_type: str,
        top_k: int = 5
    ) -> List[tuple[Document, float]]:
        """
        ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–´ë””ì„œ êµ¬í• ì§€ ëª¨ë¥¼ ë•Œ
        
        ì˜ˆì‹œ:
        -----
        Input: "í•œêµ­ SaaS ì‹œì¥ ê·œëª¨"
        Output: [Gartner (85% ì‹ ë¢°ë„), IDC Korea, ...]
        
        Parameters:
        -----------
        data_type: ì°¾ëŠ” ë°ì´í„° ìœ í˜•
        top_k: ë°˜í™˜í•  ì†ŒìŠ¤ ìˆ˜
        
        Returns:
        --------
        List of (Document, similarity_score)
        """
        if not self.source_store:
            logger.warning("  âš ï¸  ë°ì´í„° ì†ŒìŠ¤ RAG ë¯¸êµ¬ì¶•")
            return []
        
        logger.info(f"[Validator] ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰")
        logger.info(f"  ë°ì´í„° ìœ í˜•: {data_type}")
        
        results = self.source_store.similarity_search_with_score(
            data_type,
            k=top_k
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì†ŒìŠ¤ ë°œê²¬")
        for doc, score in results:
            source_name = doc.metadata.get('source_name', 'Unknown')
            reliability = doc.metadata.get('reliability', 'N/A')
            logger.info(f"    - {source_name} (ì‹ ë¢°ë„: {reliability}, ìœ ì‚¬ë„: {score:.2f})")
        
        return results
    
    def search_definition_case(
        self,
        term: str,
        top_k: int = 3
    ) -> List[tuple[Document, float]]:
        """
        ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        ë°ì´í„° ì •ì˜ê°€ ì• ë§¤í•˜ê±°ë‚˜, ì‚°ì—…ë³„ ì°¨ì´ê°€ ìˆì„ ë•Œ
        
        ì˜ˆì‹œ:
        -----
        Input: "MAU (ì›”ê°„ í™œì„± ì‚¬ìš©ì)"
        Output: [Google ì •ì˜ vs Facebook ì •ì˜, Gap 20-30%, ...]
        
        Parameters:
        -----------
        term: ê²€ì¦í•  ìš©ì–´
        top_k: ë°˜í™˜í•  ì‚¬ë¡€ ìˆ˜
        
        Returns:
        --------
        List of (Document, similarity_score)
        """
        if not self.definition_store:
            logger.warning("  âš ï¸  ì •ì˜ ê²€ì¦ RAG ë¯¸êµ¬ì¶•")
            return []
        
        logger.info(f"[Validator] ì •ì˜ ê²€ì¦ ì‚¬ë¡€ ê²€ìƒ‰")
        logger.info(f"  ìš©ì–´: {term}")
        
        results = self.definition_store.similarity_search_with_score(
            term,
            k=top_k
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì‚¬ë¡€ ë°œê²¬")
        for doc, score in results:
            case_term = doc.metadata.get('term', 'Unknown')
            gap_level = doc.metadata.get('gap_level', 'N/A')
            logger.info(f"    - {case_term} (Gap: {gap_level}, ìœ ì‚¬ë„: {score:.2f})")
        
        return results
    
    def search_gap_analysis(
        self,
        data_point: str,
        original_def: str,
        needed_def: str
    ) -> List[tuple[Document, float]]:
        """
        Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰
        
        ì‚¬ìš© ì‹œì :
        ----------
        ì›ë³¸ ì •ì˜ì™€ í•„ìš”í•œ ì •ì˜ê°€ ë‹¤ë¥¼ ë•Œ, ì¡°ì • ë°©ë²• ì°¾ê¸°
        
        ì˜ˆì‹œ:
        -----
        Input: 
          - data: "ë‚šì‹œì¸êµ¬ 750ë§Œ"
          - original: "ì—° 1íšŒ ì´ìƒ, ë°”ë‹¤ë‚šì‹œë§Œ"
          - needed: "ì›” 1íšŒ ì´ìƒ, ì „ì²´ ë‚šì‹œ"
        
        Output: [ìœ ì‚¬ Gap ì‚¬ë¡€, ì¡°ì • ë°©ë²•, ...]
        """
        if not self.definition_store:
            return []
        
        logger.info(f"[Validator] Gap ë¶„ì„ ê°€ì´ë“œ ê²€ìƒ‰")
        
        # Gap ì„¤ëª… ì¡°í•©
        gap_query = f"{data_point}: ì›ë³¸({original_def}) vs í•„ìš”({needed_def})"
        
        results = self.definition_store.similarity_search_with_score(
            gap_query,
            k=3,
            filter={"type": "gap_analysis"}
        )
        
        logger.info(f"  âœ… {len(results)}ê°œ ì¡°ì • ê°€ì´ë“œ ë°œê²¬")
        
        return results
    
    def search_definite_data(
        self,
        question: str,
        context: Optional[Any] = None
    ) -> Optional[Dict[str, Any]]:
        """
        í™•ì • ë°ì´í„° ê²€ìƒ‰ (ì¶”ì • ì „ í•„ìˆ˜ í™•ì¸!)
        
        ì—­í•  (v7.8.1 ì—„ê²©í™”):
        ---------------------
        - ì´ë¯¸ í™•ì¸ëœ ë°ì´í„°ë¥¼ ì¬ì‚¬ìš© (ìºì‹±)
        - 100% ë§¤ì¹­ ë˜ëŠ” 95% ì´ìƒ ìœ ì‚¬ë„ë§Œ í—ˆìš©
        - í•µì‹¬ í‚¤ì›Œë“œ ì™„ì „ ì¼ì¹˜ í•„ìˆ˜
        
        ì›ì¹™:
        -----
        1. Phase 2ëŠ” "ì¬ì‚¬ìš©"ì´ ëª©ì  (ìƒˆë¡œìš´ ì¶”ì • X)
        2. ê±°ì˜ ì™„ë²½í•˜ê²Œ ë§¤ì¹­ë  ë•Œë§Œ ì‚¬ìš©
        3. ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ Phase 3/4ë¡œ ë„˜ê¹€
        
        ê²€ìƒ‰ ë²”ìœ„:
        ----------
        1. data_sources_registry (ê³µì‹ í†µê³„)
        2. ë©”íƒ€ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
        3. ì—„ê²©í•œ Relevance ê²€ì¦
        
        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "í•œêµ­ ë‹´ë°° íŒë§¤ëŸ‰ì€?")
            context: ë§¥ë½ (domain, region ë“±)
        
        Returns:
            {
                'value': 87671233,
                'unit': 'ê°‘/ì¼',
                'source': 'ê¸°íšì¬ì •ë¶€',
                'confidence': 1.0,
                'definition': 'ì£¼ë¯¼ë“±ë¡ ê¸°ì¤€',
                'last_updated': '2023'
            } ë˜ëŠ” None
        
        Example:
            >>> validator = ValidatorRAG()
            >>> result = validator.search_definite_data("í•œêµ­ ì¸êµ¬ëŠ”?")
            >>> if result:
            ...     print(f"{result['value']}ëª… (ì¶œì²˜: {result['source']})")
        """
        if not self.source_store:
            logger.warning("  âš ï¸  data_sources_registry ì—†ìŒ (êµ¬ì¶• í•„ìš”)")
            return None
        
        logger.info(f"[Validator] í™•ì • ë°ì´í„° ê²€ìƒ‰: {question}")
        
        # Context ì •ë³´ ì¶”ì¶œ
        domain_str = ""
        region_str = ""
        if context and hasattr(context, 'domain'):
            domain_str = f"{context.domain} " if context.domain != "General" else ""
        if context and hasattr(context, 'region'):
            region_str = f"{context.region} " if context.region else ""
        
        # v7.9.0: ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„± (ì •ê·œí™” ì—†ì´ ì›ë³¸ ì‚¬ìš©)
        # ì´ìœ : ë°ì´í„°ë² ì´ìŠ¤ì— ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ë³¸ì´ ì €ì¥ë˜ì–´ ìˆìŒ
        # í–¥í›„: ë°ì´í„°ë² ì´ìŠ¤ ì¬êµ¬ì¶• ì‹œ ì •ê·œí™” ì ìš© ì˜ˆì •
        search_query = f"{region_str}{domain_str}{question}".strip()
        logger.info(f"  ê²€ìƒ‰: {search_query}")
        
        # data_sources_registry ê²€ìƒ‰ (top 3)
        results = self.source_store.similarity_search_with_score(
            search_query,
            k=3
        )
        
        if not results:
            logger.info("  â†’ í™•ì • ë°ì´í„° ì—†ìŒ")
            return None
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # v7.9.0: Phase 2 ì„ê³„ê°’ ê°•í™” (ê³¼ë„ ë§¤ì¹­ ë°©ì§€)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ChromaDB L2 distance (ì‹¤ì œ ì¸¡ì •ê°’):
        #   - 0.70~0.90: ê±°ì˜ ë™ì¼í•œ ì§ˆë¬¸ ("í•œêµ­ ì¸êµ¬" vs "í•œêµ­ ì´ì¸êµ¬")
        #   - 0.90~1.10: ë§¤ìš° ìœ ì‚¬ ("í•œêµ­ ì¸êµ¬" vs "ë‹´ë°° íŒë§¤ëŸ‰")
        #   - 1.10~1.30: Registry ë‚´ ë‹¤ë¥¸ í•­ëª© ("í•œêµ­ ì¸êµ¬" vs "ì„œìš¸ ì¸êµ¬")
        #   - 1.30+: ì™„ì „íˆ ë‹¤ë¥¸ ê°œë…
        # 
        # Phase 2 ëª©ì : ì´ë¯¸ í™•ì¸í•œ ë°ì´í„° ì¬ì‚¬ìš© (ìºì‹±)
        # 
        # v7.9.0 ë³€ê²½:
        # - v7.8.1: < 0.90 (100%), < 1.10 (95%)
        # - v7.9.0: < 0.85 (100%), 0.85~0.95 ì œê±°ë¨
        # 
        # ì´ìœ :
        # - "SaaS ì„œë¹„ìŠ¤ ARPU" (0.979) â†’ "B2B SaaS ARPU" ë§¤ì¹­ì€ ë¶€ì ì ˆ
        # - Phase 2ëŠ” "ê±°ì˜ ì™„ë²½í•œ ë§¤ì¹­"ë§Œ í—ˆìš© (ì¬ì‚¬ìš© ëª©ì )
        # - ì• ë§¤í•œ ì¼€ì´ìŠ¤ëŠ” Phase 3/4ë¡œ ìœ„ì„ (ì¶”ì • í•„ìš”)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        for doc, score in results:
            logger.info(f"  í›„ë³´: {doc.metadata.get('source_name', 'Unknown')} (distance: {score:.3f})")
            
            # v7.9.0: ì—„ê²©í•œ ì„ê³„ê°’ (ê±°ì˜ ì™„ë²½í•œ ë§¤ì¹­ë§Œ)
            if score < 0.85:
                confidence_level = "perfect"
                confidence = 1.0
                logger.info(f"    â†’ ê±°ì˜ ì™„ë²½í•œ ë§¤ì¹­ (100%)")
            else:
                # v7.9.0: 0.85 ì´ìƒì€ ëª¨ë‘ ìŠ¤í‚µ â†’ Phase 3/4ë¡œ ìœ„ì„
                logger.info(f"    â†’ ìœ ì‚¬ë„ ë¶ˆì¶©ë¶„ ({score:.3f}) â†’ Phase 3/4ë¡œ ìœ„ì„")
                continue
            
            metadata = doc.metadata
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ê°’ ì¶”ì¶œ
            if 'value' not in metadata or metadata['value'] is None:
                logger.info(f"    â†’ ê°’ ì—†ìŒ â†’ ìŠ¤í‚µ")
                continue
            
            # â­ ì—„ê²©í•œ Relevance ê²€ì¦!
            relevance_result = self._is_relevant_strict(question, doc, context)
            
            if not relevance_result['is_relevant']:
                logger.warning(f"    âš ï¸  Relevance ê²€ì¦ ì‹¤íŒ¨: {relevance_result['reason']}")
                continue
            
            logger.info(f"    âœ… Relevance ê²€ì¦ í†µê³¼: {relevance_result['matched_keywords']}")
            logger.info(f"  âœ… í™•ì • ë°ì´í„° ë°œê²¬! (ì‹ ë¢°ë„: {confidence:.0%})")
            
            # ê²°ê³¼ ë°˜í™˜
            result_data = {
                'value': metadata['value'],
                'unit': metadata.get('unit', ''),
                'source': metadata.get('source_name', 'Unknown'),
                'confidence': confidence,
                'confidence_level': confidence_level,
                'similarity_score': score,
                'definition': metadata.get('definition', ''),
                'last_updated': metadata.get('year', ''),
                'access_method': metadata.get('access_method', ''),
                'reliability': metadata.get('reliability', 'high'),
                'document': doc.page_content
            }
            
            # ë‹¨ìœ„ ë³€í™˜ ì‹œë„
            converted = self._convert_unit_if_needed(question, result_data, doc)
            if converted:
                result_data = converted
            
            return result_data
        
        logger.info("  â†’ í™•ì • ë°ì´í„° ì—†ìŒ (ìœ ì‚¬ë„ ë‚®ê±°ë‚˜ ê´€ë ¨ì„± ì—†ìŒ)")
        return None
    
    def _normalize_question(self, question: str) -> str:
        """
        ì§ˆë¬¸ ì •ê·œí™” (v7.9.0)
        
        ëª©ì :
        - ë™ì¼í•œ ì˜ë¯¸ì˜ ë‹¤ì–‘í•œ í‘œí˜„ì„ í†µì¼
        - ìœ ì‚¬ë„ ë§¤ì¹­ ì •í™•ë„ í–¥ìƒ
        
        ì •ê·œí™” ê·œì¹™:
        1. ëŒ€ì†Œë¬¸ì í†µì¼ (ì†Œë¬¸ì)
        2. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        3. ì¡°ì‚¬ ì œê±° ("ì€?", "ëŠ”?", "ì˜", "ë¥¼" ë“±)
        4. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±° ("í‰ê· ", "ëŒ€ëµ", "ì•½" ë“±)
        5. ì§ˆë¬¸ í˜•ì‹ ì œê±° ("?", "ì¸ê°€", "ì…ë‹ˆê¹Œ" ë“±)
        
        Args:
            question: ì›ë³¸ ì§ˆë¬¸
        
        Returns:
            ì •ê·œí™”ëœ ì§ˆë¬¸
        
        Example:
            >>> self._normalize_question("B2B SaaSì˜ í‰ê·  ARPUëŠ”?")
            "b2b saas arpu"
            >>> self._normalize_question("í•œêµ­  ìŒì‹ì   ìˆ˜ëŠ”  ëª‡ ê°œ?")
            "í•œêµ­ ìŒì‹ì  ìˆ˜"
        """
        import re
        
        # 1. ì†Œë¬¸ì ë³€í™˜
        normalized = question.lower()
        
        # 2. ì¡°ì‚¬ ì œê±° (í•œêµ­ì–´)
        # "ì€?", "ëŠ”?", "ì˜", "ë¥¼", "ì„", "ê°€", "ì´" ë“±
        normalized = re.sub(r'[ì€ëŠ”ì˜ë¥¼ì„ê°€ì´]\??', '', normalized)
        
        # 3. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°
        # "í‰ê· ", "ëŒ€ëµ", "ì•½", "ì •ë„" ë“±
        remove_words = ['í‰ê· ', 'ëŒ€ëµ', 'ì•½', 'ì •ë„', 'ë³´í†µ', 'ì¼ë°˜ì ', 'ì¼ë°˜ì ìœ¼ë¡œ']
        for word in remove_words:
            normalized = normalized.replace(word, '')
        
        # 4. ì§ˆë¬¸ í˜•ì‹ ì œê±°
        # "?", "ì¸ê°€", "ì…ë‹ˆê¹Œ", "ì¸ì§€", "ëª‡" ë“±
        normalized = re.sub(r'\?+', '', normalized)
        normalized = re.sub(r'(ì¸ê°€|ì…ë‹ˆê¹Œ|ì¸ì§€|ëª‡|ê°œ)', '', normalized)
        
        # 5. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # 6. ì•ë’¤ ê³µë°± ì œê±°
        normalized = normalized.strip()
        
        return normalized
    
    def _is_relevant_strict(
        self,
        question: str,
        doc: Any,
        context: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        ì—„ê²©í•œ Relevance ê²€ì¦ (v7.8.1)
        
        Phase 2ëŠ” "ì¬ì‚¬ìš©"ì´ ëª©ì ì´ë¯€ë¡œ
        í•µì‹¬ í‚¤ì›Œë“œê°€ ê±°ì˜ ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•´ì•¼ í•¨
        
        ê²€ì¦ í•­ëª©:
        1. í•µì‹¬ ëª…ì‚¬ ì™„ì „ ì¼ì¹˜ (í•„ìˆ˜)
        2. ë„ë©”ì¸ í‚¤ì›Œë“œ ì¼ì¹˜
        3. ë‹¨ìœ„ í˜¸í™˜ì„±
        4. ë¹„í˜¸í™˜ ì¡°í•© ì°¨ë‹¨
        
        Returns:
            {
                'is_relevant': bool,
                'reason': str,
                'matched_keywords': List[str],
                'confidence': float
            }
        """
        metadata = doc.metadata
        doc_content = doc.page_content.lower()
        question_lower = question.lower()
        
        data_point = metadata.get('data_point', '').lower()
        category = metadata.get('category', '').lower()
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ (ì—„ê²©)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        question_nouns = self._extract_core_nouns_strict(question_lower)
        data_nouns = self._extract_core_nouns_strict(data_point)
        
        if not question_nouns:
            return {
                'is_relevant': False,
                'reason': 'ì§ˆë¬¸ì—ì„œ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨',
                'matched_keywords': [],
                'confidence': 0.0
            }
        
        # êµì§‘í•© ê³„ì‚°
        matched_nouns = set(question_nouns) & set(data_nouns)
        match_ratio = len(matched_nouns) / len(question_nouns) if question_nouns else 0
        
        # â­ í•µì‹¬: ìµœì†Œ 60% ì´ìƒ ë§¤ì¹­ í•„ìš”
        if match_ratio < 0.6:
            return {
                'is_relevant': False,
                'reason': f'í•µì‹¬ ëª…ì‚¬ ì¼ì¹˜ìœ¨ ë‚®ìŒ ({match_ratio:.0%}): ì§ˆë¬¸ {question_nouns} vs ë°ì´í„° {data_nouns}',
                'matched_keywords': list(matched_nouns),
                'confidence': match_ratio
            }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. ë¹„í˜¸í™˜ ì¡°í•© ì°¨ë‹¨
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        INCOMPATIBLE_PAIRS = [
            # (ì§ˆë¬¸ í‚¤ì›Œë“œ, ë°ì´í„° í‚¤ì›Œë“œ) = ë¹„í˜¸í™˜
            (['ì–‘ì', 'ì–‘ìì»´í“¨í„°'], ['ê°€êµ¬', 'ì¸êµ¬']),
            (['ë©”íƒ€ë²„ìŠ¤', 'ê°€ìƒ'], ['ì´íƒˆë¥ ', 'ë¹„ìœ¨']),
            (['í™”ì„±', 'ìš°ì£¼', 'ì‹ë¯¼ì§€'], ['ì¸êµ¬', 'ì„œìš¸']),
            (['ai', 'ì—ì´ì „íŠ¸', 'ì¸ê³µì§€ëŠ¥'], ['ìŒì•…', 'ìŠ¤íŠ¸ë¦¬ë°']),
            (['ë“œë¡ ', 'ë°°ì†¡'], ['ì¸êµ¬']),
            (['ìˆ˜ì§ë†ì¥', 'ë†ì¥'], ['ë‹´ë°°', 'í¡ì—°']),
            (['ë¸”ë¡ì²´ì¸', 'ì•”í˜¸í™”í'], ['ìƒ´í‘¸', 'ë‹´ë°°']),
            
            # ê¸°ì¡´
            (['ì‹œì¥', 'ê·œëª¨'], ['gdp', 'êµ­ë‚´ì´ìƒì‚°']),
            (['ìˆ˜ì—…ë£Œ', 'í•™ì›'], ['ìµœì €ì„ê¸ˆ']),
            (['ìŒì‹ì ', 'ì¹´í˜'], ['ì¸êµ¬í†µê³„']),
        ]
        
        for q_keywords, d_keywords in INCOMPATIBLE_PAIRS:
            has_q = any(kw in question_lower for kw in q_keywords)
            has_d = any(kw in data_point or kw in category or kw in doc_content for kw in d_keywords)
            
            if has_q and has_d:
                return {
                    'is_relevant': False,
                    'reason': f'ë¹„í˜¸í™˜ ì¡°í•©: ì§ˆë¬¸({q_keywords}) vs ë°ì´í„°({d_keywords})',
                    'matched_keywords': [],
                    'confidence': 0.0
                }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 3. ë‹¨ìœ„ í˜¸í™˜ì„±
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        unit_compatible = self._check_unit_compatibility(question, metadata.get('unit', ''))
        
        if not unit_compatible:
            return {
                'is_relevant': False,
                'reason': f'ë‹¨ìœ„ ë¹„í˜¸í™˜: ì§ˆë¬¸ vs ë°ì´í„° {metadata.get("unit")}',
                'matched_keywords': list(matched_nouns),
                'confidence': match_ratio * 0.5
            }
        
        # í†µê³¼!
        return {
            'is_relevant': True,
            'reason': 'OK',
            'matched_keywords': list(matched_nouns),
            'confidence': match_ratio
        }
    
    def _extract_core_nouns_strict(self, text: str) -> List[str]:
        """
        í•µì‹¬ ëª…ì‚¬ ì—„ê²© ì¶”ì¶œ (v7.8.1)
        
        ëª©ì : ê±°ì˜ ì™„ë²½í•œ ë§¤ì¹­ íŒë‹¨ìš©
        """
        # ì£¼ìš” ëª…ì‚¬ ì‚¬ì „
        NOUN_DICT = {
            # ì¸êµ¬/ê°€êµ¬
            'ì¸êµ¬', 'ì´ì¸êµ¬', 'ì„œìš¸', 'ì„±ì¸',
            'ê°€êµ¬', 'ê°€êµ¬ìˆ˜',
            
            # ê²½ì œ
            'gdp', 'êµ­ë‚´ì´ìƒì‚°', 'ì†Œë“',
            
            # ì‚°ì—…
            'ë‹´ë°°', 'ìƒ´í‘¸', 'ìŒì‹ì ', 'ì¹´í˜',
            'íŒë§¤', 'íŒë§¤ëŸ‰', 'ì†Œë¹„', 'ì†Œë¹„ëŸ‰',
            
            # SaaS
            'saas', 'ì´íƒˆ', 'ì´íƒˆë¥ ', 'churn',
            'ltv', 'cac', 'ì „í™˜ìœ¨',
            
            # ì‹œì¥
            'ì‹œì¥', 'ê·œëª¨', 'ìŒì•…', 'ìŠ¤íŠ¸ë¦¬ë°',
            
            # ê¸°íƒ€
            'í¡ì—°', 'í¡ì—°ìœ¨', 'ìµœì €ì„ê¸ˆ',
            
            # ë¯¸ë˜/ê°€ìƒ
            'ì–‘ì', 'ì–‘ìì»´í“¨í„°', 'ë©”íƒ€ë²„ìŠ¤', 'í™”ì„±',
            'ì‹ë¯¼ì§€', 'ai', 'ì—ì´ì „íŠ¸', 'ë“œë¡ ',
            'ìˆ˜ì§ë†ì¥', 'ë¸”ë¡ì²´ì¸',
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ ì¶”ì¶œ
        found_nouns = []
        for noun in NOUN_DICT:
            if noun in text:
                found_nouns.append(noun)
        
        return found_nouns
    
    def _check_unit_compatibility(self, question: str, data_unit: str) -> bool:
        """
        ë‹¨ìœ„ í˜¸í™˜ì„± ì²´í¬ (v7.8.1)
        
        ì§ˆë¬¸ê³¼ ë°ì´í„° ë‹¨ìœ„ê°€ í•©ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€
        """
        if not data_unit:
            return True  # ë‹¨ìœ„ ì—†ìœ¼ë©´ í†µê³¼
        
        question_lower = question.lower()
        data_unit_lower = data_unit.lower()
        
        # ë‹¨ìœ„ ê·¸ë£¹
        COMPATIBLE_GROUPS = [
            # ì¸êµ¬ ê´€ë ¨
            {'ëª…', 'people', 'ì¸êµ¬', 'ê°€êµ¬'},
            
            # ëˆ ê´€ë ¨
            {'ì›', 'usd', 'won', 'ë‹¬ëŸ¬'},
            
            # ìˆ˜ëŸ‰ ê´€ë ¨
            {'ê°œ', 'ê°‘', 'ì”', 'kg', 'ë¦¬í„°'},
            
            # ë¹„ìœ¨ ê´€ë ¨
            {'ë¹„ìœ¨', 'ratio', '%', 'í¼ì„¼íŠ¸'},
            
            # ì‹œê°„ ê´€ë ¨
            {'ì‹œê°„', 'ì¼', 'ê°œì›”', 'ë…„'},
        ]
        
        # ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” ë‹¨ìœ„ ê·¸ë£¹ ì°¾ê¸°
        question_group = None
        for group in COMPATIBLE_GROUPS:
            if any(unit_kw in question_lower for unit_kw in group):
                question_group = group
                break
        
        # ë°ì´í„° ë‹¨ìœ„ ê·¸ë£¹ ì°¾ê¸°
        data_group = None
        for group in COMPATIBLE_GROUPS:
            if any(unit_kw in data_unit_lower for unit_kw in group):
                data_group = group
                break
        
        # ë‘˜ ë‹¤ ê·¸ë£¹ì´ ìˆìœ¼ë©´ ê°™ì€ ê·¸ë£¹ì´ì–´ì•¼ í•¨
        if question_group and data_group:
            return question_group == data_group
        
        # ê·¸ë£¹ ì—†ìœ¼ë©´ í†µê³¼
        return True
    
    def _is_relevant(
        self,
        question: str,
        doc: Any,
        context: Optional[Any] = None
    ) -> bool:
        """
        Relevance ê²€ì¦ (v7.6.1, deprecated)
        
        âš ï¸ v7.8.1ë¶€í„° _is_relevant_strict ì‚¬ìš©
        
        ìœ ì‚¬ë„ê°€ ë†’ì•„ë„ ì‹¤ì œë¡œ ê´€ë ¨ ì—†ëŠ” ë°ì´í„° í•„í„°ë§
        ì˜ˆ: "ì‹œì¥ ê·œëª¨" â†’ GDP (X)
        
        ê²€ì¦ í•­ëª©:
        1. ë¹„í˜¸í™˜ ì¡°í•© ì²´í¬ (ì‹œì¥â‰ GDP ë“±)
        2. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
        3. Scale ê²€ì¦
        """
        metadata = doc.metadata
        doc_content = doc.page_content.lower()
        question_lower = question.lower()
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. ë¹„í˜¸í™˜ ì¡°í•© ì²´í¬
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        INCOMPATIBLE_PAIRS = [
            # (ì§ˆë¬¸ í‚¤ì›Œë“œ, ë°ì´í„° ì¹´í…Œê³ ë¦¬) = ë¹„í˜¸í™˜
            (['ì‹œì¥', 'ê·œëª¨'], ['gdp', 'êµ­ë‚´ì´ìƒì‚°']),
            (['ìˆ˜ì—…ë£Œ', 'í•™ì›'], ['ìµœì €ì„ê¸ˆ', 'ë²•ì •']),
            (['ìŒì‹ì ', 'ì¹´í˜'], ['ì¸êµ¬í†µê³„']),
            (['íŒë§¤ëŸ‰', 'ì†Œë¹„'], ['ì¸êµ¬', 'ê°€êµ¬']),
        ]
        
        data_point = metadata.get('data_point', '').lower()
        category = metadata.get('category', '').lower()
        
        for q_keywords, d_keywords in INCOMPATIBLE_PAIRS:
            # ì§ˆë¬¸ì— í‚¤ì›Œë“œ ìˆê³ 
            has_q = any(kw in question_lower for kw in q_keywords)
            # ë°ì´í„°ì— ë¹„í˜¸í™˜ í‚¤ì›Œë“œ ìˆìœ¼ë©´
            has_d = any(kw in data_point or kw in category or kw in doc_content for kw in d_keywords)
            
            if has_q and has_d:
                logger.info(f"    ë¹„í˜¸í™˜: {q_keywords} vs {d_keywords}")
                return False
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. í•µì‹¬ í‚¤ì›Œë“œ í•„ìˆ˜ ë§¤ì¹­
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì§ˆë¬¸ì˜ í•µì‹¬ ëª…ì‚¬ ì¶”ì¶œ
        core_keywords = self._extract_core_keywords(question_lower)
        
        if core_keywords:
            # í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ ìµœì†Œ 1ê°œëŠ” ìˆì–´ì•¼
            matched = any(kw in doc_content for kw in core_keywords)
            
            if not matched:
                logger.info(f"    í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜: {core_keywords}")
                return False
        
        # í†µê³¼
        logger.info(f"    âœ… Relevance ê²€ì¦ í†µê³¼")
        return True
    
    def _extract_core_keywords(self, question: str) -> list:
        """ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        
        # ì£¼ìš” ëª…ì‚¬ í‚¤ì›Œë“œ ë§¤í•‘
        keyword_map = {
            'ë‹´ë°°': ['ë‹´ë°°', 'í¡ì—°'],
            'ìŒì•…': ['ìŒì•…', 'ìŒì›'],
            'ìŠ¤íŠ¸ë¦¬ë°': ['ìŠ¤íŠ¸ë¦¬ë°', 'êµ¬ë…'],
            'ìŒì‹ì ': ['ìŒì‹ì ', 'ì‹ë‹¹', 'ë ˆìŠ¤í† ë‘'],
            'ì¹´í˜': ['ì¹´í˜', 'ì»¤í”¼'],
            'í•™ì›': ['í•™ì›', 'êµìœ¡'],
            'ìˆ˜ì—…ë£Œ': ['ìˆ˜ì—…ë£Œ', 'í•™ë¹„'],
        }
        
        keywords = []
        for key, variants in keyword_map.items():
            if any(v in question for v in variants):
                keywords.extend(variants)
        
        return keywords
    
    def _convert_unit_if_needed(
        self,
        question: str,
        result_data: dict,
        doc: Any
    ) -> Optional[dict]:
        """
        ë‹¨ìœ„ ë³€í™˜ (v7.6.1)
        
        ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ë¥¼ ì¶”ì¶œí•˜ê³ 
        í•„ìš” ì‹œ ìë™ ë³€í™˜
        
        ì˜ˆ: "í•˜ë£¨ì— íŒë§¤ë˜ëŠ”" â†’ ê°‘/ì¼ í•„ìš”
            ë°ì´í„°: 32,000,000,000 ê°‘/ë…„
            ë³€í™˜: 32,000,000,000 / 365 = 87,671,233 ê°‘/ì¼
        """
        current_unit = result_data.get('unit', '')
        
        # ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ ì¶”ì¶œ
        requested_unit = self._extract_requested_unit(question)
        
        if not requested_unit or not current_unit:
            return None
        
        # ë‹¨ìœ„ ë³€í™˜ í•„ìš” ì—¬ë¶€
        if current_unit == requested_unit:
            return None  # ë³€í™˜ ë¶ˆí•„ìš”
        
        # ë³€í™˜ ê·œì¹™
        CONVERSIONS = {
            ('ê°‘/ë…„', 'ê°‘/ì¼'): ('divide', 365),
            ('ì›/ë…„', 'ì›/ì›”'): ('divide', 12),
            ('ê°œ/ë…„', 'ê°œ/ì¼'): ('divide', 365),
            
            ('ê°‘/ì¼', 'ê°‘/ë…„'): ('multiply', 365),
            ('ì›/ì›”', 'ì›/ë…„'): ('multiply', 12),
        }
        
        conversion_key = (current_unit, requested_unit)
        
        if conversion_key in CONVERSIONS:
            operation, factor = CONVERSIONS[conversion_key]
            
            original_value = result_data['value']
            
            if operation == 'divide':
                converted_value = original_value / factor
            else:  # multiply
                converted_value = original_value * factor
            
            logger.info(f"  ğŸ”„ ë‹¨ìœ„ ë³€í™˜: {original_value:,.0f} {current_unit} â†’ {converted_value:,.0f} {requested_unit}")
            
            # ë³€í™˜ëœ ê²°ê³¼ ë°˜í™˜
            converted_data = result_data.copy()
            converted_data['value'] = converted_value
            converted_data['unit'] = requested_unit
            converted_data['original_value'] = original_value
            converted_data['original_unit'] = current_unit
            converted_data['conversion_applied'] = True
            converted_data['conversion_formula'] = f"{operation} {factor}"
            
            return converted_data
        
        # ë³€í™˜ ê·œì¹™ ì—†ìŒ
        return None
    
    def _extract_requested_unit(self, question: str) -> Optional[str]:
        """
        ì§ˆë¬¸ì—ì„œ ìš”ì²­ ë‹¨ìœ„ ì¶”ì¶œ
        
        ì˜ˆ: "í•˜ë£¨ì— íŒë§¤ë˜ëŠ”" â†’ "ê°‘/ì¼"
            "ì—°ê°„ íŒë§¤ëŸ‰ì€" â†’ "ê°‘/ë…„"
            "ì›”í‰ê·  ë§¤ì¶œì€" â†’ "ì›/ì›”"
        """
        question_lower = question.lower()
        
        # ì‹œê°„ ë‹¨ìœ„
        if 'í•˜ë£¨' in question or 'ì¼ì¼' in question or 'ë§¤ì¼' in question:
            if 'ê°‘' in question:
                return 'ê°‘/ì¼'
            elif 'ê°œ' in question:
                return 'ê°œ/ì¼'
            else:
                return 'ì¼'
        
        if 'ì—°ê°„' in question or 'ë…„ê°„' in question or '1ë…„' in question:
            if 'ê°‘' in question:
                return 'ê°‘/ë…„'
            elif 'ì›' in question or 'ë§¤ì¶œ' in question:
                return 'ì›/ë…„'
        
        if 'ì›”' in question or 'í•œ ë‹¬' in question:
            if 'ì›' in question or 'ë§¤ì¶œ' in question:
                return 'ì›/ì›”'
        
        return None
    
    def validate_with_rag(
        self,
        data_point: str,
        claimed_value: Any,
        source_hint: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        RAG ê¸°ë°˜ ë°ì´í„° ê²€ì¦
        
        í”„ë¡œì„¸ìŠ¤:
        ---------
        1. ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ â†’ ì–´ë””ì„œ êµ¬í• ì§€
        2. ì •ì˜ ì‚¬ë¡€ ê²€ìƒ‰ â†’ ì •ì˜ í™•ì¸ ë°©ë²•
        3. ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸
        
        Returns:
        --------
        ê²€ì¦ ê²°ê³¼ + ì¶”ì²œ ì†ŒìŠ¤ + ì •ì˜ ì£¼ì˜ì‚¬í•­
        """
        logger.info(f"[Validator] RAG ê¸°ë°˜ ê²€ì¦: {data_point}")
        
        result = {
            'data_point': data_point,
            'value': claimed_value,
            'recommended_sources': [],
            'definition_warnings': [],
            'validation_status': 'pending'
        }
        
        # 1. ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
        sources = self.search_data_source(data_point, top_k=3)
        if sources:
            result['recommended_sources'] = [
                {
                    'name': doc.metadata.get('source_name'),
                    'reliability': doc.metadata.get('reliability'),
                    'access': doc.metadata.get('access_method'),
                    'confidence': score
                }
                for doc, score in sources
            ]
        
        # 2. ì •ì˜ ê²€ì¦ ì‚¬ë¡€
        definitions = self.search_definition_case(data_point, top_k=2)
        if definitions:
            result['definition_warnings'] = [
                {
                    'case': doc.metadata.get('term'),
                    'gap': doc.metadata.get('gap_description'),
                    'adjustment': doc.metadata.get('adjustment_method')
                }
                for doc, score in definitions
            ]
        
        # 3. ê²€ì¦ ìƒíƒœ
        if sources and sources[0][1] > 0.8:  # ë†’ì€ ìœ ì‚¬ë„
            result['validation_status'] = 'recommended'
        elif sources:
            result['validation_status'] = 'caution'
        else:
            result['validation_status'] = 'no_source_found'
        
        return result

    def load_kpi_library(self) -> Dict:
        """
        KPI ì •ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
        
        Returns:
            KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ dict
        """
        import yaml
        
        kpi_path = Path("data/raw/kpi_definitions.yaml")
        
        if not kpi_path.exists():
            logger.warning(f"KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ì—†ìŒ: {kpi_path}")
            return {}
        
        with open(kpi_path, 'r', encoding='utf-8') as f:
            library = yaml.safe_load(f)
        
        total = library.get('_meta', {}).get('total_kpis', 0)
        logger.info(f"âœ… KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ: {total}ê°œ")
        
        return library
    
    def validate_kpi_definition(
        self,
        metric_name: str,
        provided_definition: Dict
    ) -> Dict[str, Any]:
        """
        KPI ì •ì˜ ê²€ì¦ (s10 Industry KPI Library)
        
        Args:
            metric_name: KPI ì´ë¦„
            provided_definition: {
                'numerator': str,
                'denominator': str,
                'unit': str,
                'scope': {
                    'includes': [...],
                    'excludes': [...]
                }
            }
        
        Returns:
            {
                'status': 'match' | 'partial_match' | 'mismatch' | 'not_found',
                'kpi_id': str,
                'standard_definition': {...},
                'gaps': [...],
                'recommendation': str,
                'comparability_score': float (0-1)
            }
        """
        
        logger.info(f"[Validator] KPI ì •ì˜ ê²€ì¦: {metric_name}")
        
        # KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
        library = self.load_kpi_library()
        
        if not library:
            return {
                'status': 'library_not_found',
                'message': 'KPI ë¼ì´ë¸ŒëŸ¬ë¦¬ íŒŒì¼ ì—†ìŒ',
                'recommendation': 'scripts/build_kpi_library.py ì‹¤í–‰ í•„ìš”'
            }
        
        # KPI ê²€ìƒ‰
        kpi = self._search_kpi(metric_name, library)
        
        if not kpi:
            return {
                'status': 'not_found',
                'message': f"KPI '{metric_name}'ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì—†ìŠµë‹ˆë‹¤",
                'recommendation': 'manual_review',
                'create_new': True
            }
        
        logger.info(f"  âœ… KPI ë°œê²¬: {kpi['kpi_id']}")
        
        # ì •ì˜ ë¹„êµ
        gaps = []
        
        # 1. ë¶„ì ë¹„êµ
        provided_numerator = provided_definition.get('numerator', '')
        standard_numerator = kpi.get('formula', {}).get('numerator', '')
        
        if provided_numerator and provided_numerator != standard_numerator:
            gaps.append({
                'field': 'numerator',
                'provided': provided_numerator,
                'standard': standard_numerator,
                'severity': 'high'
            })
            logger.warning(f"  âš ï¸  ë¶„ì ë¶ˆì¼ì¹˜")
        
        # 2. ë¶„ëª¨ ë¹„êµ
        provided_denominator = provided_definition.get('denominator', '')
        standard_denominator = kpi.get('formula', {}).get('denominator', '')
        
        if provided_denominator and standard_denominator != 'N/A' and provided_denominator != standard_denominator:
            gaps.append({
                'field': 'denominator',
                'provided': provided_denominator,
                'standard': standard_denominator,
                'severity': 'high'
            })
            logger.warning(f"  âš ï¸  ë¶„ëª¨ ë¶ˆì¼ì¹˜")
        
        # 3. ë‹¨ìœ„ ë¹„êµ
        provided_unit = provided_definition.get('unit', '')
        standard_unit = kpi.get('unit', '')
        
        if provided_unit and provided_unit != standard_unit:
            gaps.append({
                'field': 'unit',
                'provided': provided_unit,
                'standard': standard_unit,
                'severity': 'medium'
            })
            logger.warning(f"  âš ï¸  ë‹¨ìœ„ ë¶ˆì¼ì¹˜")
        
        # 4. Scope ë¹„êµ
        scope_gaps = self._compare_scope(
            provided_definition.get('scope', {}),
            kpi.get('scope', {})
        )
        gaps.extend(scope_gaps)
        
        # ìƒíƒœ ê²°ì •
        if len(gaps) == 0:
            status = 'match'
            logger.info(f"  âœ… ì™„ì „ ì¼ì¹˜")
        elif any(g['severity'] == 'high' for g in gaps):
            status = 'mismatch'
            logger.warning(f"  âŒ ë¶ˆì¼ì¹˜ (high severity)")
        else:
            status = 'partial_match'
            logger.info(f"  âš ï¸  ë¶€ë¶„ ì¼ì¹˜")
        
        # ë¹„êµ ê°€ëŠ¥ì„± ì ìˆ˜
        comparability_score = 1.0 - (len(gaps) * 0.2)
        comparability_score = max(0, comparability_score)
        
        # ê¶Œê³ ì‚¬í•­
        if status == 'match':
            recommendation = 'âœ… í‘œì¤€ ì •ì˜ì™€ ì¼ì¹˜. ë¹„êµ ê°€ëŠ¥'
        elif status == 'mismatch':
            recommendation = 'âŒ ì •ì˜ ë¶ˆì¼ì¹˜. ë¹„êµ ë¶ˆê°€ â†’ í‘œì¤€í™” í•„ìš”'
        else:
            recommendation = 'âš ï¸  ë¶€ë¶„ ì¼ì¹˜. ì£¼ì˜í•˜ì—¬ ë¹„êµ'
        
        logger.info(f"  ë¹„êµ ê°€ëŠ¥ì„±: {comparability_score*100:.0f}%")
        
        return {
            'status': status,
            'kpi_id': kpi['kpi_id'],
            'standard_definition': kpi,
            'gaps': gaps,
            'recommendation': recommendation,
            'comparability_score': comparability_score
        }
    
    def _search_kpi(self, metric_name: str, library: Dict) -> Optional[Dict]:
        """KPI ê²€ìƒ‰"""
        
        metric_lower = metric_name.lower()
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        for key in library:
            if key.endswith('_kpis'):
                for kpi in library[key]:
                    kpi_name_lower = kpi['metric_name'].lower()
                    
                    # ì •í™•í•œ ë§¤ì¹­
                    if kpi_name_lower == metric_lower:
                        return kpi
                    
                    # ë¶€ë¶„ ë§¤ì¹­
                    if metric_lower in kpi_name_lower or kpi_name_lower in metric_lower:
                        return kpi
        
        return None
    
    def _compare_scope(
        self,
        provided_scope: Dict,
        standard_scope: Dict
    ) -> List[Dict]:
        """Scope ë¹„êµ"""
        
        gaps = []
        
        # Includes ë¹„êµ
        provided_includes = set(provided_scope.get('includes', []))
        standard_includes = set(standard_scope.get('includes', []))
        
        missing_includes = standard_includes - provided_includes
        extra_includes = provided_includes - standard_includes
        
        if missing_includes:
            gaps.append({
                'field': 'scope.includes',
                'provided': list(provided_includes),
                'standard': list(standard_includes),
                'missing': list(missing_includes),
                'severity': 'medium'
            })
        
        # Excludes ë¹„êµ
        provided_excludes = set(provided_scope.get('excludes', []))
        standard_excludes = set(standard_scope.get('excludes', []))
        
        missing_excludes = standard_excludes - provided_excludes
        
        if missing_excludes:
            gaps.append({
                'field': 'scope.excludes',
                'provided': list(provided_excludes),
                'standard': list(standard_excludes),
                'missing': list(missing_excludes),
                'severity': 'high'  # ì œì™¸ í•­ëª© ì¤‘ìš”
            })
        
        return gaps


    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # v7.7.0: Estimator êµì°¨ ê²€ì¦ (5-Phase)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def validate_estimation(
        self,
        question: str,
        claimed_value: float,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        ì¶”ì •ê°’ì˜ í•©ë¦¬ì„± ê²€ì¦ (Estimator 5-Phase êµì°¨ ê²€ì¦)
        
        ì›ì¹™ (v7.7.0):
        -------------
        1. ì§ì ‘ ì¶”ì • ê¸ˆì§€ âŒ
        2. Estimatorì—ê²Œ êµì°¨ ê²€ì¦ ìš”ì²­ âœ… (Phase 0â†’1â†’2â†’3â†’4 ìë™)
        3. ë¹„êµ ë° íŒë‹¨
        
        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "B2B SaaS Churn RateëŠ”?")
            claimed_value: ì£¼ì¥ëœ ê°’ (ì˜ˆ: 0.08)
            context: ë§¥ë½ (domain, region ë“±)
        
        Returns:
            {
                'claimed_value': 0.08,
                'estimator_value': 0.06,
                'estimator_phase': 2,  # v7.7.0: Phase 0-4
                'estimator_confidence': 0.85,
                'estimator_reasoning': {...},
                'difference_pct': 0.33,
                'validation_result': 'caution'
            }
        
        Example:
            >>> validator = ValidatorRAG()
            >>> result = validator.validate_estimation(
            ...     "B2B SaaS Churn RateëŠ”?",
            ...     claimed_value=0.08,
            ...     context={'domain': 'B2B_SaaS'}
            ... )
            >>> print(result['validation_result'])  # 'caution'
        """
        logger.info(f"[Validator] ì¶”ì •ê°’ ê²€ì¦: {question} = {claimed_value}")
        
        # Estimator Lazy ì´ˆê¸°í™”
        if self.estimator is None:
            self.estimator = get_estimator_rag()
            logger.info("  âœ… Estimator ì—°ê²° (5-Phase)")
        
        # Estimatorì—ê²Œ êµì°¨ ê²€ì¦ ìš”ì²­ (Phase 0â†’1â†’2â†’3â†’4 ìë™ ì‹œë„)
        est_result = self.estimator.estimate(
            question=question,
            domain=context.get('domain') if context else None,
            region=context.get('region') if context else None
        )
        
        if not est_result:
            return {
                'validation': 'unable',
                'reason': 'Estimator ì¶”ì • ì‹¤íŒ¨'
            }
        
        # ë¹„êµ
        diff_pct = abs(claimed_value - est_result.value) / est_result.value if est_result.value else 0
        
        validation = {
            'claimed_value': claimed_value,
            'estimator_value': est_result.value,
            'estimator_confidence': est_result.confidence,
            'estimator_phase': est_result.phase,  # v7.7.0: tier â†’ phase
            
            # v7.7.0: ìƒì„¸ ê·¼ê±° í¬í•¨
            'estimator_reasoning': est_result.reasoning_detail,
            'estimator_components': est_result.component_estimations,
            'estimator_trace': est_result.estimation_trace,
            
            'difference_pct': diff_pct,
            
            'validation_result': (
                'pass' if diff_pct < 0.30 else
                'caution' if diff_pct < 0.50 else
                'fail'
            ),
            
            'recommendation': self._generate_recommendation(
                claimed_value, est_result, diff_pct
            )
        }
        
        logger.info(f"  ê²€ì¦: {validation['validation_result']} (ì°¨ì´ {diff_pct:.0%})")
        
        return validation
    
    def _generate_recommendation(
        self,
        claimed: float,
        est_result,
        diff_pct: float
    ) -> str:
        """ê²€ì¦ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­"""
        
        lines = []
        lines.append(f"ì£¼ì¥ê°’: {claimed}")
        lines.append(f"Estimator ì¶”ì •: {est_result.value} (Phase {est_result.phase}, ì‹ ë¢°ë„ {est_result.confidence:.0%})")
        lines.append(f"ì°¨ì´: {diff_pct:.0%}")
        lines.append(f"")
        
        if diff_pct < 0.30:
            lines.append("âœ… ê²€ì¦ í†µê³¼: í•©ë¦¬ì  ë²”ìœ„")
        elif diff_pct < 0.50:
            lines.append("âš ï¸  ì£¼ì˜: ì°¨ì´ê°€ ë‹¤ì†Œ í¼")
            lines.append(f"Estimator ê·¼ê±° í™•ì¸ ê¶Œì¥:")
            if est_result.reasoning_detail:
                lines.append(f"  - ì „ëµ: {est_result.reasoning_detail.get('method')}")
                lines.append(f"  - ì¦ê±°: {est_result.reasoning_detail.get('evidence_count')}ê°œ")
        else:
            lines.append("âŒ ê²€ì¦ ì‹¤íŒ¨: ì°¨ì´ê°€ ë§¤ìš° í¼")
            lines.append(f"Estimator ì¶”ì • ì¬ê²€í†  í•„ìš”")
        
        return "\n".join(lines)


    def search_historical_data(
        self,
        market: str,
        years: range
    ) -> Dict[str, Any]:
        """
        ê³¼ê±° ë°ì´í„° íƒìƒ‰ ë° ìˆ˜ì§‘ (v7.8.0 ì‹ ê·œ)
        
        Observer Timeline ë¶„ì„ì„ ìœ„í•œ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘.
        Estimator í˜‘ì—…ìœ¼ë¡œ ëˆ„ë½ ë°ì´í„° ì¶”ì •.
        
        Args:
            market: ì‹œì¥ ì´ë¦„
            years: range(2015, 2026) â†’ 2015-2025
        
        Returns:
            {
                'market_size_by_year': {year: {value, source, reliability}, ...},
                'players_by_year': {year: {player: {share, source}, ...}, ...},
                'events': [Event, ...],
                'hhi_by_year': {year: hhi, ...},
                'player_count_by_year': {year: count, ...},
                'data_quality': {verified_ratio, avg_confidence, ...}
            }
        """
        logger.info(f"[Validator] ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘: {market} ({years.start}-{years.stop-1})")
        
        result = {
            'market_size_by_year': {},
            'players_by_year': {},
            'events': [],
            'hhi_by_year': {},
            'player_count_by_year': {},
            'data_gaps': {'missing_years': [], 'estimator_requests': []}
        }
        
        # Step 1: ê³µì‹ í†µê³„ ê²€ìƒ‰
        logger.info("  Step 1: ê³µì‹ í†µê³„ ê²€ìƒ‰")
        official_data = self._search_official_statistics(market, years)
        result['market_size_by_year'].update(official_data.get('market_size', {}))
        
        # Step 2: ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG)
        logger.info("  Step 2: ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG)")
        industry_data = self._search_industry_reports_rag(market, years)
        result['market_size_by_year'].update(industry_data.get('market_size', {}))
        
        # Step 3: ê³µì‹œ ë°ì´í„° (ìƒì¥ì‚¬)
        logger.info("  Step 3: ê³µì‹œ ë°ì´í„° ê²€ìƒ‰")
        public_data = self._search_public_filings(market, years)
        result['players_by_year'].update(public_data.get('players', {}))
        
        # Step 4: ë‰´ìŠ¤/ì‚¬ê±´
        logger.info("  Step 4: ì£¼ìš” ì‚¬ê±´ ê²€ìƒ‰")
        events = self._search_news_events(market, years)
        result['events'] = events
        
        # Step 5: Gap ì‹ë³„
        logger.info("  Step 5: ë°ì´í„° Gap ì‹ë³„")
        gaps = self._identify_data_gaps(result, years)
        result['data_gaps'] = gaps
        
        # Step 6: Estimator í˜‘ì—… (Gap ì±„ìš°ê¸°)
        if gaps['missing_years']:
            logger.info(f"  Step 6: Estimator í˜‘ì—… ({len(gaps['missing_years'])}ê°œ ëˆ„ë½ ì—°ë„)")
            result = self._fill_gaps_with_estimator(result, gaps)
        
        # Step 7: ë°ì´í„° í’ˆì§ˆ í‰ê°€
        result['data_quality'] = self._assess_data_quality(result, years)
        
        logger.info(f"  âœ… ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (í’ˆì§ˆ: {result['data_quality'].get('grade', 'N/A')})")
        
        return result
    
    def _search_official_statistics(self, market: str, years: range) -> Dict:
        """
        ê³µì‹ í†µê³„ ê²€ìƒ‰ (í†µê³„ì²­, í•œêµ­ì€í–‰ ë“±)
        
        Args:
            market: ì‹œì¥ëª…
            years: ì—°ë„ ë²”ìœ„
        
        Returns:
            Dict with market_size data
        
        Note:
            í˜„ì¬ëŠ” KOSIS API ì—°ë™ ì¤€ë¹„ ì¤‘
            ìˆ˜ë™ ìˆ˜ì§‘ ë°ì´í„° ì‚¬ìš© ê¶Œì¥
        """
        
        result = {'market_size': {}}
        
        # KOSIS APIê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´
        if hasattr(self, 'kosis_api_key') and self.kosis_api_key:
            try:
                # KOSIS search_kosis_data() ë©”ì„œë“œ í™œìš©
                kosis_result = self.search_kosis_data(
                    search_term=market,
                    data_type='market_size'
                )
                
                if kosis_result:
                    logger.info(f"    âœ… KOSIS API: ë°ì´í„° ë°œê²¬")
                    result['market_size'] = kosis_result.get('data', {})
                    return result
            except Exception as e:
                logger.warning(f"    âš ï¸ KOSIS API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        
        # Fallback: RAGì—ì„œ í†µê³„ ë°ì´í„° ê²€ìƒ‰
        if self.source_store:
            try:
                query = f"{market} official statistics ì‹œì¥ ê·œëª¨ í†µê³„ì²­"
                search_results = self.source_store.similarity_search(query, k=3)
                
                if search_results:
                    logger.info(f"    âœ… RAG í†µê³„ ì†ŒìŠ¤: {len(search_results)}ê°œ ë°œê²¬")
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    for res in search_results:
                        if hasattr(res, 'metadata') and 'year' in res.metadata:
                            year = res.metadata['year']
                            if 'market_size' in res.metadata:
                                result['market_size'][str(year)] = res.metadata['market_size']
            except Exception as e:
                logger.warning(f"    âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        logger.info("    â„¹ï¸  ëŒ€ì•ˆ: https://kosis.kr ìˆ˜ë™ í™•ì¸")
        return result
    
    def _search_industry_reports_rag(self, market: str, years: range) -> Dict:
        """
        ì‚°ì—… ë¦¬í¬íŠ¸ ê²€ìƒ‰ (RAG í™œìš©)
        
        Args:
            market: ì‹œì¥ëª…
            years: ì—°ë„ ë²”ìœ„
        
        Returns:
            Dict with market_size data extracted from reports
        """
        
        result = {'market_size': {}}
        
        # data_sources_registryì—ì„œ ê²€ìƒ‰
        if self.source_store:
            try:
                query = f"{market} market size historical data {min(years)}-{max(years)}"
                results = self.source_store.similarity_search(query, k=5)
                logger.info(f"    âœ… RAG: {len(results)}ê°œ ì†ŒìŠ¤ ë°œê²¬")
                
                # ê° ê²°ê³¼ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                for res in results:
                    try:
                        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì—°ë„ë³„ ë°ì´í„° ì¶”ì¶œ
                        if hasattr(res, 'metadata'):
                            metadata = res.metadata
                            year = metadata.get('year')
                            market_size = metadata.get('market_size')
                            
                            if year and market_size:
                                year_str = str(year)
                                if year_str not in result['market_size']:
                                    result['market_size'][year_str] = {
                                        'value': market_size,
                                        'unit': metadata.get('unit', 'USD'),
                                        'source': metadata.get('source_name', 'Industry Report'),
                                        'reliability': metadata.get('reliability', 'medium')
                                    }
                        
                        # page_contentì—ì„œ ìˆ«ì ì¶”ì¶œ ì‹œë„
                        if hasattr(res, 'page_content'):
                            # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ (í™•ì¥ ê°€ëŠ¥)
                            import re
                            content = res.page_content
                            # "2023: $100M" ê°™ì€ íŒ¨í„´
                            year_value_pattern = r'(\d{4}):\s*\$?([\d,\.]+)\s*([MB])'
                            matches = re.findall(year_value_pattern, content)
                            
                            for year, value, unit in matches:
                                if int(year) in years:
                                    multiplier = 1_000_000 if unit == 'M' else 1_000_000_000
                                    numeric_value = float(value.replace(',', '')) * multiplier
                                    
                                    if year not in result['market_size']:
                                        result['market_size'][year] = {
                                            'value': numeric_value,
                                            'unit': 'USD',
                                            'source': 'Report extraction',
                                            'reliability': 'medium'
                                        }
                    except Exception as extract_error:
                        logger.debug(f"    ì¶”ì¶œ ì‹¤íŒ¨: {extract_error}")
                        continue
                
            except Exception as e:
                logger.warning(f"    âš ï¸ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return result
    
    def _search_public_filings(self, market: str, years: range) -> Dict:
        """
        ê³µì‹œ ë°ì´í„° ê²€ìƒ‰ (DART API ë“±)
        
        Args:
            market: ì‹œì¥ëª…
            years: ì—°ë„ ë²”ìœ„
        
        Returns:
            Dict with players data from public filings
        """
        
        result = {'players': {}}
        
        # DART API ì—°ë™ (utils.dart_api í™œìš©)
        try:
            if hasattr(self, 'dart_api') and self.dart_api:
                # DART API ê²€ìƒ‰
                # ì‹œì¥ ê´€ë ¨ ì£¼ìš” ê¸°ì—… ì¶”ì¶œ
                from umis_rag.utils.dart_api import DartAPI
                
                dart = DartAPI()
                
                # í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ì—… ê²€ìƒ‰
                companies = dart.search_companies(keyword=market, limit=10)
                
                if companies:
                    logger.info(f"    âœ… DART API: {len(companies)}ê°œ ê¸°ì—… ë°œê²¬")
                    
                    for company in companies:
                        corp_code = company.get('corp_code')
                        corp_name = company.get('corp_name')
                        
                        # ì—°ë„ë³„ ì¬ë¬´ ë°ì´í„° ìˆ˜ì§‘
                        for year in years:
                            try:
                                financial_data = dart.get_financial_statement(
                                    corp_code=corp_code,
                                    year=year
                                )
                                
                                if financial_data:
                                    result['players'][corp_name] = {
                                        'year': year,
                                        'revenue': financial_data.get('revenue'),
                                        'source': 'DART',
                                        'reliability': 'high'
                                    }
                            except Exception:
                                continue
                
                return result
                
        except ImportError:
            logger.debug("    â„¹ï¸  DART API ëª¨ë“ˆ ì—†ìŒ")
        except Exception as e:
            logger.warning(f"    âš ï¸ DART API ì—°ë™ ì‹¤íŒ¨: {e}")
        
        logger.info("    â„¹ï¸  ëŒ€ì•ˆ: https://dart.fss.or.kr ìˆ˜ë™ í™•ì¸")
        return result
    
    def _search_news_events(self, market: str, years: range) -> List[Dict]:
        """
        ë‰´ìŠ¤ì—ì„œ ì£¼ìš” ì‚¬ê±´ ì¶”ì¶œ
        
        Args:
            market: ì‹œì¥ëª…
            years: ì—°ë„ ë²”ìœ„
        
        Returns:
            List of event dicts
        """
        
        events = []
        
        # Web Searchë¥¼ í™œìš©í•œ ë‰´ìŠ¤ ê²€ìƒ‰
        try:
            from duckduckgo_search import DDGS
            
            ddgs = DDGS()
            
            # ì—°ë„ë³„ ì£¼ìš” ì‚¬ê±´ ê²€ìƒ‰
            for year in years:
                query = f"{market} market {year} major events news"
                
                try:
                    results = ddgs.text(query, max_results=5)
                    
                    for res in results:
                        events.append({
                            'year': year,
                            'title': res.get('title', ''),
                            'snippet': res.get('body', ''),
                            'url': res.get('href', ''),
                            'source': 'news_search'
                        })
                    
                    if results:
                        logger.info(f"    âœ… ë‰´ìŠ¤: {year}ë…„ {len(results)}ê°œ ì‚¬ê±´")
                    
                except Exception as search_error:
                    logger.debug(f"    ê²€ìƒ‰ ì‹¤íŒ¨ ({year}): {search_error}")
                    continue
            
        except ImportError:
            logger.info("    â„¹ï¸  duckduckgo_search ë¯¸ì„¤ì¹˜ (pip install duckduckgo-search)")
        except Exception as e:
            logger.warning(f"    âš ï¸ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        logger.info(f"    ì´ {len(events)}ê°œ ì‚¬ê±´ ì¶”ì¶œ")
        return events
    
    def _identify_data_gaps(self, collected_data: Dict, years: range) -> Dict:
        """ë°ì´í„° Gap ì‹ë³„"""
        gaps = {'missing_years': [], 'estimator_requests': []}
        
        # ëˆ„ë½ ì—°ë„ íŒŒì•…
        for year in years:
            if year not in collected_data['market_size_by_year']:
                gaps['missing_years'].append(year)
                
                # Estimator ìš”ì²­ ì¤€ë¹„
                gaps['estimator_requests'].append({
                    'type': 'market_size_interpolation',
                    'year': year,
                    'market': collected_data.get('market'),
                    'known_data': collected_data['market_size_by_year']
                })
        
        logger.info(f"    Gap: {len(gaps['missing_years'])}ê°œ ëˆ„ë½ ì—°ë„")
        return gaps
    
    def _fill_gaps_with_estimator(self, data: Dict, gaps: Dict) -> Dict:
        """
        Estimator í˜‘ì—…ìœ¼ë¡œ Gap ì±„ìš°ê¸°
        
        Args:
            data: ìˆ˜ì§‘ëœ ë°ì´í„°
            gaps: ì‹ë³„ëœ Gap
        
        Returns:
            Gapì´ ì±„ì›Œì§„ ë°ì´í„°
        """
        
        try:
            from umis_rag.agents.estimator import get_estimator_rag
            from umis_rag.agents.estimator.common.estimation_result import Context
            
            estimator = get_estimator_rag()
            
            for request in gaps['estimator_requests']:
                if request['type'] == 'market_size_interpolation':
                    year = request['year']
                    market = request.get('market', 'Unknown')
                    
                    logger.info(f"      ğŸ¤– Estimator: {year}ë…„ ì¶”ì • ìš”ì²­...")
                    
                    # Context ì¤€ë¹„
                    estimation_context = Context(
                        industry=request.get('industry'),
                        time_period=str(year),
                        region=request.get('region', 'Global')
                    )
                    
                    # Estimator í˜¸ì¶œ
                    question = f"What was the {market} market size in {year}?"
                    result = estimator.estimate(
                        question=question,
                        context=estimation_context
                    )
                    
                    if result and hasattr(result, 'value'):
                        # ì¶”ì • ê²°ê³¼ ì €ì¥
                        data['market_size_by_year'][str(year)] = {
                            'value': result.value,
                            'unit': result.unit,
                            'source': 'Estimator',
                            'reliability': 'estimated',
                            'certainty': getattr(result, 'certainty', 'medium')
                        }
                        logger.info(f"      âœ… {year}ë…„: {result.value} {result.unit} (ì¶”ì •)")
                    else:
                        logger.warning(f"      âš ï¸ {year}ë…„ ì¶”ì • ì‹¤íŒ¨")
        
        except ImportError as ie:
            logger.warning(f"    âš ï¸ Estimator import ì‹¤íŒ¨: {ie}")
        except Exception as e:
            logger.warning(f"    âš ï¸ Estimator í˜‘ì—… ì‹¤íŒ¨: {e}")
        
        return data
    
    def _assess_data_quality(self, data: Dict, years: range) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        total_years = len(list(years))
        verified_years = sum(
            1 for y, d in data['market_size_by_year'].items()
            if d.get('reliability') == 'high'
        )
        estimated_years = sum(
            1 for y, d in data['market_size_by_year'].items()
            if d.get('reliability') == 'estimated'
        )
        
        verified_ratio = verified_years / total_years if total_years > 0 else 0
        
        # ë“±ê¸‰ íŒì •
        if verified_ratio >= 0.5:
            grade = 'A (High)'
        elif verified_ratio >= 0.3:
            grade = 'B (Medium)'
        else:
            grade = 'C (Low)'
        
        return {
            'total_years': total_years,
            'verified_years': verified_years,
            'estimated_years': estimated_years,
            'verified_ratio': verified_ratio,
            'grade': grade
        }
    
    # ========================================
    # API ê¸°ë°˜ ë°ì´í„° ê²€ìƒ‰ (v7.9.0)
    # ========================================
    
    def search_dart_company_financials(
        self,
        company_name: str,
        year: int = 2024
    ) -> Optional[Dict]:
        """
        DART APIë¡œ ìƒì¥ì‚¬ ì¬ë¬´ì œí‘œ ê²€ìƒ‰ (v7.9.0)
        
        Args:
            company_name: íšŒì‚¬ëª… (ì˜ˆ: "ìŠ¤íƒ€ë²…ìŠ¤ì½”ë¦¬ì•„")
            year: ì‚¬ì—…ì—°ë„
        
        Returns:
            {
                'value': 0.148,
                'unit': 'ratio',
                'source': 'DART 2024ë…„ ì‚¬ì—…ë³´ê³ ì„œ',
                'reliability': 'verified',
                'company': 'ìŠ¤íƒ€ë²…ìŠ¤ì½”ë¦¬ì•„'
            } or None
        """
        
        if not self.dart_api_key or self.dart_api_key == 'your-dart-api-key-here':
            logger.warning("[Validator] DART API Key ì—†ìŒ (.env ì„¤ì • í•„ìš”)")
            return None
        
        logger.info(f"[Validator] DART API ê²€ìƒ‰: {company_name} ({year})")
        
        try:
            from umis_rag.utils.dart_api import DARTClient
            
            client = DARTClient(self.dart_api_key)
            
            # Step 1: ê¸°ì—… ì½”ë“œ
            corp_code = client.get_corp_code(company_name)
            
            if not corp_code:
                logger.warning(f"  {company_name} ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return None
            
            logger.info(f"  âœ“ corp_code: {corp_code}")
            
            # Step 2: ì¬ë¬´ì œí‘œ ì¡°íšŒ (ê°œë³„ì¬ë¬´ì œí‘œ ìš°ì„ !)
            financials = client.get_financials(corp_code, year, fs_div='OFS')
            
            if not financials:
                logger.warning(f"  ê°œë³„ì¬ë¬´ì œí‘œ(OFS) ì—†ìŒ, ì—°ê²°(CFS) ì‹œë„...")
                financials = client.get_financials(corp_code, year, fs_div='CFS')
                fs_div_used = 'CFS'
            else:
                fs_div_used = 'OFS'
            
            if not financials:
                logger.warning(f"  ì¬ë¬´ì œí‘œ ì—†ìŒ")
                return None
            
            # Step 3: ì£¼ìš” ê³„ì • ì¶”ì¶œ
            revenue = 0
            operating_profit = 0
            cost_of_sales = 0
            sga = 0
            
            for item in financials:
                account = item.get('account_nm', '')
                amount_str = item.get('thstrm_amount', '0')
                
                try:
                    amount = float(amount_str.replace(',', ''))
                except:
                    amount = 0
                
                if 'ë§¤ì¶œì•¡' in account and 'ë§¤ì¶œì›ê°€' not in account:
                    revenue = amount
                elif 'ë§¤ì¶œì›ê°€' in account:
                    cost_of_sales = amount
                elif 'íŒë§¤ë¹„' in account or 'ê´€ë¦¬ë¹„' in account:
                    sga = amount
                elif 'ì˜ì—…ì´ìµ' in account:
                    operating_profit = amount
            
            if revenue > 0:
                opm = operating_profit / revenue
                gross_margin = (revenue - cost_of_sales) / revenue if cost_of_sales > 0 else 0
                
                logger.info(f"  âœ“ {company_name} ì¬ë¬´ ({fs_div_used}, ì–µì›):")
                logger.info(f"    ë§¤ì¶œì•¡: {revenue/100_000_000:,.0f}")
                logger.info(f"    ì˜ì—…ì´ìµë¥ : {opm:.1%}")
                logger.info(f"    ë§¤ì¶œì´ì´ìµë¥ : {gross_margin:.1%}")
                
                return {
                    'value': round(opm, 4),
                    'unit': 'ratio',
                    'source': f'DART {year}ë…„ ì‚¬ì—…ë³´ê³ ì„œ ({fs_div_used})',
                    'reliability': 'verified',
                    'data_type': 'actual',
                    'company': company_name,
                    'year': year,
                    'fs_div': fs_div_used,
                    'revenue_billion': round(revenue / 100000000, 1),
                    'cost_of_sales_billion': round(cost_of_sales / 100000000, 1),
                    'sga_billion': round(sga / 100000000, 1),
                    'operating_profit_billion': round(operating_profit / 100000000, 1),
                    'gross_margin': round(gross_margin, 4),
                    'operating_margin': round(opm, 4),
                    'verification_url': f'https://dart.fss.or.kr/dsaf001/main.do'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"  DART API ì˜¤ë¥˜: {e}")
            return None
    
    def search_kosis_industry_average(
        self,
        industry_name: str,
        ksic_code: str = None
    ) -> Optional[Dict]:
        """
        KOSIS APIë¡œ ì‚°ì—… í‰ê·  ë§ˆì§„ìœ¨ ê²€ìƒ‰ (v7.9.0)
        
        Args:
            industry_name: ì‚°ì—…ëª… (ì˜ˆ: "ìŒì‹ì ì—…")
            ksic_code: KSIC ì½”ë“œ (ì˜ˆ: "56")
        
        Returns:
            {
                'value': 0.089,
                'unit': 'ratio',
                'source': 'í†µê³„ì²­ ê¸°ì—…ê²½ì˜ë¶„ì„ 2024',
                'reliability': 'verified',
                'sample_size': 15234
            } or None
        """
        
        # âš ï¸ KOSIS APIëŠ” êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬ ìˆ˜ë™ ìˆ˜ì§‘ ê¶Œì¥
        logger.info(f"[Validator] KOSIS ê²€ìƒ‰: {industry_name}")
        
        if not self.kosis_api_key or self.kosis_api_key == 'your-kosis-api-key-here':
            logger.warning("[Validator] KOSIS API Key ì—†ìŒ")
            logger.info("  ëŒ€ì•ˆ: https://kosis.kr ìˆ˜ë™ í™•ì¸")
            return None
        
        # KOSIS API íŒŒì‹± ë¡œì§
        try:
            import requests
            
            # KOSIS OpenAPI ì—”ë“œí¬ì¸íŠ¸
            base_url = "https://kosis.kr/openapi/Param/statisticsParameterData.do"
            
            params = {
                'method': 'getList',
                'apiKey': self.kosis_api_key,
                'format': 'json',
                'jsonVD': 'Y',
                'itmId': search_term,  # í†µê³„í‘œ ID (ì‹¤ì œë¡œëŠ” ë§¤í•‘ í•„ìš”)
                'objL1': 'ALL'
            }
            
            response = requests.get(base_url, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # ë°ì´í„° íŒŒì‹± (KOSIS ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ)
                if isinstance(data, list) and len(data) > 0:
                    parsed_data = {}
                    
                    for item in data:
                        # ì—°ë„ì™€ ê°’ ì¶”ì¶œ
                        year = item.get('PRD_DE')  # ì‹œì 
                        value = item.get('DT')  # ë°ì´í„°ê°’
                        
                        if year and value:
                            try:
                                parsed_data[year] = float(value.replace(',', ''))
                            except ValueError:
                                continue
                    
                    logger.info(f"  âœ… KOSIS API: {len(parsed_data)}ê°œ ë°ì´í„° í¬ì¸íŠ¸")
                    return {
                        'data': parsed_data,
                        'source': 'KOSIS',
                        'reliability': 'high'
                    }
            else:
                logger.warning(f"  âš ï¸ KOSIS API ì‘ë‹µ ì‹¤íŒ¨: {response.status_code}")
                
        except ImportError:
            logger.warning("  âš ï¸ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš” (pip install requests)")
        except Exception as e:
            logger.warning(f"  âš ï¸ KOSIS API íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        logger.info("  â„¹ï¸  ëŒ€ì•ˆ: https://kosis.kr ìˆ˜ë™ ìˆ˜ì§‘ ê¶Œì¥")
        return None
    
    def search_api_sources(
        self,
        query: str,
        company_name: str = None,
        industry: str = None
    ) -> Optional[Dict]:
        """
        API ë°ì´í„° ì†ŒìŠ¤ í†µí•© ê²€ìƒ‰ (v7.9.0)
        
        DARTì™€ KOSISë¥¼ ìë™ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ í™•ì • ë°ì´í„° ë°˜í™˜
        
        Args:
            query: ê²€ìƒ‰ ì§ˆë¬¸
            company_name: íšŒì‚¬ëª… (DART ê²€ìƒ‰ìš©)
            industry: ì‚°ì—…ëª… (KOSIS ê²€ìƒ‰ìš©)
        
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” None
        """
        
        logger.info(f"[Validator] API í†µí•© ê²€ìƒ‰: {query}")
        
        # DART ê²€ìƒ‰ (íšŒì‚¬ëª… ìˆì„ ë•Œ)
        if company_name:
            logger.info(f"  DART ê²€ìƒ‰ ì‹œë„: {company_name}")
            result = self.search_dart_company_financials(company_name)
            if result:
                logger.info(f"  âœ“ DARTì—ì„œ ë°œê²¬!")
                return result
        
        # KOSIS ê²€ìƒ‰ (ì‚°ì—…ëª… ìˆì„ ë•Œ)
        if industry:
            logger.info(f"  KOSIS ê²€ìƒ‰ ì‹œë„: {industry}")
            result = self.search_kosis_industry_average(industry)
            if result:
                logger.info(f"  âœ“ KOSISì—ì„œ ë°œê²¬!")
                return result
        
        logger.info("  API ì†ŒìŠ¤ì—ì„œ ì°¾ì§€ ëª»í•¨")
        return None


# Validator RAG ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_validator_rag_instance = None

def get_validator_rag() -> ValidatorRAG:
    """Validator RAG ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _validator_rag_instance
    if _validator_rag_instance is None:
        _validator_rag_instance = ValidatorRAG()
    return _validator_rag_instance

