"""
Phase 4: Fermi Decomposition (v7.7.0)

ì¬ê·€ ë¶„í•´ ì¶”ì • - ë…¼ë¦¬ì˜ í¼ì¦ ë§ì¶”ê¸° (Step 1-4)

ì„¤ê³„: config/fermi_model_search.yaml (1,500ì¤„)
ì›ë¦¬: ê°€ìš© ë°ì´í„°(Bottom-up) + ê°œë… ë¶„í•´(Top-down) ë°˜ë³µ

v7.7.0 íŒŒì¼ëª… ë³€ê²½:
-------------------
- tier3.py â†’ phase4_fermi.py
- Tier3FermiPath â†’ Phase4FermiDecomposition
- Phase 4: Estimatorì˜ Fermi Decomposition
- Step 1-4: ë‚´ë¶€ ì„¸ë¶€ ë‹¨ê³„ (ìŠ¤ìº” â†’ ìƒì„± â†’ ì²´í¬ â†’ ì‹¤í–‰)

v7.6.2 ì£¼ìš” ê°œì„ :
-----------------
- í•˜ë“œì½”ë”© ì™„ì „ ì œê±° (adoption_rate, arpu ë“±)
- Boundary ê²€ì¦ ì¶”ê°€ (ê°œë… ê¸°ë°˜)
- Fallback ì²´ê³„ (confidence 0.5)

v7.8.1 ê°œì„ :
- Cursor Mode ì¬ê·€ ì¶”ì • ê°•í™”
- ì •í™•ë„ 3ë°° ê°œì„  (70% â†’ 25% ì˜¤ì°¨)
"""

from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
import time
import math
import copy
from datetime import datetime

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.agents.estimator.models import (
    Context, EstimationResult, DecompositionTrace,
    ComponentEstimation, Phase4Config
)
from umis_rag.agents.estimator.phase3_guestimation import Phase3Guestimation
from umis_rag.utils.logger import logger
from umis_rag.core.config import settings
from umis_rag.core.model_router import select_model_with_config
from umis_rag.core.model_configs import is_pro_model, model_config_manager

# LLM API
try:
    from openai import OpenAI
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

# RAG (Chroma) - v7.10.0: langchain_chroma ìš°ì„  ì‚¬ìš©
try:
    from langchain_chroma import Chroma  # ì‹ ê·œ íŒ¨í‚¤ì§€ (ê¶Œì¥)
    from langchain_openai import OpenAIEmbeddings
    HAS_CHROMA = True
except ImportError:
    try:
        from langchain_community.vectorstores import Chroma  # fallback (deprecated)
        from langchain_openai import OpenAIEmbeddings
        HAS_CHROMA = True
    except ImportError:
        HAS_CHROMA = False
        logger.warning("Chroma íŒ¨í‚¤ì§€ ì—†ìŒ (pip install langchain-chroma)")

import yaml
import re
import json


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ í…œí”Œë¦¿ - REMOVED (v7.5.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 
# v7.5.0 ë³€ê²½:
# - ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ê³„ì‚° ê³µì‹ì€ Quantifierë¡œ ì´ë™
# - EstimatorëŠ” ìˆœìˆ˜ ê°’ ì¶”ì •ë§Œ ë‹´ë‹¹
# - Phase 4ëŠ” ì¼ë°˜ì  Fermi ë¶„í•´ì— ì§‘ì¤‘
#
# ì´ì „ ìœ„ì¹˜: phase4_fermi.py BUSINESS_METRIC_TEMPLATES
# ì‹ ê·œ ìœ„ì¹˜: data/raw/calculation_methodologies.yaml (Quantifier)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# v7.5.0: ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ í…œí”Œë¦¿ ì œê±°ë¨
# Quantifierê°€ LTV, CAC, ARPU ë“±ì˜ ê³„ì‚° ê³µì‹ ì†Œìœ 
# v7.7.0: íŒŒì¼ëª… ë³€ê²½ (tier3.py â†’ phase4_fermi.py)
BUSINESS_METRIC_TEMPLATES_REMOVED = {
    # Unit Economics (ìš°ì„  - "ltv/cac" ì •í™• ë§¤ì¹­)
    "unit_economics": {
        "keywords": ["unit economics", "ltv/cac", "ë¹„ìœ¨", "ratio", "ê²½ì œì„±"],
        "models": [
            {
                "id": "UE_001",
                "formula": "ratio = ltv / cac",
                "description": "LTV/CAC ë¹„ìœ¨",
                "variables": ["ltv", "cac"]
            }
        ]
    },
    
    # ì‹œì¥ ê·œëª¨
    "market_sizing": {
        "keywords": ["ì‹œì¥", "ê·œëª¨", "TAM", "SAM", "market size"],
        "models": [
            {
                "id": "MARKET_001",
                "formula": "market = customers Ã— adoption_rate Ã— arpu Ã— 12",
                "description": "ê¸°ì—…/ê³ ê° ìˆ˜ ê¸°ë°˜ ì‹œì¥ ê·œëª¨",
                "variables": ["customers", "adoption_rate", "arpu"]
            },
            {
                "id": "MARKET_002",
                "formula": "market = population Ã— digital_rate Ã— conversion_rate Ã— arpu Ã— 12",
                "description": "ì¸êµ¬ ê¸°ë°˜ ë””ì§€í„¸ ì „í™˜ ì‹œì¥",
                "variables": ["population", "digital_rate", "conversion_rate", "arpu"]
            }
        ]
    },
    
    # ê³ ê° ìƒì•  ê°€ì¹˜
    "ltv": {
        "keywords": ["ltv", "LTV", "ìƒì• ê°€ì¹˜", "lifetime value"],
        "models": [
            {
                "id": "LTV_001",
                "formula": "ltv = arpu / churn_rate",
                "description": "ARPUë¥¼ Churnìœ¼ë¡œ ë‚˜ëˆˆ LTV",
                "variables": ["arpu", "churn_rate"]
            },
            {
                "id": "LTV_002",
                "formula": "ltv = arpu Ã— average_lifetime_months",
                "description": "í‰ê·  ìƒì•  ê¸°ê°„ ê¸°ë°˜ LTV",
                "variables": ["arpu", "average_lifetime_months"]
            }
        ]
    },
    
    # ê³ ê° íšë“ ë¹„ìš©
    "cac": {
        "keywords": ["cac", "CAC", "ê³ ê°íšë“", "customer acquisition"],
        "models": [
            {
                "id": "CAC_001",
                "formula": "cac = marketing_cost / new_customers",
                "description": "ë§ˆì¼€íŒ… ë¹„ìš©ì„ ì‹ ê·œ ê³ ê°ìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["marketing_cost", "new_customers"]
            },
            {
                "id": "CAC_002",
                "formula": "cac = cpc / conversion_rate",
                "description": "CPCë¥¼ ì „í™˜ìœ¨ë¡œ ë‚˜ëˆ”",
                "variables": ["cpc", "conversion_rate"]
            }
        ]
    },
    
    # ì „í™˜ìœ¨
    "conversion": {
        "keywords": ["ì „í™˜ìœ¨", "conversion", "CVR"],
        "models": [
            {
                "id": "CVR_001",
                "formula": "conversion = paid_users / free_users",
                "description": "ìœ ë£Œ ì „í™˜ìœ¨ (Freemium)",
                "variables": ["paid_users", "free_users"]
            },
            {
                "id": "CVR_002",
                "formula": "conversion = industry_avg Ã— product_quality_factor",
                "description": "ì—…ê³„ í‰ê·  ì¡°ì •",
                "variables": ["industry_avg", "product_quality_factor"]
            }
        ]
    },
    
    # í•´ì§€ìœ¨
    "churn": {
        "keywords": ["churn", "í•´ì§€ìœ¨", "ì´íƒˆìœ¨"],
        "models": [
            {
                "id": "CHURN_001",
                "formula": "churn = churned_customers / total_customers",
                "description": "í•´ì§€ ê³ ê° ë¹„ìœ¨",
                "variables": ["churned_customers", "total_customers"]
            },
            {
                "id": "CHURN_002",
                "formula": "churn = 1 - retention_rate",
                "description": "ìœ ì§€ìœ¨ì˜ ì—­ìˆ˜",
                "variables": ["retention_rate"]
            }
        ]
    },
    
    # ARPU
    "arpu": {
        "keywords": ["arpu", "ARPU", "í‰ê· ë§¤ì¶œ", "average revenue"],
        "models": [
            {
                "id": "ARPU_001",
                "formula": "arpu = base_fee",
                "description": "ê¸°ë³¸ë£Œë§Œ",
                "variables": ["base_fee"]
            },
            {
                "id": "ARPU_002",
                "formula": "arpu = base_fee + overage_fee",
                "description": "ê¸°ë³¸ë£Œ + ì´ˆê³¼ë£Œ",
                "variables": ["base_fee", "overage_fee"]
            },
            {
                "id": "ARPU_003",
                "formula": "arpu = base_fee + usage_fee + addon_fee",
                "description": "ê¸°ë³¸ë£Œ + ì‚¬ìš©ëŸ‰ë£Œ + ì¶”ê°€ê¸°ëŠ¥ë£Œ",
                "variables": ["base_fee", "usage_fee", "addon_fee"]
            }
        ]
    },
    
    # ì„±ì¥ë¥ 
    "growth": {
        "keywords": ["ì„±ì¥ë¥ ", "growth rate", "CAGR"],
        "models": [
            {
                "id": "GROWTH_001",
                "formula": "growth = (current_year - last_year) / last_year",
                "description": "YoY ì„±ì¥ë¥ ",
                "variables": ["current_year", "last_year"]
            },
            {
                "id": "GROWTH_002",
                "formula": "growth = market_growth + market_share_change",
                "description": "ì‹œì¥ ì„±ì¥ + ì ìœ ìœ¨ ë³€í™”",
                "variables": ["market_growth", "market_share_change"]
            }
        ]
    },
    
    # Payback Period (v7.5.0)
    "payback": {
        "keywords": ["payback", "íšŒìˆ˜ê¸°ê°„", "íˆ¬ìíšŒìˆ˜"],
        "models": [
            {
                "id": "PAYBACK_001",
                "formula": "payback = cac / (arpu Ã— gross_margin)",
                "description": "CACë¥¼ ì›” ê¸°ì—¬ì´ìµìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["cac", "arpu", "gross_margin"]
            },
            {
                "id": "PAYBACK_002",
                "formula": "payback = initial_investment / monthly_profit",
                "description": "ì´ˆê¸° íˆ¬ìë¥¼ ì›” ìˆ˜ìµìœ¼ë¡œ ë‚˜ëˆ”",
                "variables": ["initial_investment", "monthly_profit"]
            }
        ]
    },
    
    # Rule of 40 (v7.5.0)
    "rule_of_40": {
        "keywords": ["rule of 40", "40 ë²•ì¹™"],
        "models": [
            {
                "id": "R40_001",
                "formula": "rule_40 = growth_rate + profit_margin",
                "description": "ì„±ì¥ë¥  + ì´ìµë¥  (40% ì´ìƒì´ ê±´ê°•)",
                "variables": ["growth_rate", "profit_margin"]
            }
        ]
    },
    
    # Net Revenue Retention (v7.5.0)
    "nrr": {
        "keywords": ["nrr", "net revenue retention", "ìˆœë§¤ì¶œìœ ì§€ìœ¨"],
        "models": [
            {
                "id": "NRR_001",
                "formula": "nrr = (beginning_mrr + expansion - contraction - churn) / beginning_mrr",
                "description": "ìˆœë§¤ì¶œ ìœ ì§€ìœ¨ (100% ì´ìƒì´ ê±´ê°•)",
                "variables": ["beginning_mrr", "expansion", "contraction", "churn"]
            },
            {
                "id": "NRR_002",
                "formula": "nrr = 1 + expansion_rate - churn_rate",
                "description": "í™•ì¥ë¥  - í•´ì§€ìœ¨ + 1",
                "variables": ["expansion_rate", "churn_rate"]
            }
        ]
    },
    
    # Gross Margin (v7.5.0)
    "gross_margin": {
        "keywords": ["gross margin", "ë§¤ì¶œì´ì´ìµë¥ ", "gross profit"],
        "models": [
            {
                "id": "GM_001",
                "formula": "gross_margin = (revenue - cogs) / revenue",
                "description": "ë§¤ì¶œì´ì´ìµë¥ ",
                "variables": ["revenue", "cogs"]
            },
            {
                "id": "GM_002",
                "formula": "gross_margin = 1 - (cogs / revenue)",
                "description": "1 - COGS ë¹„ìœ¨",
                "variables": ["cogs", "revenue"]
            }
        ]
    }
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë°ì´í„° ëª¨ë¸ (Phase 4 ì „ìš©)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class FermiVariable:
    """
    Fermi ëª¨í˜•ì˜ ë³€ìˆ˜

    Attributes:
        name: ë³€ìˆ˜ ì´ë¦„ (ì˜ˆ: "restaurants", "arpu")
        available: ê°€ìš© ì—¬ë¶€
        value: ê°’ (ì±„ì›Œì§„ ê²½ìš°)
        source: ì¶œì²˜ ("project_data", "tier2", "recursive")
        confidence: ì‹ ë¢°ë„
        need_estimate: ì¶”ì • í•„ìš” ì—¬ë¶€
        estimation_result: ì¶”ì • ê²°ê³¼ (ì¬ê·€ë¡œ ì±„ìš´ ê²½ìš°)
        description: ë³€ìˆ˜ ì„¤ëª…
        estimation_question: ì¶”ì •ìš© ì§ˆë¬¸ (LLM ìƒì„±)
        is_result: ê²°ê³¼ ë³€ìˆ˜ ì—¬ë¶€
        concept: ë„ë©”ì¸ íŠ¹í™” ê°œë… (v7.10.0)
    """
    name: str
    available: bool
    value: Optional[float] = None
    source: str = ""
    confidence: float = 0.0
    need_estimate: bool = False
    uncertainty: float = 0.3

    # ì¬ê·€ ì¶”ì • ê²°ê³¼
    estimation_result: Optional[EstimationResult] = None

    # ë©”íƒ€ë°ì´í„°
    description: str = ""
    estimation_question: Optional[str] = None
    is_result: bool = False
    concept: str = ""  # v7.10.0: ë„ë©”ì¸ íŠ¹í™” ê°œë…


@dataclass
class FermiModel:
    """
    Fermi ì¶”ì • ëª¨í˜•
    
    ì˜ˆ: "ì‹œì¥ = ìŒì‹ì  Ã— ë””ì§€í„¸ìœ¨ Ã— ì „í™˜ìœ¨ Ã— ARPU Ã— 12"
    
    Attributes:
        model_id: ëª¨í˜• ID (MODEL_001, MODEL_002, ...)
        name: ëª¨í˜• ì´ë¦„
        formula: ìˆ˜ì‹ (ë¬¸ìì—´)
        description: ì„¤ëª…
        variables: ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
        total_variables: ì´ ë³€ìˆ˜ ê°œìˆ˜
        unknown_count: Unknown ë³€ìˆ˜ ê°œìˆ˜
        feasibility_score: ì‹¤í–‰ ê°€ëŠ¥ì„± ì ìˆ˜
    """
    model_id: str
    name: str
    formula: str
    description: str
    variables: Dict[str, FermiVariable] = field(default_factory=dict)
    
    # í†µê³„
    total_variables: int = 0
    unknown_count: int = 0
    available_count: int = 0
    
    # í‰ê°€
    feasibility_score: float = 0.0
    unknown_filled: bool = False
    
    # ì„ íƒ
    selection_reason: str = ""
    is_alternative: bool = False
    why_not_selected: str = ""


@dataclass
class RankedModel:
    """
    ì ìˆ˜í™”ëœ ëª¨í˜•
    
    ëª¨í˜• ì„ íƒ ê¸°ì¤€ 4ê°œ:
    - Unknown count (50%)
    - Confidence (30%)
    - Complexity (20%)
    - Depth (10% bonus)
    """
    rank: int
    model: FermiModel
    score: float
    
    # ì ìˆ˜ ë¶„í•´
    unknown_score: float = 0.0
    confidence_score: float = 0.0
    complexity_score: float = 0.0
    depth_score: float = 0.0
    
    # ìƒíƒœ
    status: str = "feasible"  # feasible/partial/infeasible
    missing: List[str] = field(default_factory=list)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë³€ìˆ˜ ìˆ˜ë ´ ì •ì±… (Simple ë°©ì‹)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SimpleVariablePolicy:
    """
    ë‹¨ìˆœ ë³€ìˆ˜ ì •ì±… (ì‹¤ìš©ì )
    
    ì›ì¹™:
    - 6ê°œ: ê¶Œì¥ (Occam's Razor)
    - 7-10ê°œ: í—ˆìš© (ê²½ê³ )
    - 10ê°œ+: ê¸ˆì§€ (Miller's Law)
    
    íš¨ê³¼: 98% (Hybrid ëŒ€ë¹„ 2% ì°¨ì´)
    ì½”ë“œ: 20ì¤„ (Hybrid ëŒ€ë¹„ 15ë°° ê°„ë‹¨)
    """
    
    def __init__(self):
        self.recommended_max = 6   # Occam's Razor
        self.absolute_max = 10     # Miller's Law (7Â±2)
    
    def check(self, variable_count: int) -> Tuple[bool, Optional[str]]:
        """
        ë³€ìˆ˜ ê°œìˆ˜ ì²´í¬
        
        Args:
            variable_count: í˜„ì¬ ë³€ìˆ˜ ê°œìˆ˜
        
        Returns:
            (allowed, warning)
                allowed: True/False
                warning: None ë˜ëŠ” ê²½ê³  ë©”ì‹œì§€
        """
        # ì ˆëŒ€ ìƒí•œ
        if variable_count > self.absolute_max:
            return False, f"ğŸ›‘ ì ˆëŒ€ ìƒí•œ {self.absolute_max}ê°œ ì´ˆê³¼ (ì¸ì§€ í•œê³„)"
        
        # ê¶Œì¥ ìƒí•œ (ê²½ê³ ë§Œ)
        if variable_count > self.recommended_max:
            return True, f"âš ï¸  ê¶Œì¥ ìƒí•œ {self.recommended_max}ê°œ ì´ˆê³¼ (ë³µì¡ë„â†‘)"
        
        # ì •ìƒ
        return True, None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Phase 4 ë©”ì¸ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Phase4FermiDecomposition:
    """
    Phase 4: Fermi Decomposition (v7.7.0)
    
    ì¬ê·€ ë¶„í•´ ì¶”ì • - ë…¼ë¦¬ì˜ í¼ì¦ ë§ì¶”ê¸°
    
    í”„ë¡œì„¸ìŠ¤:
    ---------
    Step 1: ì´ˆê¸° ìŠ¤ìº” (ê°€ìš© ë°ì´í„° íŒŒì•…, Bottom-up)
    Step 2: ëª¨í˜• ìƒì„± (LLM 3-5ê°œ í›„ë³´, Top-down)
    Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€ ì¶”ì •ìœ¼ë¡œ í¼ì¦ ë§ì¶”ê¸°)
    Step 4: ëª¨í˜• ì‹¤í–‰ (Backtrackingìœ¼ë¡œ ì¬ì¡°ë¦½)
    
    ì•ˆì „ ì¥ì¹˜:
    ----------
    - Max depth: 4 (ë¬´í•œ ì¬ê·€ ë°©ì§€)
    - ìˆœí™˜ ê°ì§€: Call stack ì¶”ì 
    - ë³€ìˆ˜ ì œí•œ: 6ê°œ ê¶Œì¥, 10ê°œ ì ˆëŒ€
    
    Usage:
        >>> phase4 = Phase4FermiDecomposition()
        >>> result = phase4.estimate(
        ...     "ìŒì‹ì  SaaS ì‹œì¥ì€?",
        ...     context=Context(domain="Food_Service")
        ... )
        >>> result.decomposition.depth  # 2
        >>> result.value  # 20,160,000,000
    """
    
    def __init__(self, config: Phase4Config = None):
        """ì´ˆê¸°í™” (v7.10.0)"""
        self.config = config or Phase4Config()

        # v7.9.0: llm_modeë¥¼ Propertyë¡œ ë³€ê²½ (ë™ì  ì½ê¸°)
        # Phase 3 ì˜ì¡´ì„± (Noneìœ¼ë¡œ ì „ë‹¬ â†’ ë™ì  ì½ê¸°)
        self.phase3 = Phase3Guestimation(llm_mode=None)

        # ì¬ê·€ ì¶”ì 
        self.call_stack: List[str] = []
        self.max_depth = self.config.max_depth  # 4

        # v7.10.0: ì „ì—­ ì‹œë„ ì œí•œ (ì¬ê·€ í­ë°œ ë°©ì§€)
        self.max_global_attempts = 3  # ìµœëŒ€ 3íšŒ ì‹œë„
        self._global_attempt_count = 0  # í˜„ì¬ ì‹œë„ íšŸìˆ˜
        self._attempt_results: List[EstimationResult] = []  # ì‹œë„ë³„ ê²°ê³¼ ì €ì¥

        # v7.10.0: ì‹œë„ë³„ confidence threshold (ì ì§„ì  ì™„í™”)
        # 1íšŒì°¨: 0.80 (ì—„ê²©), 2íšŒì°¨: 0.60 (ì™„í™”), 3íšŒì°¨: 0.40 (ì¶”ê°€ ì™„í™”)
        self._confidence_thresholds = [0.80, 0.60, 0.40]
        self._current_threshold_idx = 0

        # v7.10.2: ì „ì—­ ë³€ìˆ˜ ì¶”ì • íšŸìˆ˜ ì œí•œ (ì¬ê·€ í­ë°œ ì™„ì „ ë°©ì§€)
        self.max_variable_estimates = 20  # ìµœëŒ€ 20ê°œ ë³€ìˆ˜ë§Œ ì¶”ì •
        self._total_variable_estimate_count = 0  # ì´ ë³€ìˆ˜ ì¶”ì • ì‹œë„ íšŸìˆ˜

        # ë³€ìˆ˜ ì •ì±…
        self.variable_policy = SimpleVariablePolicy()

        # LLM Client (ì´ˆê¸°í™” ì‹œì—ëŠ” ìƒì„± ì•ˆ í•¨, í•„ìš”í•  ë•Œ ë™ì  ìƒì„±)
        self._llm_client = None

        logger.info("[Phase 4] Fermi Decomposition ì´ˆê¸°í™”")
        logger.info(f"  Max depth: {self.max_depth}")
        logger.info(f"  Max attempts: {self.max_global_attempts} (ì ì§„ì  ì™„í™”: {self._confidence_thresholds})")
        logger.info(f"  Max variables: {self.max_variable_estimates} (v7.10.2: ì¬ê·€ í­ë°œ ì™„ì „ ë°©ì§€)")
        logger.info(f"  ë³€ìˆ˜ ì •ì±…: ê¶Œì¥ 6ê°œ, ì ˆëŒ€ 10ê°œ")
        logger.info(f"  LLM ëª¨ë“œ: {self.llm_mode}")

        # ì´ˆê¸°í™” ì‹œì ì˜ ëª¨ë“œ ë¡œê¹…
        if self.llm_mode != 'cursor':
            logger.info(f"  API Mode: {self.llm_mode}")
        else:
            logger.info("  Cursor AI Mode (ë¹„ìš© $0)")
            logger.info("     ì§ì ‘ ëª¨í˜• ìƒì„±: ì§ˆë¬¸ ë¶„ì„ -> ìƒì‹ ê¸°ë°˜ ì¶”ì • (ì¬ê·€ ìµœì†Œí™”)")
    
    @property
    def llm_mode(self) -> str:
        """
        LLM ëª¨ë“œ ë™ì  ì½ê¸° (v7.9.0)
        
        Property íŒ¨í„´ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ settings ë³€ê²½ ì‹œ ì¦‰ì‹œ ë°˜ì˜
        """
        from umis_rag.core.config import settings
        return settings.llm_mode
    
    @property
    def llm_client(self):
        """
        LLM Client ë™ì  ìƒì„± (v7.9.0)
        
        cursor ëª¨ë“œê°€ ì•„ë‹ ë•Œë§Œ OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        ë§¤ë²ˆ í˜„ì¬ llm_modeë¥¼ í™•ì¸í•˜ì—¬ í•„ìš” ì‹œ ì¬ìƒì„±
        """
        # cursor ëª¨ë“œë©´ None ë°˜í™˜
        if self.llm_mode == 'cursor':
            return None
        
        # API ëª¨ë“œì´ì§€ë§Œ í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ê±°ë‚˜ ëª¨ë“œê°€ ë³€ê²½ë˜ì—ˆìœ¼ë©´ ìƒì„±
        if self._llm_client is None or getattr(self, '_cached_mode', None) != self.llm_mode:
            from umis_rag.core.config import settings
            if HAS_OPENAI and settings.openai_api_key:
                from openai import OpenAI
                self._llm_client = OpenAI(api_key=settings.openai_api_key)
                self._cached_mode = self.llm_mode
                logger.debug(f"  OpenAI Client ìƒì„±: {self.llm_mode}")
            else:
                logger.warning(f"  âš ï¸  API ëª¨ë“œ({self.llm_mode})ì§€ë§Œ OpenAI API í‚¤ ì—†ìŒ")
                return None
        
        return self._llm_client
    
    def estimate(
        self,
        question: str,
        context: Context = None,
        available_data: Dict = None,
        depth: int = 0,
        parent_data: Dict = None
    ) -> Optional[EstimationResult]:
        """
        Fermi Decomposition ì¶”ì • (v7.10.0: 3íšŒ ì‹œë„ + ì ì§„ì  ì™„í™”)

        Args:
            question: ì§ˆë¬¸ (ì˜ˆ: "ìŒì‹ì  SaaS ì‹œì¥ì€?")
            context: ë§¥ë½ (domain, region, time)
            available_data: ê°€ìš© ë°ì´í„° (í”„ë¡œì íŠ¸ ì œê³µ)
            depth: í˜„ì¬ ì¬ê·€ ê¹Šì´
            parent_data: ë¶€ëª¨ ë°ì´í„° (ì¬ê·€ ì‹œ ìƒì†) v7.5.0+

        Returns:
            EstimationResult (decomposition í¬í•¨) ë˜ëŠ” None

        v7.10.0 ì¬ì‹œë„ ë¡œì§:
            - ìµœëŒ€ 3íšŒ ì‹œë„ (depth == 0ì¼ ë•Œë§Œ)
            - 1íšŒì°¨: confidence >= 0.80 (ì—„ê²©)
            - 2íšŒì°¨: confidence >= 0.60 (ì™„í™”)
            - 3íšŒì°¨: confidence >= 0.40 (ì¶”ê°€ ì™„í™”)
            - 3íšŒ ì‹¤íŒ¨ ì‹œ ìµœì„  ê²°ê³¼ ë°˜í™˜
        """
        # v7.10.0: ìµœìƒìœ„ í˜¸ì¶œì—ì„œë§Œ ì¬ì‹œë„ ë¡œì§ ì ìš©
        if depth == 0:
            # ìƒíƒœ ì´ˆê¸°í™”
            self._global_attempt_count = 0
            self._attempt_results = []
            self._current_threshold_idx = 0
            
            # v7.10.2: ë³€ìˆ˜ ì¶”ì • ì¹´ìš´í„° ì´ˆê¸°í™”
            self._total_variable_estimate_count = 0

            # ìµœëŒ€ 3íšŒ ì‹œë„
            for attempt in range(self.max_global_attempts):
                self._global_attempt_count = attempt + 1
                self._current_threshold_idx = attempt
                threshold = self._confidence_thresholds[min(attempt, len(self._confidence_thresholds) - 1)]

                logger.info(f"\n[Phase 4] === ì‹œë„ {attempt + 1}/{self.max_global_attempts} (threshold: {threshold:.2f}) ===")

                result = self._single_estimate_attempt(
                    question, context, available_data, depth, parent_data
                )

                if result:
                    # ê²°ê³¼ ì €ì¥
                    self._attempt_results.append(result)

                    # ì¶©ë¶„í•œ ì‹ ë¢°ë„ë©´ ì¦‰ì‹œ ë°˜í™˜
                    if result.confidence >= threshold:
                        logger.info(f"[Phase 4] ì‹œë„ {attempt + 1} ì„±ê³µ (confidence: {result.confidence:.2f} >= {threshold:.2f})")
                        return result
                    else:
                        logger.info(f"[Phase 4] ì‹œë„ {attempt + 1} ì‹ ë¢°ë„ ë¶€ì¡± (confidence: {result.confidence:.2f} < {threshold:.2f})")
                else:
                    logger.warning(f"[Phase 4] ì‹œë„ {attempt + 1} ì‹¤íŒ¨ (ê²°ê³¼ ì—†ìŒ)")

            # 3íšŒ ëª¨ë‘ ì‹¤íŒ¨ ì‹œ: ìµœì„  ê²°ê³¼ ì„ íƒ
            if self._attempt_results:
                best_result = max(self._attempt_results, key=lambda r: r.confidence)
                logger.info(f"[Phase 4] 3íšŒ ì‹œë„ ì™„ë£Œ -> ìµœì„  ê²°ê³¼ ì„ íƒ (confidence: {best_result.confidence:.2f})")
                best_result.reasoning_detail['retry_info'] = {
                    'total_attempts': self._global_attempt_count,
                    'selected_from': len(self._attempt_results),
                    'selection_reason': 'best_confidence'
                }
                return best_result
            else:
                logger.warning("[Phase 4] 3íšŒ ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨ (ê²°ê³¼ ì—†ìŒ)")
                return None
        else:
            # ì¬ê·€ í˜¸ì¶œ (depth > 0): ë‹¨ì¼ ì‹œë„ë§Œ
            return self._single_estimate_attempt(
                question, context, available_data, depth, parent_data
            )

    def _get_current_confidence_threshold(self) -> float:
        """í˜„ì¬ ì‹œë„ì˜ confidence threshold ë°˜í™˜ (v7.10.0)"""
        idx = min(self._current_threshold_idx, len(self._confidence_thresholds) - 1)
        return self._confidence_thresholds[idx]

    def _single_estimate_attempt(
        self,
        question: str,
        context: Context = None,
        available_data: Dict = None,
        depth: int = 0,
        parent_data: Dict = None
    ) -> Optional[EstimationResult]:
        """
        ë‹¨ì¼ ì¶”ì • ì‹œë„ (v7.10.0)

        ê¸°ì¡´ estimate ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ì¬ì‹œë„ ê°€ëŠ¥í•˜ê²Œ í•¨
        """
        start_time = time.time()

        logger.info(f"\n{'  ' * depth}[Phase 4] Fermi Estimation (depth {depth})")
        logger.info(f"{'  ' * depth}  ì§ˆë¬¸: {question}")

        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì•ˆì „ ì²´í¬
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        # 1. Max depth ì²´í¬
        if depth >= self.max_depth:
            logger.warning(f"{'  ' * depth}  Max depth {self.max_depth} ë„ë‹¬ -> Phase 3 Fallback")
            # Fallback to Phase 3
            return self.phase3.estimate(question, context or Context())

        # 2. ìˆœí™˜ ê°ì§€
        if self._detect_circular(question):
            logger.warning(f"{'  ' * depth}  ìˆœí™˜ ì˜ì¡´ì„± ê°ì§€ (A->B->A) -> ì¤‘ë‹¨")
            return None

        # 3. Call stack ì¶”ê°€
        self.call_stack.append(question)

        try:
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 1: ì´ˆê¸° ìŠ¤ìº” (ë°ì´í„° ìƒì† v7.5.0)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            scan_result = self._step1_scan(question, context, available_data, depth, parent_data)

            if not scan_result:
                logger.warning(f"{'  ' * depth}  Step 1 ì‹¤íŒ¨")
                return None

            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 2: ëª¨í˜• ìƒì„± + ë°˜ë³µ ê°œì„ 
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            candidate_models = self._step2_generate_models(
                question,
                scan_result['available'],
                scan_result['unknown'],
                depth,
                context or Context()
            )

            if not candidate_models:
                logger.warning(f"{'  ' * depth}  Step 2 ì‹¤íŒ¨ (ëª¨í˜• ì—†ìŒ)")
                return None

            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€!)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            ranked_models = self._step3_check_feasibility(
                candidate_models,
                context or Context(),
                depth
            )

            if not ranked_models:
                logger.warning(f"{'  ' * depth}  Step 3 ì‹¤íŒ¨ (ì‹¤í–‰ ë¶ˆê°€ëŠ¥)")
                return None

            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Step 4: ìµœì„  ëª¨í˜• ì‹¤í–‰
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            result = self._step4_execute(ranked_models[0], depth, context or Context())

            if result:
                execution_time = time.time() - start_time

                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                # Phase 5: Boundary ê²€ì¦ (v7.6.2)
                # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
                boundary_check = self._phase5_boundary_validation(
                    result, question, ranked_models[0], depth
                )

                if not boundary_check.is_valid:
                    logger.warning(f"{'  ' * depth}  Boundary ê²€ì¦ ì‹¤íŒ¨: {boundary_check.reasoning}")
                    logger.warning(f"{'  ' * depth}  -> ë‹¤ìŒ ëª¨í˜• ì‹œë„ ë˜ëŠ” None ë°˜í™˜")
                    # TODO: ë‹¤ìŒ ìˆœìœ„ ëª¨í˜• ì‹œë„
                    return None

                # Boundary ê²€ì¦ ì •ë³´ ì¶”ê°€
                result.reasoning_detail['boundary_check'] = {
                    'is_valid': boundary_check.is_valid,
                    'hard_violations': boundary_check.hard_violations,
                    'soft_warnings': boundary_check.soft_warnings,
                    'confidence_adjustment': boundary_check.confidence
                }

                # Soft warningì´ ìˆìœ¼ë©´ confidence ì¡°ì •
                if boundary_check.soft_warnings:
                    original_conf = result.confidence
                    result.confidence = result.confidence * boundary_check.confidence
                    logger.info(f"{'  ' * depth}  Soft warning -> confidence {original_conf:.2f} -> {result.confidence:.2f}")

                logger.info(f"{'  ' * depth}  Phase 4 ì™„ë£Œ: {result.value} ({execution_time:.2f}ì´ˆ)")

            return result

        except Exception as e:
            logger.error(f"{'  ' * depth}  Phase 4 ì—ëŸ¬: {e}")
            return None

        finally:
            # Call stackì—ì„œ ì œê±° (ì¤‘ìš”!)
            if self.call_stack and self.call_stack[-1] == question:
                self.call_stack.pop()
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 1: ì´ˆê¸° ìŠ¤ìº” (ê°€ìš© ë°ì´í„° íŒŒì•…)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step1_scan(
        self,
        question: str,
        context: Optional[Context],
        available_data: Optional[Dict],
        depth: int,
        parent_data: Optional[Dict] = None
    ) -> Optional[Dict]:
        """
        Step 1: ì´ˆê¸° ìŠ¤ìº” (Bottom-up) - í™•ì¥ë¨
        
        ê°€ìš©í•œ ë°ì´í„° íŒŒì•… (ìš°ì„ ìˆœìœ„ ìˆœ):
        0. ë¶€ëª¨ ë°ì´í„° ìƒì† (ì¬ê·€ ì‹œ)
        1. í”„ë¡œì íŠ¸ ë°ì´í„° (ìµœìš°ì„ )
        2. RAG ê²€ìƒ‰ (ë²¤ì¹˜ë§ˆí¬, ì—…ê³„ í‰ê· )
        3. Phase 3 Source (í†µê³„, ëª…í™•í•œ ê°’)
        4. Context ìƒìˆ˜ (ë¬¼ë¦¬/í†µê³„ ìƒìˆ˜)
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
            available_data: í”„ë¡œì íŠ¸ ë°ì´í„°
            depth: ê¹Šì´
            parent_data: ë¶€ëª¨ ë°ì´í„°
        
        Returns:
            {'available': Dict[str, FermiVariable], 'unknown': []}
        """
        logger.info(f"{'  ' * depth}  [Step 1] ì´ˆê¸° ìŠ¤ìº” (í™•ì¥)")
        
        available = {}
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 0: ë¶€ëª¨ ë°ì´í„° ìƒì† (ìš°ì„ ìˆœìœ„ 2)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if parent_data:
            for key, val in parent_data.items():
                if isinstance(val, FermiVariable):
                    available[key] = val
                    logger.info(f"{'  ' * depth}    [ë¶€ëª¨] {key} = {val.value}")
                elif isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source=val.get('source', 'parent_inherited'),
                        confidence=val.get('confidence', 0.8)
                    )
                    logger.info(f"{'  ' * depth}    [ë¶€ëª¨] {key} = {val.get('value')}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 1: í”„ë¡œì íŠ¸ ë°ì´í„° (ìš°ì„ ìˆœìœ„ 1, ìµœìš°ì„ )
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if available_data:
            for key, val in available_data.items():
                # í”„ë¡œì íŠ¸ ë°ì´í„°ëŠ” í•­ìƒ ë®ì–´ì“°ê¸° (ìµœìš°ì„ )
                if isinstance(val, dict):
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val.get('value'),
                        source="project_data",
                        confidence=val.get('confidence', 1.0),
                        uncertainty=val.get('uncertainty', 0.0)
                    )
                else:
                    available[key] = FermiVariable(
                        name=key,
                        available=True,
                        value=val,
                        source="project_data",
                        confidence=1.0,
                        uncertainty=0.0
                    )
                logger.info(f"{'  ' * depth}    [í”„ë¡œì íŠ¸] {key} = {val if not isinstance(val, dict) else val.get('value')}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2: RAG ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ 3, ìµœìƒìœ„ë§Œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if depth == 0 and context:  # ìµœìƒìœ„ë§Œ ê²€ìƒ‰ (ë¹„ìš© ì ˆê°)
            logger.info(f"{'  ' * depth}    [RAG] ë²¤ì¹˜ë§ˆí¬ ê²€ìƒ‰ ì¤‘...")
            rag_data = self._search_rag_benchmarks(question, context)
            
            for key, var in rag_data.items():
                if key not in available:  # í”„ë¡œì íŠ¸ ë°ì´í„° ìš°ì„ 
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [RAG] {key} = {var.value} (conf: {var.confidence:.2f})")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 3: Phase 3 Source (ìš°ì„ ìˆœìœ„ 4, ìµœìƒìœ„ë§Œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if depth == 0 and context:
            logger.info(f"{'  ' * depth}    [Phase 3] Source ì¡°íšŒ ì¤‘...")
            phase3_data = self._query_phase3_sources(question, context)
            
            for key, var in phase3_data.items():
                if key not in available:  # í”„ë¡œì íŠ¸/RAG ìš°ì„ 
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [Phase 3] {key} = {var.value} (conf: {var.confidence:.2f})")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 4: Context ìƒìˆ˜ (ìš°ì„ ìˆœìœ„ 5)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if context:
            context_data = self._extract_context_constants(question, context)
            
            for key, var in context_data.items():
                if key not in available:
                    available[key] = var
                    logger.info(f"{'  ' * depth}    [Context] {key} = {var.value}")
        
        logger.info(f"{'  ' * depth}    ì´ ê°€ìš© ë°ì´í„°: {len(available)}ê°œ")
        
        return {
            'available': available,
            'unknown': []
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 2: ëª¨í˜• ìƒì„± (LLM)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step2_generate_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        unknown: List[str],
        depth: int,
        context: Optional[Context] = None
    ) -> List[FermiModel]:
        """
        Step 2: ëª¨í˜• ìƒì„± (Top-down) + ë°˜ë³µ ê°œì„ 
        
        í”„ë¡œì„¸ìŠ¤:
        2a. LLM ëª¨í˜• ìƒì„±
        2b. ì œì•ˆ ë³€ìˆ˜ ì¬ê²€ìƒ‰ (ìµœëŒ€ 2íšŒ)
        2c. ë³€ìˆ˜ ì •ì±… í•„í„°ë§
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            unknown: ë¯¸ì§€ìˆ˜ ë¦¬ìŠ¤íŠ¸
            depth: ê¹Šì´
            context: ë§¥ë½ (Step 2bìš©)
        
        Returns:
            3-5ê°œ FermiModel í›„ë³´
        """
        logger.info(f"{'  ' * depth}  [Step 2] ëª¨í˜• ìƒì„±")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2a: LLM ëª¨í˜• ìƒì„±
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        models = self._generate_default_models(question, available, depth, context)
        
        if not models:
            return []
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2b: ë³€ìˆ˜ ì¬ê²€ìƒ‰ ë° ê°œì„ 
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if context and depth == 0:  # ìµœìƒìœ„ë§Œ (ë¹„ìš© ì ˆê°)
            models = self._phase2b_refine_with_data_search(
                models, question, context, depth
            )
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Step 2c: ë³€ìˆ˜ ì •ì±… í•„í„°ë§
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        filtered_models = []
        for model in models:
            allowed, warning = self.variable_policy.check(model.total_variables)
            
            if not allowed:
                logger.warning(f"{'  ' * depth}    ëª¨í˜• {model.model_id} ì œì™¸: {warning}")
                model.why_not_selected = warning
                continue
            
            if warning:
                logger.warning(f"{'  ' * depth}    ëª¨í˜• {model.model_id}: {warning}")
            
            filtered_models.append(model)
        
        logger.info(f"{'  ' * depth}    ìµœì¢… ëª¨í˜•: {len(filtered_models)}ê°œ")
        
        return filtered_models
    
    def _generate_default_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int,
        context: Optional[Context] = None
    ) -> List[FermiModel]:
        """
        ê¸°ë³¸ ëª¨í˜• ìƒì„± (v7.8.1: Model Config í†µí•©)
        
        Model Config ì‹œìŠ¤í…œì„ í†µí•´ í†µí•©ëœ ì²˜ë¦¬:
        - Cursor/API ëª¨ë‘ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        - ì°¨ì´ëŠ” LLM í˜¸ì¶œ ë°©ì‹ë§Œ (Cursor AI vs External API)
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            depth: ê¹Šì´
            context: ë§¥ë½
        
        Returns:
            FermiModel ë¦¬ìŠ¤íŠ¸
        """
        # v7.8.1: Model Config ì‹œìŠ¤í…œ ì‚¬ìš©
        # Cursor/API ëª¨ë‘ _generate_llm_models ì‚¬ìš©
        # ë‹¨ì§€ LLM í˜¸ì¶œ ë°©ì‹ë§Œ ë‹¤ë¦„
        
        logger.info(f"{'  ' * depth}    [Phase 4] ëª¨í˜• ìƒì„± ì‹œì‘ (Mode: {self.llm_mode})")
        
        models = self._generate_llm_models(question, available, depth)
        
        if models:
            return models
        
        # Fallback: Phase 3ìœ¼ë¡œ ìœ„ì„
        logger.info(f"{'  ' * depth}    Fallback â†’ Phase 3 ìœ„ì„")
        return []
    
    def _generate_llm_models(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        depth: int
    ) -> List[FermiModel]:
        """
        LLMìœ¼ë¡œ ëª¨í˜• ìƒì„± (v7.8.1: Native/External í†µí•©)
        
        ì„¤ê³„: fermi_model_search.yaml Line 1158-1181
        
        v7.8.1: Cursor/API í†µí•©
        - Cursor Mode: Cursor AIì—ê²Œ instruction ì „ë‹¬ (ë¬´ë£Œ, ëŒ€í™” ì»¨í…ìŠ¤íŠ¸)
        - API Mode: External LLM API í˜¸ì¶œ (ìœ ë£Œ)
        - ì°¨ì´ëŠ” LLM í˜¸ì¶œ ë°©ì‹ë§Œ, ë¡œì§ì€ ë™ì¼
        
        v7.8.0: Model Config ì‹œìŠ¤í…œ í†µí•©
        - select_model_with_config() ì‚¬ìš©
        - API íƒ€ì… ìë™ ë¶„ê¸° (Responses/Chat)
        - Pro ëª¨ë¸ Fast Mode ìë™ ì ìš©
        
        Args:
            question: ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜
            depth: ê¹Šì´
        
        Returns:
            LLMì´ ìƒì„±í•œ FermiModel ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"{'  ' * depth}      [LLM] ëª¨í˜• ìƒì„± ìš”ì²­ (Mode: {self.llm_mode})")

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Cursor/API ê³µí†µ)
        # v7.10.0: Pro ëª¨ë¸ìš© Fast Mode ì§€ì›ì„ ìœ„í•´ model_name ì „ë‹¬
        model_name = self.llm_mode if self.llm_mode != 'cursor' else None
        prompt = self._build_llm_prompt(question, available, model_name=model_name)
        
        try:
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # Cursor AI: instruction ì „ë‹¬ (ëŒ€í™”í˜•)
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            if self.llm_mode == 'cursor':  # v7.8.1: cursor = Cursor AI
                logger.info(f"{'  ' * depth}      [Cursor AI] ëŒ€í™”í˜• ëª¨í˜• ìƒì„± - instruction ì‘ì„±")
                logger.info(f"{'  ' * depth}      [Cursor AI] ë¹„ìš©: $0 (ë¬´ë£Œ)")
                
                instruction = f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Fermi ëª¨í˜• ìƒì„± ìš”ì²­ (Cursor AI)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

{prompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âš ï¸ ì¤‘ìš”: ìœ„ í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ Fermi ëª¨í˜•ì„ ìƒì„±í•´ì£¼ì„¸ìš”!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
                logger.warning(f"{'  ' * depth}      [Cursor AI] ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ ì‘ë‹µ í•„ìš”")
                logger.info(f"{'  ' * depth}      [Cursor AI] Instruction ì‘ì„± ì™„ë£Œ â†’ Phase 3 Fallback")
                return []
            
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            # External API: OpenAI API í˜¸ì¶œ
            # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            else:  # External API (self.llm_mode = gpt-4o-mini, o1-mini ë“±)
                # v7.8.0: Model Config ì‹œìŠ¤í…œ ì‚¬ìš©
                model_name, model_config = select_model_with_config(phase=4)
                
                logger.info(f"{'  ' * depth}      [LLM] ëª¨ë¸: {model_name}")
                logger.info(f"{'  ' * depth}      [LLM] API: {model_config.api_type}")
                
                # Fast Mode ì ìš© (Pro ëª¨ë¸)
                if is_pro_model(model_name):
                    logger.info(f"{'  ' * depth}      [LLM] Fast Mode ì ìš© (Pro ëª¨ë¸)")
                    fast_mode_prefix = """ğŸ”´ SPEED OPTIMIZATION MODE
â±ï¸ ëª©í‘œ ì‘ë‹µ ì‹œê°„: 60ì´ˆ ì´ë‚´
ğŸ“ ìµœëŒ€ ì¶œë ¥ ê¸¸ì´: 2,000ì ì´ë‚´

"""
                    prompt = fast_mode_prefix + prompt
                
                # API íŒŒë¼ë¯¸í„° êµ¬ì„± (ìë™)
                api_params = model_config.build_api_params(
                    prompt=prompt,
                    reasoning_effort='medium'  # Phase 4 ê¸°ë³¸ê°’
                )
                
                # API íƒ€ì…ë³„ ë¶„ê¸° (Responses vs Chat)
                if model_config.api_type == 'responses':
                    # Responses API (o1, o3, gpt-5 ì‹œë¦¬ì¦ˆ)
                    response = self.llm_client.responses.create(**api_params)
                else:
                    # Chat Completions API (gpt-4 ì‹œë¦¬ì¦ˆ)
                    # System message ì¶”ê°€
                    if 'messages' in api_params:
                        api_params['messages'].insert(0, {
                            "role": "system",
                            "content": "ë‹¹ì‹ ì€ Fermi Estimation ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ê³„ì‚° ê°€ëŠ¥í•œ ìˆ˜í•™ì  ëª¨í˜•ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”."
                        })
                    
                    response = self.llm_client.chat.completions.create(**api_params)
                
                # â­ v7.8.1: í†µí•© íŒŒì‹± (êµ¬ì¡°ì  ì‘ë‹µ íŒŒì‹±)
                llm_output = self._parse_llm_response(
                    response=response,
                    api_type=model_config.api_type,
                    depth=depth
                )
                
                # v7.8.1: llm_outputì´ Noneì¼ ìˆ˜ ìˆìŒ (ë¹ˆ ì‘ë‹µ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨)
                if not llm_output:
                    logger.warning(f"{'  ' * depth}      âš ï¸ LLM ë¹ˆ ì‘ë‹µ ë˜ëŠ” íŒŒì‹± ì‹¤íŒ¨")
                    return []
                
                logger.info(f"{'  ' * depth}      [LLM] ì‘ë‹µ ìˆ˜ì‹  ({len(llm_output)}ì)")
                
                # ì‘ë‹µ íŒŒì‹±
                models = self._parse_llm_models(llm_output, depth)
                
                if not models:
                    logger.warning(f"{'  ' * depth}      âš ï¸ íŒŒì‹± ê²°ê³¼ ì—†ìŒ")
                    return []
                
                logger.info(f"{'  ' * depth}      [LLM] íŒŒì‹± ì™„ë£Œ: {len(models)}ê°œ ëª¨í˜•")
                
                return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}      âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_llm_response(
        self,
        response: Any,
        api_type: str,
        depth: int = 0
    ) -> Optional[str]:
        """
        LLM ì‘ë‹µ íŒŒì‹± (API Typeë³„ í†µí•©)
        
        v7.8.1: êµ¬ì¡°ì  ì‘ë‹µ íŒŒì‹± (ë²¤ì¹˜ë§ˆí¬ íŒ¨í„´ ì ìš©)
        
        Args:
            response: API ì‘ë‹µ ê°ì²´
            api_type: 'responses', 'chat', 'cursor'
            depth: ë¡œê·¸ ë“¤ì—¬ì“°ê¸°
        
        Returns:
            íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        try:
            # API Typeë³„ íŒŒì‹±
            if api_type == 'responses':
                # Responses API (o1, o3, o4, gpt-5 ì‹œë¦¬ì¦ˆ)
                
                # Level 1: í‘œì¤€ í”„ë¡œí¼í‹° (output_text)
                if hasattr(response, 'output_text'):
                    logger.info(f"{'  ' * depth}      [Parser] Level 1: output_text í”„ë¡œí¼í‹° ì‚¬ìš©")
                    return response.output_text
                
                # Level 2: ê°ì²´ êµ¬ì¡° íƒìƒ‰ (output)
                if hasattr(response, 'output'):
                    output = response.output
                    
                    # outputì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    if isinstance(output, list) and output:
                        output_item = output[0]
                        
                        # ResponseOutputMessage ê°ì²´
                        if hasattr(output_item, 'content'):
                            content = output_item.content
                            
                            # contentê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì‹¤ì œ êµ¬ì¡°!)
                            if isinstance(content, list) and content:
                                # ResponseOutputText ê°ì²´
                                if hasattr(content[0], 'text'):
                                    logger.info(f"{'  ' * depth}      [Parser] Level 2: output[0].content[0].text")
                                    return content[0].text
                            
                            # contentê°€ ë¬¸ìì—´ì¸ ê²½ìš°
                            if isinstance(content, str):
                                logger.info(f"{'  ' * depth}      [Parser] Level 2: output[0].content (string)")
                                return content
                        
                        # text í”„ë¡œí¼í‹° ì§ì ‘ ì¡´ì¬
                        if hasattr(output_item, 'text'):
                            logger.info(f"{'  ' * depth}      [Parser] Level 2: output[0].text")
                            return output_item.text
                    
                    # outputì´ ë¬¸ìì—´ì¸ ê²½ìš°
                    if isinstance(output, str):
                        logger.info(f"{'  ' * depth}      [Parser] Level 2: output (string)")
                        return output
                
                # Level 3: ë¬¸ìì—´ ë³€í™˜
                logger.warning(f"{'  ' * depth}      âš ï¸ Responses API: ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°, str() ë³€í™˜")
                return str(response)
            
            elif api_type == 'chat':
                # Chat Completions API (gpt-4, gpt-4o ì‹œë¦¬ì¦ˆ)
                
                # Level 1: í‘œì¤€ êµ¬ì¡°
                if hasattr(response, 'choices') and response.choices:
                    message = response.choices[0].message
                    if hasattr(message, 'content'):
                        logger.info(f"{'  ' * depth}      [Parser] Level 1: choices[0].message.content")
                        return message.content
                
                # Level 2: Fallback
                logger.warning(f"{'  ' * depth}      âš ï¸ Chat API: ì•Œ ìˆ˜ ì—†ëŠ” ì‘ë‹µ êµ¬ì¡°")
                return str(response)
            
            elif api_type == 'cursor':
                # Cursor AI (ëŒ€í™”í˜•)
                logger.info(f"{'  ' * depth}      â„¹ï¸  Cursor AIëŠ” ëŒ€í™”í˜• ëª¨ë“œ (íŒŒì‹± ë¶ˆí•„ìš”)")
                return None
            
            else:
                logger.error(f"{'  ' * depth}      âŒ ì•Œ ìˆ˜ ì—†ëŠ” API Type: {api_type}")
                return None
        
        except Exception as e:
            logger.error(f"{'  ' * depth}      âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _build_llm_prompt(
        self,
        question: str,
        available: Dict[str, FermiVariable],
        model_name: Optional[str] = None
    ) -> str:
        """
        LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (v7.10.0: ë²¤ì¹˜ë§ˆí¬ íŒ¨í„´ ì ìš©)

        v7.10.0 ê°œì„ :
        - concept í•„ë“œ ê°•ì œ (ëª¨ë“  decomposition ë‹¨ê³„)
        - final_calculation, calculation_verification í•„ìˆ˜í™”
        - ë§ˆì§€ë§‰ ë‹¨ê³„ value = ìµœìƒìœ„ value ì •í™•íˆ ì¼ì¹˜ ê°•ì œ
        - ëª…í™•í•œ ì‚¬ì¹™ì—°ì‚° ê°•ì œ
        - Pro ëª¨ë¸ìš© Fast Mode constraint ì¶”ê°€

        ì„¤ê³„: benchmarks/estimator/phase4/common.py get_improved_fewshot_prompt()

        Args:
            question: ì¶”ì • ì§ˆë¬¸
            available: ê°€ìš© ë³€ìˆ˜ ë”•ì…”ë„ˆë¦¬
            model_name: ëª¨ë¸ ì´ë¦„ (Pro ëª¨ë¸ì´ë©´ Fast Mode ì ìš©)
        """
        # v7.10.0: Pro ëª¨ë¸ìš© Fast Mode constraint
        pro_models = ['gpt-5-pro', 'o1-pro', 'o1-pro-2025-03-19', 'o1-preview']
        fast_mode_constraint = ""

        if model_name and model_name in pro_models:
            fast_mode_constraint = """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
SPEED OPTIMIZATION MODE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ëª©í‘œ ì‘ë‹µ ì‹œê°„: 60ì´ˆ ì´ë‚´
ìµœëŒ€ ì¶œë ¥ ê¸¸ì´: 2,000ì ì´ë‚´ (ì•½ 500 í† í°)
decomposition: 3-5ë‹¨ê³„ë§Œ (í•„ìˆ˜ ë‹¨ê³„ë§Œ í¬í•¨)
reasoning: ê° ë‹¨ê³„ 15ë‹¨ì–´ ì´ë‚´

ë¹ ë¥´ê³  ê°„ê²°í•˜ê²Œ í•µì‹¬ë§Œ ë‹µë³€í•˜ì„¸ìš”!
ê¹Šì€ ì¶”ë¡ ë³´ë‹¤ëŠ” ì§ê´€ì  ê·¼ì‚¬ì¹˜ë¥¼ ìš°ì„ í•˜ì„¸ìš”.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
            logger.info(f"  [Fast Mode] {model_name}ì— ì†ë„ ìµœì í™” í”„ë¡¬í”„íŠ¸ ì ìš©")

        # v7.10.0: í•„ìˆ˜ í•„ë“œ ê°•ì œ í—¤ë”
        mandatory_header = """â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CRITICAL MANDATORY FIELDS (ëˆ„ë½ ì‹œ ì‹¤íŒ¨!):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. decompositionì˜ ëª¨ë“  ë‹¨ê³„ì— "concept" í•„ë“œ í•„ìˆ˜!
2. ìµœìƒìœ„ "final_calculation" í•„ë“œ í•„ìˆ˜!
3. ìµœìƒìœ„ "calculation_verification" í•„ë“œ í•„ìˆ˜!
4. decomposition[-1]["value"] == JSON["value"] (ì •í™•íˆ ì¼ì¹˜!)

ì´ í•„ë“œë“¤ì´ í•˜ë‚˜ë¼ë„ ëˆ„ë½ë˜ë©´ ì¶”ì •ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        # v7.10.0: ê°œì„ ëœ Few-shot ì˜ˆì‹œ (concept í•„ë“œ í¬í•¨, value ì¼ì¹˜)
        fewshot_example = """
ì˜¬ë°”ë¥¸ ì˜ˆì‹œ: ì„œìš¸ì‹œ íƒì‹œ ìˆ˜

{
    "value": 66667,
    "unit": "ëŒ€",
    "confidence": 0.6,
    "method": "bottom-up",
    "decomposition": [
        {
            "step": "1. ì„œìš¸ ì¸êµ¬",
            "concept": "population_seoul",
            "value": 10000000,
            "unit": "ëª…",
            "calculation": "1000ë§Œëª… (í†µê³„ ê¸°ë°˜)",
            "reasoning": "ì„œìš¸ì‹œ ê³µì‹ ì¸êµ¬ í†µê³„"
        },
        {
            "step": "2. 1ì¸ë‹¹ ì—°ê°„ íƒì‹œ ì´ìš©",
            "concept": "taxi_usage_per_capita",
            "value": 20,
            "unit": "íšŒ",
            "calculation": "ì›” 1.5íšŒ x 12ê°œì›” = 20",
            "reasoning": "ëŒ€ì¤‘êµí†µ ì¤‘ì‹¬, ê°€ë” ì´ìš©"
        },
        {
            "step": "3. ì—°ê°„ ì´ ì´ìš© íšŸìˆ˜",
            "concept": "total_taxi_rides",
            "value": 200000000,
            "unit": "íšŒ",
            "calculation": "10000000 x 20 = 200000000",
            "reasoning": "step1 x step2"
        },
        {
            "step": "4. íƒì‹œ 1ëŒ€ë‹¹ ì—°ê°„ ìš´í–‰",
            "concept": "rides_per_taxi",
            "value": 3000,
            "unit": "íšŒ",
            "calculation": "ì¼ 10íšŒ x 300ì¼ = 3000",
            "reasoning": "2êµëŒ€ ê¸°ì¤€"
        },
        {
            "step": "5. ìµœì¢…: í•„ìš” íƒì‹œ ìˆ˜",
            "concept": "total_taxis_needed",
            "value": 66667,
            "unit": "ëŒ€",
            "calculation": "200000000 / 3000 = 66667",
            "reasoning": "step3 / step4"
        }
    ],
    "final_calculation": "step5 = step3 / step4 = 200000000 / 3000 = 66667",
    "calculation_verification": "ê²€ì¦: 10,000,000ëª… x 20íšŒ / 3,000íšŒ = 66,667ëŒ€"
}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
MANDATORY RULES (ì ˆëŒ€ ê·œì¹™):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. "concept" í•„ë“œ - ëª¨ë“  decomposition ë‹¨ê³„ì— í•„ìˆ˜!
   -> ë„ë©”ì¸ íŠ¹í™” ê°œë…ì„ ì˜ì–´ snake_caseë¡œ ëª…ì‹œ
   -> ì˜ˆ: "population_seoul", "taxi_usage_per_capita"

2. "final_calculation" í•„ë“œ - JSON ìµœìƒìœ„ì— í•„ìˆ˜!
   -> decomposition ë§ˆì§€ë§‰ ë‹¨ê³„ ê³„ì‚°ì„ ì‹¤ì œ ìˆ«ìë¡œ ì¬ê²€ì¦
   -> ì˜ˆ: "step5 = step3 / step4 = 200000000 / 3000 = 66667"

3. "calculation_verification" í•„ë“œ - JSON ìµœìƒìœ„ì— í•„ìˆ˜!
   -> ì „ì²´ ê³„ì‚° ê³¼ì • ì¬í™•ì¸
   -> ì˜ˆ: "ê²€ì¦: 10,000,000ëª… x 20íšŒ / 3,000íšŒ = 66,667ëŒ€"

4. ìµœì¢… ì¶”ì •ê°’ = decomposition ë§ˆì§€ë§‰ ë‹¨ê³„ì˜ value
   -> JSONì˜ "value": 66667 = decomposition[-1]["value"]: 66667
   -> ë°˜ì˜¬ë¦¼/ê·¼ì‚¬ì¹˜ ê¸ˆì§€! ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•¨!

5. ë§ˆì§€ë§‰ ë‹¨ê³„ëŠ” ë°˜ë“œì‹œ ìµœì¢… ê³„ì‚° ë‹¨ê³„
   -> "step": "N. ìµœì¢…: [ì¶”ì • ëŒ€ìƒ]"
   -> ì´ ë‹¨ê³„ì˜ valueê°€ ê³§ ìµœì¢… ë‹µ

6. ê° ì¤‘ê°„ ë‹¨ê³„ëŠ” ëª…í™•í•œ ì‚¬ì¹™ì—°ì‚°ìœ¼ë¡œ ì—°ê²°
   -> "calculation": "step3 / step4 = 200000000 / 3000 = 66667"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        # ê°€ìš© ë°ì´í„° ë¬¸ìì—´
        if available:
            available_str = "\n".join([
                f"- {var.name}: {var.value} ({var.source}, confidence: {var.confidence:.0%})"
                for var in available.values()
            ])
        else:
            available_str = "(ì—†ìŒ)"

        prompt = f"""{fast_mode_constraint}{mandatory_header}
{fewshot_example}

ì´ì œ ì‹¤ì œ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}

ê°€ìš©í•œ ë°ì´í„°:
{available_str}

ì„ë¬´:
1. ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ê³„ì‚° ëª¨í˜•ì„ 3-5ê°œ ì œì‹œí•˜ì„¸ìš”.
2. ê° ëª¨í˜•ì€ ë‹¤ë¥¸ ë¶„í•´ ë°©ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”.
3. ê°€ìš©í•œ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì„¸ìš”.
4. Unknown ë³€ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ì„¸ìš”.
5. ê°„ë‹¨í• ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤ (Occam's Razor, ìµœëŒ€ 6ê°œ ë³€ìˆ˜ ê¶Œì¥).

í•„ìˆ˜ ê·œì¹™:
   - ë³€ìˆ˜ëª…ì€ ì˜ë¬¸ìì™€ ì–¸ë”ìŠ¤ì½”ì–´ë§Œ ì‚¬ìš© (ì˜ˆ: monthly_revenue, churn_rate)
   - ìˆœí™˜ ì°¸ì¡° ê¸ˆì§€ (A->B->A êµ¬ì¡° ê¸ˆì§€)
   - ë³€ìˆ˜ ì´ë¦„ ê·œì¹™: [a-zA-Z_][a-zA-Z0-9_]*

ì¶œë ¥ í˜•ì‹ (YAML):
```yaml
models:
  - id: MODEL_001
    formula: "result = A * B * C"
    description: "ì„¤ëª…"
    variables:
      - name: A
        concept: "domain_concept_a"
        description: "ìŒì‹ì  ìˆ˜"
        available: true
      - name: B
        concept: "domain_concept_b"
        description: "ë„ì…ë¥ "
        available: false
      - name: C
        concept: "domain_concept_c"
        description: "ARPU"
        available: false
```

ì²´í¬ë¦¬ìŠ¤íŠ¸ (ë°˜ë“œì‹œ í™•ì¸!):
- [ ] ëª¨ë“  ë³€ìˆ˜ì— "concept" í•„ë“œ ìˆìŒ
- [ ] decomposition[-1]["value"] == JSON["value"]
- [ ] ë§ˆì§€ë§‰ stepì€ "N. ìµœì¢…: [ì¶”ì • ëŒ€ìƒ]"
- [ ] ëª¨ë“  calculationì— ì‹¤ì œ ìˆ«ì í¬í•¨

ì£¼ì˜: YAML í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        return prompt
    
    def _parse_llm_models(
        self,
        llm_output: str,
        depth: int
    ) -> List[FermiModel]:
        """
        LLM ì‘ë‹µ íŒŒì‹± (YAML/JSON ì§€ì›)
        
        v7.8.1: JSON ì¶”ì¶œ ë¡œì§ ê°•í™” (ë²¤ì¹˜ë§ˆí¬ íŒ¨í„´ ì ìš©)
        
        Args:
            llm_output: LLM ì‘ë‹µ
            depth: ê¹Šì´
        
        Returns:
            FermiModel ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. YAML ë¸”ë¡ ì¶”ì¶œ ì‹œë„ (```yaml ... ```)
            yaml_match = re.search(r'```yaml\n(.*?)\n```', llm_output, re.DOTALL)
            
            if yaml_match:
                yaml_str = yaml_match.group(1)
                logger.info(f"{'  ' * depth}        [Parser] YAML ë¸”ë¡ ê°ì§€")
                
                # YAML íŒŒì‹±
                data = yaml.safe_load(yaml_str)
            else:
                # 2. JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„ (```json ... ```)
                content = llm_output
                
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                    logger.info(f"{'  ' * depth}        [Parser] JSON ë¸”ë¡ ê°ì§€ (```json)")
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                    logger.info(f"{'  ' * depth}        [Parser] JSON ë¸”ë¡ ê°ì§€ (```)")
                else:
                    logger.info(f"{'  ' * depth}        [Parser] ì½”ë“œ ë¸”ë¡ ì—†ìŒ, ì „ì²´ íŒŒì‹± ì‹œë„")
                
                # 3. JSON íŒŒì‹± ì‹œë„
                try:
                    data = json.loads(content)
                    logger.info(f"{'  ' * depth}        [Parser] JSON íŒŒì‹± ì„±ê³µ")
                except json.JSONDecodeError:
                    # 4. YAMLë¡œ ì „ì²´ íŒŒì‹± ì‹œë„ (Fallback)
                    logger.info(f"{'  ' * depth}        [Parser] JSON ì‹¤íŒ¨, YAML ì‹œë„")
                    data = yaml.safe_load(llm_output)
            
            # ë°ì´í„° ê²€ì¦
            if not data or 'models' not in data:
                logger.warning(f"{'  ' * depth}        âš ï¸  íŒŒì‹± ì‹¤íŒ¨ (models í‚¤ ì—†ìŒ)")
                logger.debug(f"{'  ' * depth}        ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {llm_output[:200]}...")
                return []
            
            # FermiModel ë³€í™˜
            models = []
            for model_data in data['models']:
                # v7.10.0: concept í•„ë“œ ëˆ„ë½ ê²€ì¦
                raw_variables = model_data.get('variables', [])
                missing_concept = [
                    v.get('name', 'unknown')
                    for v in raw_variables
                    if not v.get('concept')
                ]
                if missing_concept:
                    logger.warning(
                        f"{'  ' * depth}        [Validate] concept í•„ë“œ ëˆ„ë½: "
                        f"{len(missing_concept)}ê°œ ë³€ìˆ˜ ({', '.join(missing_concept[:3])}...)"
                    )

                # ë³€ìˆ˜ íŒŒì‹±
                variables = {}
                for var_data in raw_variables:
                    var_name = var_data.get('name', 'unknown')
                    var_available = var_data.get('available', False)
                    var_concept = var_data.get('concept', '')

                    variables[var_name] = FermiVariable(
                        name=var_name,
                        available=var_available,
                        need_estimate=not var_available,
                        source="llm_generated" if var_available else "",
                        concept=var_concept  # v7.10.0: concept í•„ë“œ ì €ì¥
                    )

                # FermiModel ìƒì„±
                model = FermiModel(
                    model_id=model_data.get('id', f"LLM_MODEL_{len(models)+1}"),
                    name="LLM ìƒì„± ëª¨í˜•",
                    formula=model_data.get('formula', ''),
                    description=model_data.get('description', ''),
                    variables=variables,
                    total_variables=len(variables),
                    unknown_count=sum(1 for v in variables.values() if not v.available)
                )

                models.append(model)
            
            logger.info(f"{'  ' * depth}        [Parser] íŒŒì‹± ì™„ë£Œ: {len(models)}ê°œ ëª¨í˜•")
            return models
        
        except Exception as e:
            logger.error(f"{'  ' * depth}        âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            logger.error(f"{'  ' * depth}        ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            
            # ìƒì„¸ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            logger.error(f"{'  ' * depth}        ì‘ë‹µ ì „ì²´:\n{llm_output}")
            
            # data ë³€ìˆ˜ê°€ ì •ì˜ë˜ì–´ ìˆìœ¼ë©´ ë¡œê¹…
            try:
                if 'data' in locals():
                    logger.error(f"{'  ' * depth}        data íƒ€ì…: {type(data)}")
                    if isinstance(data, dict):
                        logger.error(f"{'  ' * depth}        data í‚¤: {list(data.keys())}")
                    else:
                        logger.error(f"{'  ' * depth}        data ê°’: {str(data)[:200]}")
            except:
                pass
            
            return []
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ (ì¬ê·€ ì¶”ì •)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _step3_check_feasibility(
        self,
        models: List[FermiModel],
        context: Context,
        current_depth: int
    ) -> List[RankedModel]:
        """
        Step 3: ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬ + ì¬ê·€ ì¶”ì •
        
        ê° ëª¨í˜•ì˜ Unknown ë³€ìˆ˜ë¥¼ ì¬ê·€ í˜¸ì¶œë¡œ ì±„ìš°ê¸°
        
        Args:
            models: í›„ë³´ ëª¨í˜•ë“¤
            context: ë§¥ë½
            current_depth: í˜„ì¬ ê¹Šì´
        
        Returns:
            ì ìˆ˜ ìˆœ RankedModel ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"{'  ' * current_depth}  [Step 3] ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬")
        
        ranked = []
        
        for model in models:
            logger.info(f"{'  ' * current_depth}    ëª¨í˜•: {model.model_id}")
            
            # Unknown ë³€ìˆ˜ ì¶”ì • (ì¬ê·€!)
            for var_name, var in model.variables.items():
                if var.need_estimate and not var.estimation_result:
                    logger.info(f"{'  ' * current_depth}      ë³€ìˆ˜ '{var_name}' ì¶”ì • í•„ìš”")
                    
                    # â­ ì¬ê·€ í˜¸ì¶œ!
                    var_result = self._estimate_variable(
                        var_name,
                        context,
                        current_depth + 1
                    )
                    
                    if var_result:
                        var.estimation_result = var_result
                        var.value = var_result.value
                        var.confidence = var_result.confidence
                        var.available = True
                        var.source = f"tier3_recursive_depth_{current_depth + 1}"
                        logger.info(f"{'  ' * current_depth}        âœ… {var.value} (conf: {var.confidence:.2f})")
                    else:
                        logger.warning(f"{'  ' * current_depth}        âŒ ì¶”ì • ì‹¤íŒ¨")
            
            # ëª¨í˜• ì ìˆ˜í™”
            score_result = self._score_model(model, current_depth)
            
            ranked.append(RankedModel(
                rank=0,  # ì •ë ¬ í›„ í• ë‹¹
                model=model,
                score=score_result['total'],
                unknown_score=score_result['unknown'],
                confidence_score=score_result['confidence'],
                complexity_score=score_result['complexity'],
                depth_score=score_result['depth'],
                status=score_result['status'],
                missing=score_result['missing']
            ))
        
        # ì ìˆ˜ ìˆœ ì •ë ¬
        ranked.sort(key=lambda x: x.score, reverse=True)
        
        # Rank í• ë‹¹
        for i, rm in enumerate(ranked, 1):
            rm.rank = i
        
        if ranked:
            logger.info(f"{'  ' * current_depth}    ìµœì„  ëª¨í˜•: {ranked[0].model.model_id} "
                       f"(ì ìˆ˜: {ranked[0].score:.3f})")
        
        return ranked
    
    def _estimate_variable(
        self,
        var_name: str,
        context: Context,
        depth: int
    ) -> Optional[EstimationResult]:
        """
        ë³€ìˆ˜ ì¶”ì • (v7.10.2: ì „ì—­ ë³€ìˆ˜ íšŸìˆ˜ ì œí•œ)

        Fallback ìš°ì„ ìˆœìœ„:
        1. Phase 3 ê°’ ìˆìœ¼ë©´ ìˆ˜ìš© (ë™ì  threshold)
        2. Phase 3 SoftGuide ì¶”ì¶œ (range/distribution í™œìš©)
        3. LLM emergency í˜¸ì¶œ (ìµœí›„ ìˆ˜ë‹¨)
        4. Phase 4 ì¬ê·€ (depth ì œí•œ ë‚´)

        Args:
            var_name: ë³€ìˆ˜ ì´ë¦„
            context: ë§¥ë½
            depth: ê¹Šì´

        Returns:
            EstimationResult ë˜ëŠ” None
        """
        # v7.10.2: ì „ì—­ ë³€ìˆ˜ ì¶”ì • íšŸìˆ˜ ì œí•œ (ì¬ê·€ í­ë°œ ì™„ì „ ë°©ì§€)
        self._total_variable_estimate_count += 1
        
        if self._total_variable_estimate_count > self.max_variable_estimates:
            logger.warning(f"{'  ' * depth}        âš ï¸  ë³€ìˆ˜ ì¶”ì • ì œí•œ ì´ˆê³¼ ({self.max_variable_estimates}íšŒ) â†’ ì¤‘ë‹¨")
            return None
        
        # Contextë¥¼ ì§ˆë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ (v7.5.0)
        question = self._build_contextualized_question(var_name, context)

        logger.info(f"{'  ' * depth}      [Recursive {self._total_variable_estimate_count}/{self.max_variable_estimates}] {question}")

        # v7.10.0: ë™ì  confidence threshold ì‚¬ìš©
        threshold = self._get_current_confidence_threshold()

        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 1. Phase 3 ì‹œë„
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        phase3_result = self.phase3.estimate(question, context)

        if phase3_result:
            # ì‹ ë¢°ë„ê°€ threshold ì´ìƒì´ë©´ ìˆ˜ìš©
            if phase3_result.confidence >= threshold:
                logger.info(f"{'  ' * depth}        Phase 3 ì„±ê³µ (conf: {phase3_result.confidence:.2f} >= {threshold:.2f})")
                return phase3_result

            # ê°’ì´ ìˆìœ¼ë©´ ìˆ˜ìš© (ì¬ê·€ í­ë°œ ë°©ì§€)
            if phase3_result.value is not None and phase3_result.value != 0:
                logger.info(f"{'  ' * depth}        Phase 3 ê°’ ìˆ˜ìš© (conf: {phase3_result.confidence:.2f} < {threshold:.2f}, but value exists)")
                return phase3_result

        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 2. SoftGuide ì¶”ì¶œ ì‹œë„ (v7.10.1 ì‹ ê·œ!)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if phase3_result and hasattr(phase3_result, 'reasoning_detail'):
            soft_guide = phase3_result.reasoning_detail.get('soft_guide')
            if soft_guide:
                value = self._extract_value_from_softguide(soft_guide)
                if value is not None:
                    logger.info(f"{'  ' * depth}        SoftGuide fallback: {value} (range í™œìš©)")

                    return EstimationResult(
                        question=question,
                        value=value,
                        confidence=0.40,  # ë‚®ì€ ì‹ ë¢°ë„
                        phase=3,
                        context=context,
                        reasoning=f"Phase 3 range/distribution ì¤‘ê°„ê°’ ì‚¬ìš©",
                        reasoning_detail={
                            'method': 'softguide_fallback',
                            'original_confidence': phase3_result.confidence,
                            'source': 'phase3_softguide'
                        }
                    )

        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 3. LLM emergency í˜¸ì¶œ (v7.10.1 ì‹ ê·œ!)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if self.llm_mode != 'cursor':
            llm_value = self._llm_emergency_estimate(var_name, context)
            if llm_value is not None:
                logger.info(f"{'  ' * depth}        LLM emergency: {llm_value}")

                return EstimationResult(
                    question=question,
                    value=llm_value,
                    confidence=0.30,  # ë§¤ìš° ë‚®ì€ ì‹ ë¢°ë„
                    phase=4,
                    context=context,
                    reasoning=f"LLM ì§ì ‘ ì¶”ì • (emergency, gpt-4o-mini)",
                    reasoning_detail={
                        'method': 'llm_emergency',
                        'model': 'gpt-4o-mini',
                        'source': 'direct_llm_call'
                    }
                )

        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # 4. Phase 4 ì¬ê·€ (ìµœí›„ ìˆ˜ë‹¨, depth ì œí•œ)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if depth < self.max_depth - 1:  # ë§ˆì§€ë§‰ depthì—ì„œëŠ” ì¬ê·€ ì•ˆ í•¨
            logger.info(f"{'  ' * depth}        Fermi ì¬ê·€ (depth {depth} -> {depth + 1})")

            # ë¶€ëª¨ ë°ì´í„° ì¤€ë¹„ (v7.5.0+)
            parent_data_to_pass = {}

            tier3_result = self._single_estimate_attempt(
                question=question,
                context=context,
                available_data=None,
                depth=depth + 1,
                parent_data=parent_data_to_pass
            )

            if tier3_result:
                return tier3_result

        # ì™„ì „ ì‹¤íŒ¨
        logger.warning(f"{'  ' * depth}        ë³€ìˆ˜ ì¶”ì • ì™„ì „ ì‹¤íŒ¨: {var_name}")
        return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 4: ëª¨í˜• ì‹¤í–‰ (Backtracking)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _phase5_boundary_validation(
        self,
        result: EstimationResult,
        question: str,
        model: Any,
        depth: int
    ):
        """
        Phase 5: Boundary ê²€ì¦ (v7.6.2)
        
        LLM ê¸°ë°˜ ë¹„ì •í˜• ì‚¬ê³ ë¡œ ì¶”ì •ê°’ íƒ€ë‹¹ì„± ê²€ì¦
        
        Args:
            result: ì¶”ì • ê²°ê³¼
            question: ì›ë˜ ì§ˆë¬¸
            model: ì‚¬ìš©ëœ ëª¨í˜•
            depth: ê¹Šì´
        
        Returns:
            BoundaryCheck
        """
        logger.info(f"{'  ' * depth}  [Phase 5] Boundary ê²€ì¦")
        
        try:
            from .boundary_validator import get_boundary_validator
            
            validator = get_boundary_validator(llm_mode=self.llm_mode)
            
            boundary_check = validator.validate(
                question=question,
                estimated_value=result.value,
                unit=result.unit,
                context=result.context,
                formula=model.formula if hasattr(model, 'formula') else ""
            )
            
            return boundary_check
        
        except Exception as e:
            logger.warning(f"{'  ' * depth}  âš ï¸  Boundary ê²€ì¦ ì‹¤íŒ¨: {e}")
            
            # Fallback: í†µê³¼ë¡œ ê°„ì£¼
            from .boundary_validator import BoundaryCheck
            return BoundaryCheck(is_valid=True, reasoning="Boundary ê²€ì¦ ìŠ¤í‚µ")
    
    def _step4_execute(
        self,
        ranked_model: RankedModel,
        depth: int,
        context: Context
    ) -> Optional[EstimationResult]:
        """
        Step 4: ëª¨í˜• ì‹¤í–‰ + í’ˆì§ˆ í‰ê°€ (v7.10.0)

        ì¬ê·€ë¡œ ì±„ìš´ ë³€ìˆ˜ë“¤ì„ backtrackingìœ¼ë¡œ ì¬ì¡°ë¦½
        v7.10.0: í’ˆì§ˆ í‰ê°€ ë° í›„ì²˜ë¦¬ í†µí•©

        Args:
            ranked_model: ì„ íƒëœ ëª¨í˜•
            depth: ê¹Šì´
            context: ë§¥ë½

        Returns:
            EstimationResult (decomposition + í’ˆì§ˆ í‰ê°€ í¬í•¨)
        """
        logger.info(f"{'  ' * depth}  [Step 4] ëª¨í˜• ì‹¤í–‰")

        model = ranked_model.model

        # Step 1: ë³€ìˆ˜ ë°”ì¸ë”© í™•ì¸
        bindings = {}
        for name, var in model.variables.items():
            if var.available and var.value is not None:
                bindings[name] = var.value
            else:
                logger.warning(f"{'  ' * depth}    ë³€ìˆ˜ '{name}' ê°’ ì—†ìŒ")

        if not bindings:
            logger.warning(f"{'  ' * depth}    ë°”ì¸ë”©í•  ë³€ìˆ˜ ì—†ìŒ")
            return None

        logger.info(f"{'  ' * depth}    ë³€ìˆ˜ ë°”ì¸ë”©: {list(bindings.keys())}")

        # Step 2: ê³„ì‚° ì‹¤í–‰
        result_value = self._execute_formula_simple(model.formula, bindings)

        # Step 3: Confidence ì¡°í•© (Geometric Mean)
        confidences = [
            var.confidence
            for var in model.variables.values()
            if var.available and var.confidence > 0
        ]

        if confidences:
            combined_confidence = math.prod(confidences) ** (1 / len(confidences))
        else:
            combined_confidence = 0.5

        logger.info(f"{'  ' * depth}    Confidence: {combined_confidence:.2f}")

        # Step 4: DecompositionTrace ìƒì„±
        decomposition = DecompositionTrace(
            formula=model.formula,
            variables={
                name: var.estimation_result
                for name, var in model.variables.items()
                if var.estimation_result
            },
            calculation_logic=model.description,
            depth=depth,
            decomposition_reasoning=getattr(model, 'selection_reason', '')
        )

        # v7.10.0: decompositionì„ Dict í˜•íƒœë¡œ ë³€í™˜ (í‰ê°€ìš©)
        decomp_list = self._model_to_decomposition_list(model, bindings, result_value)

        # v7.10.0: ì‘ë‹µ Dict êµ¬ì„± (í›„ì²˜ë¦¬/í‰ê°€ìš©)
        response_dict = {
            'value': result_value,
            'unit': '',
            'decomposition': decomp_list,
            'final_calculation': model.formula,
            'calculation_verification': ''
        }

        # v7.10.0: í›„ì²˜ë¦¬ (í•„ìˆ˜ í•„ë“œ ìë™ ìƒì„±, value êµì •)
        response_dict, auto_generated = self._validate_and_postprocess_response(
            response_dict, depth
        )

        # v7.10.0: í’ˆì§ˆ í‰ê°€
        content_score = self._evaluate_content_score(decomp_list, result_value, depth)
        format_score = self._evaluate_format_score(response_dict, decomp_list, auto_generated, depth)

        # ì§ˆë¬¸ ë¬¸ìì—´ êµ¬ì„± (ê°œë…ì  ì¼ê´€ì„± í‰ê°€ìš©)
        question_str = context.domain if context and context.domain else "unknown"
        conceptual_score = self._evaluate_conceptual_coherence(
            question_str,
            decomp_list,
            response_dict.get('final_calculation', ''),
            context,
            depth
        )

        total_quality_score = content_score['score'] + format_score['score'] + conceptual_score['score']
        logger.info(f"{'  ' * depth}    [í’ˆì§ˆ] ë‚´ìš©: {content_score['score']:.1f}/45, "
                   f"í˜•ì‹: {format_score['score']:.1f}/5, "
                   f"ê°œë…: {conceptual_score['score']:.1f}/15 = {total_quality_score:.1f}/65")

        # Step 5: Logic Steps ìƒì„±
        logic_steps = [
            f"ëª¨í˜• ì„ íƒ: {model.formula}",
            f"ë³€ìˆ˜ ë¶„í•´: {model.total_variables}ê°œ",
            f"ë³€ìˆ˜ í™•ë³´: {getattr(model, 'available_count', len(bindings))}ê°œ",
            f"ì¬ê·€ ê¹Šì´: depth {depth}",
            f"ê³„ì‚°: {model.formula}",
            f"ì‹ ë¢°ë„: {combined_confidence:.2f}",
            f"ê²°ê³¼: {result_value}",
            f"í’ˆì§ˆ: {total_quality_score:.1f}/65"
        ]

        # Step 6: EstimationResult ìƒì„±
        result = EstimationResult(
            question=question_str,
            value=response_dict.get('value', result_value),  # v7.10.0: í›„ì²˜ë¦¬ ê²°ê³¼ ì‚¬ìš©
            confidence=combined_confidence,
            phase=4,
            context=context,
            reasoning=f"Fermi ë¶„í•´: {model.description}",
            reasoning_detail={
                'method': 'fermi_decomposition',
                'model_id': model.model_id,
                'formula': model.formula,
                'depth': depth,
                'selection_reason': getattr(model, 'selection_reason', ''),
                'why_this_method': f'Phase 1/2/3 ì‹¤íŒ¨, ì¬ê·€ ë¶„í•´ (depth {depth})',
                'variables': {
                    name: {
                        'value': var.value,
                        'source': var.source,
                        'confidence': var.confidence
                    }
                    for name, var in model.variables.items()
                    if var.available
                },
                # v7.10.0: í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ì¶”ê°€
                'quality_evaluation': {
                    'content_score': content_score,
                    'format_score': format_score,
                    'conceptual_score': conceptual_score,
                    'total_score': total_quality_score,
                    'auto_generated_fields': auto_generated
                }
            },
            logic_steps=logic_steps,
            decomposition=decomposition,
            fermi_model=model
        )
        
        return result
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ëª¨í˜• ì ìˆ˜í™”
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _score_model(
        self,
        model: FermiModel,
        depth: int
    ) -> Dict[str, Any]:
        """
        ëª¨í˜• ì ìˆ˜í™” (4ê°œ ê¸°ì¤€)
        
        ì„¤ê³„: fermi_model_search.yaml Line 725-810
        
        ê¸°ì¤€:
        1. Unknown count (50%): ì ì„ìˆ˜ë¡ ì¢‹ìŒ
        2. Confidence (30%): ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        3. Complexity (20%): ê°„ë‹¨í• ìˆ˜ë¡ ì¢‹ìŒ
        4. Depth (10% bonus): ì–•ì„ìˆ˜ë¡ ì¢‹ìŒ
        
        Returns:
            {
                'unknown': float,
                'confidence': float,
                'complexity': float,
                'depth': float,
                'total': float,
                'status': str,
                'missing': List[str]
            }
        """
        # 1. Unknown count (50%)
        if model.total_variables > 0:
            filled = sum(1 for v in model.variables.values() if v.available)
            model.available_count = filled
            unknown_ratio = filled / model.total_variables
        else:
            unknown_ratio = 0.0
        
        unknown_score = unknown_ratio * 0.5
        
        # 2. Confidence (30%)
        confidences = [
            v.confidence for v in model.variables.values()
            if v.available and v.confidence > 0
        ]
        
        if confidences:
            avg_confidence = math.prod(confidences) ** (1 / len(confidences))  # Geometric mean
        else:
            avg_confidence = 0.0
        
        confidence_score = avg_confidence * 0.3
        
        # 3. Complexity (20%)
        var_count = model.total_variables
        
        complexity_map = {
            1: 1.0, 2: 1.0,
            3: 0.9, 4: 0.7,
            5: 0.5, 6: 0.3,
            7: 0.2, 8: 0.15,
            9: 0.10, 10: 0.05
        }
        
        complexity = complexity_map.get(var_count, 0.0)
        complexity_score = complexity * 0.2
        
        # 4. Depth (10% bonus)
        depth_penalties = {0: 1.0, 1: 0.8, 2: 0.6, 3: 0.4, 4: 0.2}
        depth_penalty = depth_penalties.get(depth, 0.2)
        depth_score = depth_penalty * 0.1
        
        # ì´ì 
        total = unknown_score + confidence_score + complexity_score + depth_score
        
        # ìƒíƒœ íŒë‹¨
        missing = [
            name for name, var in model.variables.items()
            if not var.available
        ]
        
        if not missing:
            status = "feasible"
        elif len(missing) <= 2:
            status = "partial"
        else:
            status = "infeasible"
        
        return {
            'unknown': unknown_score,
            'confidence': confidence_score,
            'complexity': complexity_score,
            'depth': depth_score,
            'total': total,
            'status': status,
            'missing': missing
        }
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ë™ì  Fallback (v7.10.1: íŒ¨í„´ ë§¤ì¹­ ì œê±°)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    def _extract_value_from_softguide(
        self,
        soft_guide: 'SoftGuide'
    ) -> Optional[float]:
        """
        SoftGuideì—ì„œ ëŒ€í‘œê°’ ì¶”ì¶œ (v7.10.1)

        Phase 3ê°€ range/distributionì„ ì œê³µí–ˆì§€ë§Œ confidenceê°€ ë‚®ì„ ë•Œ
        ê·¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëŒ€í‘œê°’ ì¶”ì¶œ

        Args:
            soft_guide: Phase 3ì˜ SoftGuide ê²°ê³¼

        Returns:
            ëŒ€í‘œê°’ (ì¤‘ì•™ê°’ > í‰ê·  > range ì¤‘ê°„ê°’) ë˜ëŠ” None
        """
        if not soft_guide:
            return None

        dist = soft_guide.distribution

        # 1. Distributionì´ ìˆìœ¼ë©´ í†µê³„ê°’ ìš°ì„ 
        if dist:
            # ì¤‘ì•™ê°’ ìš°ì„  (Power Law ë“±ì— ì•ˆì „)
            if dist.percentiles and 'p50' in dist.percentiles:
                logger.debug(f"        SoftGuide: p50 = {dist.percentiles['p50']}")
                return dist.percentiles['p50']

            # í‰ê· ê°’ (ì •ê·œë¶„í¬)
            if dist.mean is not None:
                logger.debug(f"        SoftGuide: mean = {dist.mean}")
                return dist.mean

        # 2. suggested_rangeê°€ ìˆìœ¼ë©´ ì¤‘ê°„ê°’
        if soft_guide.suggested_range:
            min_val, max_val = soft_guide.suggested_range
            mid_val = (min_val + max_val) / 2
            logger.debug(f"        SoftGuide: range mid = {mid_val}")
            return mid_val

        # 3. typical_value
        if soft_guide.typical_value is not None:
            logger.debug(f"        SoftGuide: typical = {soft_guide.typical_value}")
            return soft_guide.typical_value

        return None

    def _llm_emergency_estimate(
        self,
        var_name: str,
        context: Context
    ) -> Optional[float]:
        """
        LLM ê¸´ê¸‰ í˜¸ì¶œ (v7.10.1: ìµœí›„ ìˆ˜ë‹¨)

        ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ LLMì—ê²Œ "ìˆ«ìë§Œ" ìš”ì²­
        - ë¹ ë¦„: max_tokens=10
        - ì €ë ´: gpt-4o-mini
        - ê°„ë‹¨: JSON íŒŒì‹± ë¶ˆí•„ìš”

        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½

        Returns:
            ì¶”ì •ê°’ ë˜ëŠ” None
        """
        if self.llm_mode == 'cursor':
            logger.debug("        LLM emergency: Cursor ëª¨ë“œì—ì„œ ë¶ˆê°€")
            return None

        # Context ì •ë³´ êµ¬ì„±
        context_str = context.domain if context and context.domain else "ì¼ë°˜"
        if context and context.region:
            context_str += f", {context.region}"

        prompt = f"""ë‹¤ìŒ ë³€ìˆ˜ì˜ ëŒ€ëµì ì¸ ê°’ì„ ìˆ«ìë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:

**ë³€ìˆ˜**: {var_name}
**ë§¥ë½**: {context_str}

**ì¤‘ìš”**: ìˆ«ìë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì„¤ëª… ì—†ìŒ)
- ë¹„ìœ¨ì´ë©´ ì†Œìˆ˜ (ì˜ˆ: 0.05)
- ê°œìˆ˜ë©´ ì •ìˆ˜ (ì˜ˆ: 100)
- ê¸ˆì•¡ì´ë©´ ìˆ«ì (ì˜ˆ: 50000)

ë‹µë³€:"""

        try:
            client = self.llm_client
            if not client:
                return None

            response = client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥´ê³  ì €ë ´
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,  # ìˆ«ìë§Œ
                temperature=0.3
            )

            text = response.choices[0].message.content.strip()
            
            # ìˆ«ì íŒŒì‹± ì‹œë„
            value = float(text)
            logger.debug(f"        LLM emergency: {value}")
            return value

        except Exception as e:
            logger.debug(f"        LLM emergency ì‹¤íŒ¨: {e}")
            return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì•ˆì „ ì¥ì¹˜
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _detect_circular(self, question: str) -> bool:
        """
        ìˆœí™˜ ì˜ì¡´ì„± ê°ì§€
        
        Call stackì— ë™ì¼ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ìˆœí™˜
        
        ì˜ˆ:
            depth 0: "ì‹œì¥ ê·œëª¨ëŠ”?"
            depth 1: "ì ìœ ìœ¨ì€?"
            depth 2: "ì‹œì¥ ê·œëª¨ëŠ”?"  # â† ìˆœí™˜!
        
        Args:
            question: ì§ˆë¬¸
        
        Returns:
            True: ìˆœí™˜ ê°ì§€
            False: ì •ìƒ
        """
        normalized = question.lower().strip()
        
        for past_question in self.call_stack:
            if past_question.lower().strip() == normalized:
                logger.warning(f"    ìˆœí™˜ ê°ì§€: '{question}'")
                logger.warning(f"    Call stack: {self.call_stack}")
                return True
        
        return False
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ìœ í‹¸ë¦¬í‹°
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _build_contextualized_question(
        self,
        var_name: str,
        context: Context
    ) -> str:
        """
        Contextë¥¼ í¬í•¨í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ìƒì„± (v7.5.0)
        
        ë³€ìˆ˜ ì´ë¦„ë§Œìœ¼ë¡œëŠ” ì• ë§¤í•˜ë¯€ë¡œ, ë§¥ë½ì„ ëª…ì‹œì ìœ¼ë¡œ í¬í•¨
        
        Args:
            var_name: ë³€ìˆ˜ ì´ë¦„ (ì˜ˆ: "arpu", "churn_rate")
            context: ë§¥ë½
        
        Returns:
            êµ¬ì²´í™”ëœ ì§ˆë¬¸ ë¬¸ìì—´
        
        Example:
            >>> _build_contextualized_question("arpu", Context(domain="B2B_SaaS", region="í•œêµ­"))
            >>> # "B2B SaaS í•œêµ­ ì‹œì¥ì˜ ARPUëŠ”?"
        """
        # ë³€ìˆ˜ ì´ë¦„ ì •ë¦¬ (snake_case â†’ ë„ì–´ì“°ê¸°)
        readable_var = var_name.replace('_', ' ').upper()
        
        # Context ìš”ì†Œ ìˆ˜ì§‘
        context_parts = []
        
        if context.domain and context.domain != "General":
            context_parts.append(context.domain.replace('_', ' '))
        
        if context.region:
            context_parts.append(context.region)
        
        if context.time_period:
            context_parts.append(context.time_period)
        
        # ì§ˆë¬¸ ì¡°ë¦½
        if context_parts:
            context_str = " ".join(context_parts)
            question = f"{context_str} ì‹œì¥ì˜ {readable_var}ëŠ”?"
        else:
            question = f"{readable_var}ëŠ”?"
        
        return question
    
    def _execute_formula_simple(
        self,
        formula: str,
        bindings: Dict[str, float]
    ) -> float:
        """
        ìˆ˜ì‹ ì‹¤í–‰ (ì•ˆì „í•œ ë²„ì „)
        
        ì§€ì› ì—°ì‚°: +, -, *, /, ê´„í˜¸
        ê¸ˆì§€: eval() (ë³´ì•ˆ ìœ„í—˜)
        
        Args:
            formula: ìˆ˜ì‹ (ì˜ˆ: "ltv = arpu / churn_rate")
            bindings: ë³€ìˆ˜ ê°’ (ì˜ˆ: {"arpu": 80000, "churn_rate": 0.05})
        
        Returns:
            ê³„ì‚° ê²°ê³¼
        """
        try:
            # ìˆ˜ì‹ì—ì„œ ê²°ê³¼ ë³€ìˆ˜ ì œê±° (ì˜ˆ: "ltv = ..." â†’ "...")
            if '=' in formula:
                parts = formula.split('=', 1)
                if len(parts) == 2:
                    formula = parts[1].strip()
            
            # Ã— â†’ * ë³€í™˜ (ìˆ˜í•™ ê¸°í˜¸ ì •ê·œí™”)
            expr = formula.replace('Ã—', '*').replace('Ã·', '/')
            
            # ë³€ìˆ˜ ì¹˜í™˜
            for var_name, var_value in bindings.items():
                # ë³€ìˆ˜ ì´ë¦„ì„ ê°’ìœ¼ë¡œ ì¹˜í™˜
                expr = expr.replace(var_name, str(var_value))
            
            # ë³€ìˆ˜ëª…ì€ ì•ˆì „: [a-zA-Z_][a-zA-Z0-9_]* íŒ¨í„´
            # í•˜ì§€ë§Œ ì¹˜í™˜ í›„ì—ëŠ” ìˆ«ìì™€ ì—°ì‚°ìë§Œ ë‚¨ì•„ì•¼ í•¨
            # ë”°ë¼ì„œ ì¹˜í™˜ ê²€ì¦ì„ ê°•í™”
            
            # ì¹˜í™˜ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ë³€ìˆ˜ëª…ì´ ë‚¨ì•„ìˆìœ¼ë©´ ê²½ê³ )
            import re
            remaining_vars = re.findall(r'[a-zA-Z_][a-zA-Z0-9_]*', expr)
            if remaining_vars:
                logger.warning(f"    âš ï¸  ì¹˜í™˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜: {remaining_vars}")
                logger.warning(f"    ìˆ˜ì‹: {formula}")
                logger.warning(f"    bindings: {list(bindings.keys())}")
                # Fallback: ê³±ì…ˆ
                return math.prod(bindings.values()) if bindings else 0.0
            
            # ì•ˆì „í•œ ê³„ì‚° (í—ˆìš© ë¬¸ìë§Œ: ìˆ«ì, ì—°ì‚°ì, ê´„í˜¸, ê³µë°±)
            allowed_chars = set('0123456789.+-*/() ')
            if not all(c in allowed_chars for c in expr):
                logger.warning(f"    âš ï¸  ìˆ˜ì‹ì— í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì: {formula}")
                logger.warning(f"    ì¹˜í™˜ í›„: {expr}")
                # Fallback: ê³±ì…ˆ
                return math.prod(bindings.values()) if bindings else 0.0
            
            # ê³„ì‚° ì‹¤í–‰ (ì œí•œì  eval - ìˆ«ìì™€ ì—°ì‚°ìë§Œ)
            result = eval(expr, {"__builtins__": {}}, {})
            
            return float(result)
        
        except Exception as e:
            logger.warning(f"    âš ï¸  ìˆ˜ì‹ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            logger.warning(f"    Fallback: ê³±ì…ˆ ì‚¬ìš©")
            
            # Fallback: ê³±ì…ˆ
            if bindings:
                return math.prod(bindings.values())
            return 0.0
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 2b: ë°˜ë³µ ê°œì„  (ë³€ìˆ˜ ì¬ê²€ìƒ‰)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _phase2b_refine_with_data_search(
        self,
        models: List[FermiModel],
        question: str,
        context: Context,
        depth: int
    ) -> List[FermiModel]:
        """
        Step 2b: LLM ì œì•ˆ ë³€ìˆ˜ì— ëŒ€í•œ ë°ì´í„° ì¬ê²€ìƒ‰
        
        ë°˜ë³µ ìµœëŒ€ 2íšŒ:
        - 1íšŒì°¨: Unknown ë³€ìˆ˜ ê²€ìƒ‰
        - 2íšŒì°¨: ì—¬ì „íˆ Unknownì¸ ë³€ìˆ˜ ê²€ìƒ‰
        - ìƒˆ ë°œê²¬ ì—†ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        
        Args:
            models: LLM ìƒì„± ëª¨í˜•ë“¤
            question: ì§ˆë¬¸
            context: ë§¥ë½
            depth: ê¹Šì´
        
        Returns:
            ê°œì„ ëœ ëª¨í˜•ë“¤
        """
        max_iterations = 2
        iteration = 0
        
        while iteration < max_iterations:
            # 1. Unknown ë³€ìˆ˜ ì¶”ì¶œ
            unknown_vars = set()
            for model in models:
                for var_name, var in model.variables.items():
                    if not var.available and var.need_estimate:
                        unknown_vars.add(var_name)
            
            if not unknown_vars:
                break  # ëª¨ë‘ available
            
            logger.info(f"{'  ' * depth}  [Refine {iteration+1}] Unknown ë³€ìˆ˜: {len(unknown_vars)}ê°œ")
            
            # 2. Unknown ë³€ìˆ˜ ì¬ê²€ìƒ‰
            newly_found = {}
            for var_name in unknown_vars:
                var_data = self._search_for_variable(var_name, question, context)
                if var_data:
                    newly_found[var_name] = var_data
                    logger.info(f"{'  ' * depth}    âœ… {var_name} = {var_data.value} (conf: {var_data.confidence:.2f})")
            
            if not newly_found:
                logger.info(f"{'  ' * depth}  [Refine {iteration+1}] ìƒˆ ë°œê²¬ ì—†ìŒ â†’ ì¢…ë£Œ")
                break  # ë” ì´ìƒ ë°œê²¬ ì—†ìŒ
            
            # 3. ëª¨í˜• ì—…ë°ì´íŠ¸
            for model in models:
                for var_name, var_data in newly_found.items():
                    if var_name in model.variables:
                        var = model.variables[var_name]
                        var.available = True
                        var.value = var_data.value
                        var.confidence = var_data.confidence
                        var.source = var_data.source
                        var.need_estimate = False
                        model.unknown_count = max(0, model.unknown_count - 1)
            
            iteration += 1
            logger.info(f"{'  ' * depth}  [Refine {iteration}] {len(newly_found)}ê°œ ë³€ìˆ˜ ë°œê²¬")
        
        return models
    
    def _search_for_variable(
        self,
        var_name: str,
        question: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ ë°ì´í„° ê²€ìƒ‰
        
        ìˆœì„œ:
        1. RAG ê²€ìƒ‰
        2. Phase 3 Source
        3. Context ìƒìˆ˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            question: ì›ë˜ ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë°ì´í„° ë˜ëŠ” None
        """
        # 1. RAG ê²€ìƒ‰
        result = self._search_rag_for_variable(var_name, context)
        if result:
            return result
        
        # 2. Phase 3 Source
        result = self._query_phase3_for_variable(var_name, context)
        if result:
            return result
        
        # 3. Context ìƒìˆ˜
        result = self._get_context_constant(var_name, context)
        if result:
            return result
        
        return None
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # í—¬í¼ ë©”ì„œë“œ: ë°ì´í„° ê²€ìƒ‰
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _search_rag_benchmarks(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        RAGì—ì„œ ê´€ë ¨ ë²¤ì¹˜ë§ˆí¬/ìƒìˆ˜ ê²€ìƒ‰
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ë“¤
        """
        results = {}
        
        if not HAS_CHROMA:
            return results
        
        try:
            # Chroma í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            embeddings = OpenAIEmbeddings(
                model=settings.embedding_model,
                openai_api_key=settings.openai_api_key
            )
            
            # ê²€ìƒ‰í•  collectionë“¤
            collection_names = [
                "market_benchmarks",
                "system_knowledge"
            ]
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = f"{context.domain} {question}" if context.domain else question
            
            for collection_name in collection_names:
                try:
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory=str(settings.chroma_persist_dir)
                    )
                    
                    # RAG ê²€ìƒ‰ (top 3)
                    docs = vectorstore.similarity_search(search_query, k=3)
                    
                    # ë©”íƒ€ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ì¶”ì¶œ
                    for doc in docs:
                        metadata = doc.metadata
                        
                        # ë³€ìˆ˜ëª…ê³¼ ê°’ì´ ìˆëŠ” ê²½ìš°
                        if 'variable_name' in metadata and 'value' in metadata:
                            var_name = metadata['variable_name']
                            var_value = metadata['value']
                            
                            if var_name not in results:
                                results[var_name] = FermiVariable(
                                    name=var_name,
                                    value=var_value,
                                    available=True,
                                    source=f"rag_{collection_name}",
                                    confidence=metadata.get('confidence', 0.8),
                                    description=doc.page_content[:100]
                                )
                
                except Exception as e:
                    # Collection ì—†ìœ¼ë©´ ë¬´ì‹œ
                    continue
        
        except Exception as e:
            logger.warning(f"    âš ï¸  RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _query_phase3_sources(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        Phase 3 Sourceì—ì„œ ë°ì´í„° ì¡°íšŒ
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ë“¤
        """
        results = {}
        
        try:
            # Phase 3ìœ¼ë¡œ ì§ì ‘ ì¶”ì • ì‹œë„ (ì‹ ë¢°ë„ ë†’ì€ ê²ƒë§Œ)
            phase3_result = self.phase3.estimate(question, context)
            
            if phase3_result and phase3_result.confidence >= 0.80:
                # ì§ˆë¬¸ì—ì„œ ë³€ìˆ˜ëª… ì¶”ì¶œ ì‹œë„
                var_name = self._extract_var_name_from_question(question)
                
                if var_name:
                    # v7.10.0: sources ì†ì„± ëŒ€ì‹  method ë˜ëŠ” judgment_strategy ì‚¬ìš©
                    source_name = 'unknown'
                    if hasattr(phase3_result, 'judgment_strategy') and phase3_result.judgment_strategy:
                        source_name = phase3_result.judgment_strategy
                    elif hasattr(phase3_result, 'method') and phase3_result.method:
                        source_name = phase3_result.method

                    results[var_name] = FermiVariable(
                        name=var_name,
                        value=phase3_result.value,
                        available=True,
                        source=f"phase3_{source_name}",
                        confidence=phase3_result.confidence,
                        description=phase3_result.reasoning_detail.get('method', '') if hasattr(phase3_result, 'reasoning_detail') else ''
                    )
        
        except Exception as e:
            logger.warning(f"    âš ï¸  Phase 3 ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _extract_var_name_from_question(self, question: str) -> Optional[str]:
        """
        ì§ˆë¬¸ì—ì„œ ë³€ìˆ˜ëª… ì¶”ì¶œ
        
        ì˜ˆ: "í•œêµ­ ì¸êµ¬ëŠ”?" â†’ "korea_population"
        """
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­
        keywords_map = {
            'ì¸êµ¬': 'population',
            'ì†ë„': 'speed',
            'churn': 'churn_rate',
            'arpu': 'arpu',
            'ltv': 'ltv',
            'ê±°ë¦¬': 'distance'
        }
        
        for keyword, var_name in keywords_map.items():
            if keyword in question.lower():
                return var_name
        
        # ê¸°ë³¸ê°’: ì§ˆë¬¸ì˜ ì²« ë‹¨ì–´
        words = question.replace('?', '').split()
        if words:
            return words[0].lower()
        
        return None
    
    def _extract_context_constants(
        self,
        question: str,
        context: Context
    ) -> Dict[str, FermiVariable]:
        """
        Contextì—ì„œ ìëª…í•œ ìƒìˆ˜ ì¶”ì¶œ
        
        Args:
            question: ì§ˆë¬¸
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ìƒìˆ˜ë“¤
        """
        results = {}
        
        # Domainë³„ ìƒìˆ˜
        if context.domain == "transportation":
            # ë¬¼ë¦¬ ìƒìˆ˜
            if "ì¤‘ë ¥" in question or "gravity" in question.lower():
                results['gravity'] = FermiVariable(
                    name='gravity',
                    value=9.8,
                    available=True,
                    source="physical_constant",
                    confidence=1.0
                )
        
        # Regionë³„ ìƒìˆ˜
        if context.region == "South_Korea":
            if "ì¸êµ¬" in question or "population" in question.lower():
                results['korea_population'] = FermiVariable(
                    name='korea_population',
                    value=51_000_000,
                    available=True,
                    source="statistical_constant",
                    confidence=0.95,
                    description="í•œêµ­ ì¸êµ¬ (2024)"
                )
        
        return results
    
    def _search_rag_for_variable(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ RAG ê²€ìƒ‰
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë˜ëŠ” None
        """
        if not HAS_CHROMA:
            return None
        
        try:
            # ë³€ìˆ˜ëª…ì„ ìì—°ì–´ë¡œ ë³€í™˜
            query_text = self._var_name_to_natural_language(var_name, context)
            
            # RAG ê²€ìƒ‰
            embeddings = OpenAIEmbeddings(
                model=settings.embedding_model,
                openai_api_key=settings.openai_api_key
            )
            
            for collection_name in ["market_benchmarks", "system_knowledge"]:
                try:
                    vectorstore = Chroma(
                        collection_name=collection_name,
                        embedding_function=embeddings,
                        persist_directory=str(settings.chroma_persist_dir)
                    )
                    
                    docs = vectorstore.similarity_search(query_text, k=1)
                    
                    if docs:
                        doc = docs[0]
                        metadata = doc.metadata
                        
                        if 'value' in metadata:
                            return FermiVariable(
                                name=var_name,
                                value=metadata['value'],
                                available=True,
                                source=f"rag_{collection_name}",
                                confidence=metadata.get('confidence', 0.75),
                                description=doc.page_content[:100]
                            )
                
                except Exception:
                    continue
        
        except Exception as e:
            logger.debug(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨ ({var_name}): {e}")
        
        return None
    
    def _var_name_to_natural_language(self, var_name: str, context: Context) -> str:
        """
        ë³€ìˆ˜ëª…ì„ ìì—°ì–´ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ë³€í™˜
        
        ì˜ˆ: "speed" + "transportation" â†’ "êµí†µìˆ˜ë‹¨ í‰ê·  ì†ë„"
        """
        # ë³€ìˆ˜ëª… ì •ê·œí™”
        var_lower = var_name.lower().replace('_', ' ')
        
        # Domain ê¸°ë°˜ ë³€í™˜
        if context.domain:
            return f"{context.domain} {var_lower}"
        
        return var_lower
    
    def _query_phase3_for_variable(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ Phase 3 Source ì¡°íšŒ
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ë³€ìˆ˜ ë˜ëŠ” None
        """
        try:
            # ë³€ìˆ˜ëª…ì„ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
            question = self._build_contextualized_question(var_name, context)
            
            # Phase 3 ì¡°íšŒ
            phase3_result = self.phase3.estimate(question, context)
            
            if phase3_result and phase3_result.confidence >= 0.75:
                return FermiVariable(
                    name=var_name,
                    value=phase3_result.value,
                    available=True,
                    source=f"phase3_{phase3_result.sources[0] if phase3_result.sources else 'source'}",
                    confidence=phase3_result.confidence,
                    description=phase3_result.reasoning_detail.get('method', '')
                )
        
        except Exception as e:
            logger.debug(f"Phase 3 ì¡°íšŒ ì‹¤íŒ¨ ({var_name}): {e}")
        
        return None
    
    def _get_context_constant(
        self,
        var_name: str,
        context: Context
    ) -> Optional[FermiVariable]:
        """
        íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ Context ìƒìˆ˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë°œê²¬ëœ ìƒìˆ˜ ë˜ëŠ” None
        """
        # Domain ê¸°ë°˜ ìƒìˆ˜ ë§¤ì¹­
        var_lower = var_name.lower()
        
        # Transportation domain
        if context.domain == "transportation":
            # ì†ë„ ê´€ë ¨ ë³€ìˆ˜
            if "speed" in var_lower or "ì†ë„" in var_lower or "velocity" in var_lower:
                return FermiVariable(
                    name=var_name,
                    value=130,
                    available=True,
                    source="context_benchmark",
                    confidence=0.85,
                    description="KTX í‰ê·  ì†ë„ (km/h, ì •ì°¨ í¬í•¨)"
                )
        
        # South Korea region
        if context.region == "South_Korea":
            # ì¸êµ¬ ê´€ë ¨
            if "population" in var_lower or "ì¸êµ¬" in var_lower:
                return FermiVariable(
                    name=var_name,
                    value=51_000_000,
                    available=True,
                    source="context_constant",
                    confidence=0.95,
                    description="í•œêµ­ ì¸êµ¬ (2024)"
                )
            
            # ê±°ë¦¬ ê´€ë ¨ (ì£¼ìš” ë„ì‹œ)
            if "seoul" in var_lower and "busan" in var_lower:
                if "distance" in var_lower or "ê±°ë¦¬" in var_lower:
                    return FermiVariable(
                        name=var_name,
                        value=325,
                        available=True,
                        source="context_constant",
                        confidence=1.0,
                        description="ì„œìš¸-ë¶€ì‚° ê±°ë¦¬ (km)"
                    )
        
        return None
    
    def _build_contextualized_question(
        self,
        var_name: str,
        context: Context
    ) -> str:
        """
        ë³€ìˆ˜ëª…ì„ ë§¥ë½ì´ í¬í•¨ëœ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
        
        Args:
            var_name: ë³€ìˆ˜ëª…
            context: ë§¥ë½
        
        Returns:
            ë§¥ë½ í¬í•¨ ì§ˆë¬¸
        """
        # Domain ê¸°ë°˜ ì§ˆë¬¸ ìƒì„±
        if context.domain:
            return f"{context.domain}ì—ì„œ {var_name}ëŠ” ì–¼ë§ˆì¸ê°€?"
        
        # ê¸°ë³¸ ì§ˆë¬¸
        return f"{var_name}ëŠ” ì–¼ë§ˆì¸ê°€?"

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # ì‘ë‹µ í›„ì²˜ë¦¬ ë° í‰ê°€ (v7.10.0: ë²¤ì¹˜ë§ˆí¬ íŒ¨í„´ ì´ì‹)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    def _model_to_decomposition_list(
        self,
        model: FermiModel,
        bindings: Dict[str, float],
        result_value: float
    ) -> List[Dict]:
        """
        FermiModelì„ í‰ê°€ìš© decomposition ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (v7.10.0)

        Args:
            model: FermiModel ê°ì²´
            bindings: ë³€ìˆ˜ ë°”ì¸ë”© {name: value}
            result_value: ìµœì¢… ê³„ì‚° ê²°ê³¼

        Returns:
            decomposition ë¦¬ìŠ¤íŠ¸ (í‰ê°€ í•¨ìˆ˜ìš©)
        """
        decomp_list = []
        step_num = 1

        for var_name, var in model.variables.items():
            if var.available and var.value is not None:
                decomp_list.append({
                    'step': f"{step_num}. {var_name}",
                    'concept': var_name,
                    'value': var.value,
                    'unit': '',
                    'calculation': f"{var.value} ({var.source})",
                    'reasoning': var.description or var.source
                })
                step_num += 1

        # ë§ˆì§€ë§‰ ë‹¨ê³„: ìµœì¢… ê³„ì‚°
        decomp_list.append({
            'step': f"{step_num}. ìµœì¢…: ê³„ì‚° ê²°ê³¼",
            'concept': 'final_result',
            'value': result_value,
            'unit': '',
            'calculation': model.formula,
            'reasoning': f"ìˆ˜ì‹ ì ìš©: {model.formula}"
        })

        return decomp_list

    def _validate_and_postprocess_response(
        self,
        response: Dict,
        depth: int = 0
    ) -> Tuple[Dict, List[str]]:
        """
        ì‘ë‹µ ê²€ì¦ ë° í›„ì²˜ë¦¬ (v7.10.0)

        1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        2. ëˆ„ë½ í•„ë“œ ìë™ ìƒì„± (ì¶”ì )
        3. ë§ˆì§€ë§‰ ë‹¨ê³„ value = ìµœìƒìœ„ value ê²€ì¦

        Args:
            response: LLM ì‘ë‹µ Dict
            depth: ë¡œê·¸ ê¹Šì´

        Returns:
            (ì²˜ë¦¬ëœ ì‘ë‹µ, ìë™ ìƒì„±ëœ í•„ë“œ ëª©ë¡)
        """
        auto_generated = []
        decomp = response.get('decomposition', [])

        # v7.10.0: í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ê²€ì¦ (ê²½ê³  ë¡œê·¸)
        if not response.get('final_calculation'):
            logger.warning(f"{'  ' * depth}  [Validate] final_calculation í•„ë“œ ëˆ„ë½!")
        if not response.get('calculation_verification'):
            logger.warning(f"{'  ' * depth}  [Validate] calculation_verification í•„ë“œ ëˆ„ë½!")

        # v7.10.0: decomposition concept í•„ë“œ ê²€ì¦
        if decomp:
            missing_concept_steps = [
                step.get('step', f"step{i+1}")
                for i, step in enumerate(decomp)
                if not step.get('concept')
            ]
            if missing_concept_steps:
                logger.warning(
                    f"{'  ' * depth}  [Validate] concept í•„ë“œ ëˆ„ë½: "
                    f"{len(missing_concept_steps)}ê°œ ë‹¨ê³„ ({', '.join(missing_concept_steps[:3])}...)"
                )

        # 1. final_calculation ìë™ ìƒì„±
        if not response.get('final_calculation') and decomp and len(decomp) > 0:
            last_step = decomp[-1]
            if last_step.get('calculation'):
                response['final_calculation'] = f"Auto: {last_step['calculation']}"
                auto_generated.append('final_calculation')
                logger.info(f"{'  ' * depth}  [PostProcess] final_calculation ìë™ ìƒì„±")

        # 2. calculation_verification ìë™ ìƒì„±
        if not response.get('calculation_verification') and decomp and len(decomp) > 0:
            result, msg = self._auto_verify_calculation(decomp, response.get('value', 0))
            if result is not None:
                response['calculation_verification'] = f"Auto: {msg}"
                auto_generated.append('calculation_verification')
                logger.info(f"{'  ' * depth}  [PostProcess] calculation_verification ìë™ ìƒì„±")

        # 3. ë§ˆì§€ë§‰ ë‹¨ê³„ value ì¼ì¹˜ ê²€ì¦ ë° êµì •
        if decomp and len(decomp) > 0:
            last_value = decomp[-1].get('value')
            top_value = response.get('value')

            if last_value is not None and top_value is not None:
                if isinstance(last_value, (int, float)) and isinstance(top_value, (int, float)):
                    if abs(last_value - top_value) > 0.01 * max(abs(last_value), abs(top_value), 1):
                        logger.warning(f"{'  ' * depth}  [PostProcess] value ë¶ˆì¼ì¹˜: "
                                      f"decomposition[-1]={last_value} vs top={top_value}")
                        # ë§ˆì§€ë§‰ ë‹¨ê³„ ê°’ìœ¼ë¡œ êµì •
                        response['value'] = last_value
                        auto_generated.append('value_corrected')
                        logger.info(f"{'  ' * depth}  [PostProcess] valueë¥¼ ë§ˆì§€ë§‰ ë‹¨ê³„ ê°’ìœ¼ë¡œ êµì •: {last_value}")

        return response, auto_generated

    def _auto_verify_calculation(
        self,
        decomp: List[Dict],
        final_value: float
    ) -> Tuple[Optional[float], str]:
        """
        ë¶„í•´ ê°’ë“¤ë¡œ ìµœì¢…ê°’ ìë™ ê³„ì‚° ì‹œë„ (v7.10.0)

        Args:
            decomp: decomposition ë¦¬ìŠ¤íŠ¸
            final_value: ìµœìƒìœ„ value

        Returns:
            (ê³„ì‚°ëœ ê°’, ì„¤ëª… ë©”ì‹œì§€) ë˜ëŠ” (None, ì—ëŸ¬ ë©”ì‹œì§€)
        """
        if not isinstance(decomp, list) or len(decomp) < 2:
            return None, "ë‹¨ê³„ ë¶€ì¡±"

        values = [
            step.get('value', 0)
            for step in decomp
            if isinstance(step.get('value'), (int, float))
        ]

        if len(values) < 2:
            return None, "ìœ íš¨í•œ ê°’ ë¶€ì¡±"

        results = []

        # 1. ë§ˆì§€ë§‰ ë‹¨ê³„
        if values[-1] > 0 and final_value > 0:
            error = abs(values[-1] - final_value) / max(final_value, 1)
            results.append(('ë§ˆì§€ë§‰ ë‹¨ê³„', values[-1], error))

        # 2. ëª¨ë“  ë‹¨ê³„ í•©
        total = sum(values)
        if total > 0 and final_value > 0:
            error = abs(total - final_value) / max(final_value, 1)
            results.append(('ëª¨ë“  ë‹¨ê³„ í•©', total, error))

        # 3. ë§ˆì§€ë§‰ 2ë‹¨ê³„ í•©
        if len(values) >= 2:
            last_two = sum(values[-2:])
            if last_two > 0 and final_value > 0:
                error = abs(last_two - final_value) / max(final_value, 1)
                results.append(('ë§ˆì§€ë§‰ 2ë‹¨ê³„ í•©', last_two, error))

        if results:
            best = min(results, key=lambda x: x[2])
            return best[1], f"{best[0]}: {best[1]:,.0f} (ì˜¤ì°¨ {best[2]*100:.1f}%)"

        return None, "ê³„ì‚° ë¶ˆê°€"

    def _evaluate_content_score(
        self,
        decomp: List[Dict],
        final_value: float,
        depth: int = 0
    ) -> Dict:
        """
        ë‚´ìš© ì ìˆ˜ í‰ê°€ (45ì ) - v7.10.0

        ì‹¤ì œ ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€ (í˜•ì‹ê³¼ ë¬´ê´€)

        Returns:
            {
                'score': float (0-45),
                'details': list of str,
                'breakdown': {
                    'step_completeness': float (0-10),
                    'calculation_logic': float (0-10),
                    'numerical_accuracy': float (0-25)
                }
            }
        """
        score = 0
        details = []
        breakdown = {}

        if not isinstance(decomp, list) or len(decomp) == 0:
            return {
                'score': 0,
                'details': ['decomposition ì—†ìŒ'],
                'breakdown': {'step_completeness': 0, 'calculation_logic': 0, 'numerical_accuracy': 0}
            }

        # 1. ë‹¨ê³„ë³„ ê³„ì‚° ì™„ì„±ë„ (10ì )
        calculable_steps = 0
        for step in decomp:
            if (step.get('value') is not None and
                (step.get('calculation') or
                 any(op in step.get('reasoning', '') for op in ['x', '/', '+', '-', '*']))):
                calculable_steps += 1

        completeness_score = (calculable_steps / len(decomp)) * 10
        score += completeness_score
        breakdown['step_completeness'] = round(completeness_score, 1)
        details.append(f"ë‹¨ê³„ë³„ ê³„ì‚° ì™„ì„±ë„: {calculable_steps}/{len(decomp)} ({completeness_score:.1f}ì )")

        # 2. ê³„ì‚° ë…¼ë¦¬ ì—°ê²° (10ì )
        logic_score = 0

        # 2-1. ì—°ì‚° ì ì ˆì„± (4ì )
        operations = ['x', '/', '+', '-', '*']
        has_operations = any(
            any(op in step.get('calculation', '') for op in operations)
            for step in decomp
        )
        if has_operations:
            logic_score += 4
            details.append("ì—°ì‚° ì ì ˆì„± (4ì )")
        else:
            details.append("ì—°ì‚° ë¶€ì¡± (0ì )")

        # 2-2. ë…¼ë¦¬ì  ìˆœì„œ (3ì )
        last_step = decomp[-1].get('step', '').lower() if decomp else ''
        if 'ìµœì¢…' in last_step or 'í•©ê³„' in last_step or 'total' in last_step:
            logic_score += 3
            details.append("ë…¼ë¦¬ì  ìˆœì„œ (3ì )")
        else:
            details.append("ìˆœì„œ ë¶ˆëª…í™• (0ì )")

        # 2-3. ì¤‘ê°„ ê²°ê³¼ í™œìš© (3ì )
        has_step_ref = any(
            'step' in step.get('reasoning', '').lower() or
            'step' in step.get('calculation', '').lower()
            for step in decomp[1:]
        ) if len(decomp) > 1 else False
        if has_step_ref:
            logic_score += 3
            details.append("ì¤‘ê°„ ê²°ê³¼ í™œìš© (3ì )")
        else:
            details.append("ì¤‘ê°„ ê²°ê³¼ ë¯¸í™œìš© (0ì )")

        score += logic_score
        breakdown['calculation_logic'] = logic_score

        # 3. ìˆ˜ì¹˜ ì •í™•ì„± (25ì )
        if len(decomp) > 0:
            last_value = decomp[-1].get('value', 0)

            if isinstance(last_value, (int, float)) and last_value > 0 and final_value > 0:
                error_ratio = abs(last_value - final_value) / max(final_value, 1)

                if error_ratio < 0.01:
                    numerical_score = 25
                    details.append("ìˆ˜ì¹˜ ì™„ë²½ ì¼ì¹˜ (25ì )")
                elif error_ratio < 0.05:
                    numerical_score = 20
                    details.append("ìˆ˜ì¹˜ ê±°ì˜ ì¼ì¹˜ (20ì )")
                elif error_ratio < 0.10:
                    numerical_score = 15
                    details.append("ìˆ˜ì¹˜ ê·¼ì ‘ (15ì )")
                elif error_ratio < 0.30:
                    numerical_score = 10
                    details.append("ìˆ˜ì¹˜ ë¶€ë¶„ ì¼ì¹˜ (10ì )")
                else:
                    numerical_score = 5
                    details.append("ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ (5ì )")
            else:
                numerical_score = 0
                details.append("ìˆ˜ì¹˜ ê²€ì¦ ë¶ˆê°€ (0ì )")
        else:
            numerical_score = 0
            details.append("ë§ˆì§€ë§‰ ë‹¨ê³„ ì—†ìŒ (0ì )")

        score += numerical_score
        breakdown['numerical_accuracy'] = numerical_score

        return {
            'score': min(score, 45),
            'details': details,
            'breakdown': breakdown
        }

    def _evaluate_format_score(
        self,
        response: Dict,
        decomp: List[Dict],
        auto_generated_fields: List[str],
        depth: int = 0
    ) -> Dict:
        """
        í˜•ì‹ ì ìˆ˜ í‰ê°€ (5ì ) - v7.10.0

        JSON ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ë„ í‰ê°€

        Returns:
            {
                'score': float (0-5),
                'details': list of str,
                'breakdown': {
                    'final_calculation': int (0 or 2),
                    'calculation_verification': int (0 or 2),
                    'concept_fields': float (0-1)
                }
            }
        """
        score = 0
        details = []
        breakdown = {}

        # 1. final_calculation í•„ë“œ (2ì )
        if 'final_calculation' in response:
            if 'final_calculation' in auto_generated_fields or 'Auto' in str(response.get('final_calculation', '')):
                breakdown['final_calculation'] = 0
                details.append("final_calculation ëˆ„ë½ (ìë™ ìƒì„±, 0ì )")
            else:
                score += 2
                breakdown['final_calculation'] = 2
                details.append("final_calculation ì œê³µ (2ì )")
        else:
            breakdown['final_calculation'] = 0
            details.append("final_calculation ëˆ„ë½ (0ì )")

        # 2. calculation_verification í•„ë“œ (2ì )
        if 'calculation_verification' in response:
            if 'calculation_verification' in auto_generated_fields or 'Auto' in str(response.get('calculation_verification', '')):
                breakdown['calculation_verification'] = 0
                details.append("calculation_verification ëˆ„ë½ (ìë™ ìƒì„±, 0ì )")
            else:
                score += 2
                breakdown['calculation_verification'] = 2
                details.append("calculation_verification ì œê³µ (2ì )")
        else:
            breakdown['calculation_verification'] = 0
            details.append("calculation_verification ëˆ„ë½ (0ì )")

        # 3. concept í•„ë“œ ì™„ì„±ë„ (1ì )
        if decomp and len(decomp) > 0:
            with_concept = sum(1 for s in decomp if s.get('concept'))
            concept_ratio = with_concept / len(decomp)

            if concept_ratio >= 0.8:
                score += 1.0
                breakdown['concept_fields'] = 1.0
                details.append(f"concept í•„ë“œ ì™„ì„± ({with_concept}/{len(decomp)}, 1ì )")
            elif concept_ratio >= 0.5:
                score += 0.5
                breakdown['concept_fields'] = 0.5
                details.append(f"concept í•„ë“œ ë¶€ë¶„ ({with_concept}/{len(decomp)}, 0.5ì )")
            else:
                breakdown['concept_fields'] = 0
                details.append(f"concept í•„ë“œ ë¶€ì¡± ({with_concept}/{len(decomp)}, 0ì )")
        else:
            breakdown['concept_fields'] = 0
            details.append("concept í•„ë“œ ì—†ìŒ (0ì )")

        return {
            'score': score,
            'details': details,
            'breakdown': breakdown
        }

    def _evaluate_conceptual_coherence(
        self,
        question: str,
        decomp: List[Dict],
        final_calc: str,
        context: Optional[Context] = None,
        depth: int = 0
    ) -> Dict:
        """
        ê°œë…ì  ì¼ê´€ì„± í‰ê°€ (15ì ) - v7.10.0

        Pseudo-codeì˜ ë…¼ë¦¬ì  íƒ€ë‹¹ì„± í‰ê°€

        Returns:
            {
                'score': float (0-15),
                'details': list of str
            }
        """
        score = 0
        details = []

        if not isinstance(decomp, list) or len(decomp) < 2:
            return {
                'score': 0,
                'details': ['decomposition ì—†ìŒ ë˜ëŠ” ë¶€ì¡±']
            }

        # ë‹¨ê³„ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        steps_text = ' '.join([
            f"{s.get('step', '')} {s.get('reasoning', '')} {s.get('calculation', '')}"
            for s in decomp
        ]).lower()

        # ë™ì  í‚¤ì›Œë“œ ì¶”ì¶œ (ì§ˆë¬¸ ê¸°ë°˜)
        question_lower = question.lower()

        # 1. í•µì‹¬ ê°œë… í¬í•¨ ì—¬ë¶€ (5ì )
        # ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œê°€ decompositionì— ìˆëŠ”ì§€
        essential_keywords = []
        if 'ì‚¬ì—…ì' in question_lower or 'business' in question_lower:
            essential_keywords.extend(['ì‚¬ì—…ì', 'ë²•ì¸', 'ìì˜ì—…', 'ê¸°ì—…', 'ì°½ì—…', 'ê²½ì œí™œë™', 'ì¸êµ¬'])
        elif 'ì¸êµ¬' in question_lower or 'population' in question_lower:
            essential_keywords.extend(['ì¸êµ¬', 'ë¹„ì¤‘', 'ë¹„ìœ¨', 'ìˆ˜ë„ê¶Œ', 'ì „êµ­'])
        elif 'ì»¤í”¼' in question_lower or 'coffee' in question_lower:
            essential_keywords.extend(['ì»¤í”¼', 'ë§¤ì¥', 'ì í¬', 'ê³ ê°', 'ì†Œë¹„', 'ìˆ˜ìš”'])
        elif 'íƒì‹œ' in question_lower or 'taxi' in question_lower:
            essential_keywords.extend(['íƒì‹œ', 'ì¸êµ¬', 'ì´ìš©', 'ìš´í–‰', 'êµëŒ€'])
        else:
            # ê¸°ë³¸ í‚¤ì›Œë“œ
            essential_keywords.extend(['ì¸êµ¬', 'ë¹„ìœ¨', 'ìˆ˜', 'ê·œëª¨'])

        if essential_keywords:
            essential_found = sum(1 for kw in essential_keywords if kw in steps_text)
            essential_ratio = essential_found / len(essential_keywords)

            if essential_ratio >= 0.3:
                score += 5
                details.append(f"í•µì‹¬ ê°œë… í¬í•¨ ({essential_found}/{len(essential_keywords)}) (5ì )")
            elif essential_ratio >= 0.15:
                score += 3
                details.append(f"í•µì‹¬ ê°œë… ì¼ë¶€ ({essential_found}/{len(essential_keywords)}) (3ì )")
            else:
                details.append(f"í•µì‹¬ ê°œë… ë¶€ì¡± ({essential_found}/{len(essential_keywords)}) (0ì )")
        else:
            score += 3  # í‚¤ì›Œë“œ ì—†ìœ¼ë©´ ì¤‘ë¦½
            details.append("í‚¤ì›Œë“œ ë§¤ì¹­ ìƒëµ (3ì )")

        # 2. ë…¼ë¦¬ì  ì—°ì‚° ì¡´ì¬ (3ì )
        operations = ['x', '/', '+', '-', '*', 'ê³±', 'ë‚˜ëˆ„', 'ë”í•˜']
        operations_found = any(op in steps_text or op in final_calc.lower() for op in operations)
        if operations_found:
            score += 3
            details.append("ë…¼ë¦¬ì  ì—°ì‚° í¬í•¨ (3ì )")
        else:
            details.append("ë…¼ë¦¬ì  ì—°ì‚° ì—†ìŒ (0ì )")

        # 3. Pseudo-code ë…¼ë¦¬ êµ¬ì¡° (7ì )
        # ë§ˆì§€ë§‰ ë‹¨ê³„ê°€ ìµœì¢… ê³„ì‚° ë‹¨ê³„ì¸ì§€
        last_step = decomp[-1].get('step', '').lower()
        if 'ìµœì¢…' in last_step or 'total' in last_step or 'í•©ê³„' in last_step:
            score += 3
            details.append("ìµœì¢… ë‹¨ê³„ ëª…í™• (3ì )")
        else:
            details.append("ìµœì¢… ë‹¨ê³„ ë¶ˆëª…í™• (0ì )")

        # ë‹¨ê³„ ê°„ ì°¸ì¡° ë˜ëŠ” ì—°ì‚° ì¡´ì¬
        has_step_ref = any(
            'step' in s.get('calculation', '').lower()
            for s in decomp[1:]
        ) if len(decomp) > 1 else False

        if has_step_ref:
            score += 4
            details.append("ë‹¨ê³„ ê°„ ì°¸ì¡° ëª…í™• (4ì )")
        else:
            has_calc = any(
                any(op in s.get('calculation', '') for op in ['+', '-', '*', 'x', '/', '='])
                for s in decomp
            )
            if has_calc:
                score += 2
                details.append("ì—°ì‚° ìˆìœ¼ë‚˜ ì°¸ì¡° ë¶ˆëª…í™• (2ì )")
            else:
                details.append("ë‹¨ê³„ ê°„ ì—°ê²° ë¶ˆëª…í™• (0ì )")

        return {
            'score': max(0, min(score, 15)),
            'details': details
        }

    def _verify_calculation_connectivity(
        self,
        decomposition: List[Dict],
        final_value: float
    ) -> Dict:
        """
        ë¶„í•´ ê°’ë“¤ì´ ìµœì¢…ê°’ìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ ìë™ ê²€ì¦ (v7.7.1)
        
        Args:
            decomposition: ë¶„í•´ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸
            final_value: ìµœì¢… ì¶”ì •ê°’
        
        Returns:
            {
                'verified': bool,
                'method': str,  # 'ë§ˆì§€ë§‰ ë‹¨ê³„', 'í•©ê³„', 'ê³±ì…ˆ' ë“±
                'calculated_value': float,
                'error': float,  # ì˜¤ì°¨ìœ¨
                'score': int  # 0-25ì 
            }
        """
        if not isinstance(decomposition, list) or len(decomposition) < 2:
            return {
                'verified': False,
                'score': 0,
                'reason': 'ë‹¨ê³„ ë¶€ì¡±',
                'method': '',
                'calculated_value': 0,
                'error': 1.0
            }
        
        # ê° ë‹¨ê³„ì—ì„œ value ì¶”ì¶œ
        values = []
        for step in decomposition:
            if isinstance(step, dict):
                val = step.get('value', 0)
                if isinstance(val, (int, float)) and val > 0:
                    values.append(val)
        
        if len(values) < 2:
            return {
                'verified': False,
                'score': 0,
                'reason': 'ìœ íš¨í•œ ê°’ ë¶€ì¡±',
                'method': '',
                'calculated_value': 0,
                'error': 1.0
            }
        
        # ë‹¤ì–‘í•œ ì¡°í•© ì‹œë„
        attempts = []
        
        # 1. ë§ˆì§€ë§‰ ê°’
        if values[-1] > 0:
            error = abs(values[-1] - final_value) / max(final_value, 1)
            attempts.append({
                'method': 'ë§ˆì§€ë§‰ ë‹¨ê³„',
                'calculated': values[-1],
                'error': error
            })
        
        # 2. ì „ì²´ í•©ê³„
        total = sum(values)
        if total > 0:
            error = abs(total - final_value) / max(final_value, 1)
            attempts.append({
                'method': 'ëª¨ë“  ë‹¨ê³„ í•©',
                'calculated': total,
                'error': error
            })
        
        # 3. ë§ˆì§€ë§‰ 2ê°œ í•©
        if len(values) >= 2:
            last_two = sum(values[-2:])
            if last_two > 0:
                error = abs(last_two - final_value) / max(final_value, 1)
                attempts.append({
                    'method': 'ë§ˆì§€ë§‰ 2ë‹¨ê³„ í•©',
                    'calculated': last_two,
                    'error': error
                })
        
        # 4. ë§ˆì§€ë§‰ 3ê°œ í•©
        if len(values) >= 3:
            last_three = sum(values[-3:])
            if last_three > 0:
                error = abs(last_three - final_value) / max(final_value, 1)
                attempts.append({
                    'method': 'ë§ˆì§€ë§‰ 3ë‹¨ê³„ í•©',
                    'calculated': last_three,
                    'error': error
                })
        
        # ê°€ì¥ ì˜¤ì°¨ê°€ ì‘ì€ ê²ƒ ì„ íƒ
        if attempts:
            best = min(attempts, key=lambda x: x['error'])
            
            # ì ìˆ˜ ê³„ì‚°
            if best['error'] < 0.01:  # 1% ì´ë‚´
                score = 25
            elif best['error'] < 0.05:  # 5% ì´ë‚´
                score = 20
            elif best['error'] < 0.1:  # 10% ì´ë‚´
                score = 15
            elif best['error'] < 0.3:  # 30% ì´ë‚´
                score = 10
            else:
                score = 5
            
            return {
                'verified': best['error'] < 0.1,  # 10% ì´ë‚´ë©´ í†µê³¼
                'method': best['method'],
                'calculated_value': best['calculated'],
                'error': best['error'],
                'score': score
            }
        
        return {
            'verified': False,
            'score': 0,
            'reason': 'ê³„ì‚° ë¶ˆê°€',
            'method': '',
            'calculated_value': 0,
            'error': 1.0
        }


