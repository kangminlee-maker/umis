"""
Three-Stage Evaluator: Guardian Meta-RAG

3ë‹¨ê³„ í‰ê°€ ì‹œìŠ¤í…œ:
- Stage 1: Weighted Scoring (ë¹ ë¦„, 80%)
- Stage 2: Cross-Encoder (ì •ë°€, 15%)
- Stage 3: LLM + RAE (ìµœì¢…, 5%)
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.utils.logger import get_logger
from umis_rag.guardian.rae_memory import RAEMemory
from langchain_openai import ChatOpenAI
from umis_rag.core.config import settings

logger = get_logger(__name__)


@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼"""
    grade: str  # A/B/C/D
    score: float  # 0-1
    rationale: str
    evidence_ids: List[str]
    stage: str  # stage_1 / stage_2 / stage_3
    confidence: float  # 0-1


class ThreeStageEvaluator:
    """
    3-Stage í‰ê°€ ì‹œìŠ¤í…œ
    
    Stage 1 (ë¹ ë¦„, 80%):
      ìë™ ì ìˆ˜ ê³„ì‚°ìœ¼ë¡œ ë¹ ë¥¸ í•„í„°ë§
      
    Stage 2 (ì •ë°€, 15%):
      Cross-Encoderë¡œ ì •ë°€ ì¬í‰ê°€
      
    Stage 3 (ìµœì¢…, 5%):
      LLM + RAE Indexë¡œ ìµœì¢… íŒë‹¨
    
    ì‚¬ìš©:
    -----
    evaluator = ThreeStageEvaluator()
    
    result = evaluator.evaluate(deliverable)
    # â†’ Stage 1ë¡œ ëŒ€ë¶€ë¶„ ì²˜ë¦¬
    # â†’ ì• ë§¤í•œ ê²½ìš°ë§Œ Stage 2, 3
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.rae_memory = RAEMemory()
        self.llm = ChatOpenAI(
            model=settings.llm_model,
            temperature=0.3,  # ë‚®ì€ temperature (ì¼ê´€ì„±)
            openai_api_key=settings.openai_api_key
        )
        
        logger.info("ThreeStageEvaluator ì´ˆê¸°í™”")
        logger.info("  Stage 1: Weighted Scoring (ìë™)")
        logger.info("  Stage 2: Cross-Encoder (ì •ë°€)")
        logger.info("  Stage 3: LLM + RAE (ìµœì¢…)")
    
    def evaluate(
        self,
        deliverable: Dict[str, Any],
        force_stage: Optional[int] = None
    ) -> EvaluationResult:
        """
        3-Stage í‰ê°€ ì‹¤í–‰
        
        Args:
            deliverable: í‰ê°€ ëŒ€ìƒ ì‚°ì¶œë¬¼
            force_stage: ê°•ì œ Stage (í…ŒìŠ¤íŠ¸ìš©)
        
        Returns:
            EvaluationResult
        """
        deliverable_id = deliverable.get('id', 'unknown')
        deliverable_content = deliverable.get('content', '')
        
        logger.info(f"[Guardian] 3-Stage í‰ê°€ ì‹œì‘: {deliverable_id}")
        
        # Stage 1: Weighted Scoring (ë¹ ë¦„)
        if force_stage is None or force_stage == 1:
            stage1_result = self._stage1_weighted_scoring(deliverable)
            
            # ëª…í™•í•œ ì¼€ì´ìŠ¤ë©´ Stage 1ì—ì„œ ì¢…ë£Œ (80%)
            if stage1_result.confidence >= 0.90:
                logger.info(f"  âœ… Stage 1 í™•ì •: {stage1_result.grade} (ì‹ ë¢°ë„ {stage1_result.confidence:.2f})")
                return stage1_result
        
        # Stage 2: Cross-Encoder (ì •ë°€) - 15%
        if force_stage is None or force_stage == 2:
            stage2_result = self._stage2_cross_encoder(deliverable, stage1_result if force_stage is None else None)
            
            # ëª…í™•í•´ì§€ë©´ Stage 2ì—ì„œ ì¢…ë£Œ
            if stage2_result.confidence >= 0.85:
                logger.info(f"  âœ… Stage 2 í™•ì •: {stage2_result.grade} (ì‹ ë¢°ë„ {stage2_result.confidence:.2f})")
                return stage2_result
        
        # Stage 3: LLM + RAE (ìµœì¢…) - 5%
        logger.info(f"  ğŸ”„ Stage 3 (LLM + RAE) í•„ìš” - ì• ë§¤í•œ ì¼€ì´ìŠ¤")
        stage3_result = self._stage3_llm_rae(deliverable)
        logger.info(f"  âœ… Stage 3 ìµœì¢…: {stage3_result.grade} (ì‹ ë¢°ë„ {stage3_result.confidence:.2f})")
        
        return stage3_result
    
    def _stage1_weighted_scoring(self, deliverable: Dict[str, Any]) -> EvaluationResult:
        """
        Stage 1: Weighted Scoring (ìë™)
        
        ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ìë™ ì ìˆ˜ ê³„ì‚°:
        - ëª…í™•ì„± (Clarity): 30%
        - ì‹¤í–‰ê°€ëŠ¥ì„± (Feasibility): 30%
        - ê·¼ê±° (Evidence): 25%
        - ì •ëŸ‰í™” (Quantification): 15%
        """
        logger.info("  [Stage 1] Weighted Scoring ì‹œì‘")
        
        content = deliverable.get('content', '')
        metadata = deliverable.get('metadata', {})
        
        scores = {}
        
        # 1. ëª…í™•ì„± (30%)
        clarity_score = 0.0
        if len(content) > 100:  # ì¶©ë¶„í•œ ì„¤ëª…
            clarity_score += 0.3
        if 'ëª©í‘œ' in content or 'target' in content.lower():  # ëª©í‘œ ëª…í™•
            clarity_score += 0.4
        if 'ì „ëµ' in content or 'strategy' in content.lower():  # ì „ëµ ìˆìŒ
            clarity_score += 0.3
        
        scores['clarity'] = min(clarity_score, 1.0)
        
        # 2. ì‹¤í–‰ê°€ëŠ¥ì„± (30%)
        feasibility_score = 0.0
        if 'ì‹œì¥' in content or 'market' in content.lower():  # ì‹œì¥ ì–¸ê¸‰
            feasibility_score += 0.3
        if 'ì‚¬ë¡€' in content or 'case' in content.lower():  # ì‚¬ë¡€ ìˆìŒ
            feasibility_score += 0.4
        if metadata.get('has_examples'):  # ë©”íƒ€ë°ì´í„°ì— ì‚¬ë¡€
            feasibility_score += 0.3
        
        scores['feasibility'] = min(feasibility_score, 1.0)
        
        # 3. ê·¼ê±° (25%)
        evidence_score = 0.0
        evidence_ids = metadata.get('evidence_ids', [])
        if evidence_ids:
            evidence_score = min(len(evidence_ids) * 0.3, 1.0)
        
        scores['evidence'] = evidence_score
        
        # 4. ì •ëŸ‰í™” (15%)
        quant_score = 0.0
        if any(char.isdigit() for char in content):  # ìˆ«ì ìˆìŒ
            quant_score += 0.5
        if '$' in content or 'ì›' in content or 'SAM' in content:  # ê¸ˆì•¡/ì‹œì¥ í¬ê¸°
            quant_score += 0.5
        
        scores['quant'] = min(quant_score, 1.0)
        
        # ê°€ì¤‘ í‰ê· 
        total_score = (
            scores['clarity'] * 0.30 +
            scores['feasibility'] * 0.30 +
            scores['evidence'] * 0.25 +
            scores['quant'] * 0.15
        )
        
        # ë“±ê¸‰ ê²°ì •
        if total_score >= 0.85:
            grade = 'A'
            confidence = 0.95
        elif total_score >= 0.70:
            grade = 'B'
            confidence = 0.92
        elif total_score >= 0.50:
            grade = 'C'
            confidence = 0.88
        else:
            grade = 'D'
            confidence = 0.85
        
        # Rationale ìƒì„±
        rationale_parts = []
        if scores['clarity'] >= 0.7:
            rationale_parts.append("ëª…í™•í•œ ëª©í‘œì™€ ì „ëµ")
        if scores['feasibility'] >= 0.7:
            rationale_parts.append("ì‹¤í–‰ ê°€ëŠ¥ì„± ë†’ìŒ")
        if scores['evidence'] >= 0.5:
            rationale_parts.append(f"{len(evidence_ids)}ê°œ ê·¼ê±° ì‚¬ë¡€")
        if scores['quant'] >= 0.5:
            rationale_parts.append("ì •ëŸ‰í™”ë¨")
        
        rationale = ", ".join(rationale_parts) if rationale_parts else f"ì ìˆ˜ {total_score:.2f}"
        
        logger.info(f"    ì ìˆ˜: {total_score:.3f}, ë“±ê¸‰: {grade}, ì‹ ë¢°ë„: {confidence:.2f}")
        
        return EvaluationResult(
            grade=grade,
            score=total_score,
            rationale=rationale,
            evidence_ids=evidence_ids,
            stage='stage_1',
            confidence=confidence
        )
    
    def _stage2_cross_encoder(
        self,
        deliverable: Dict[str, Any],
        stage1_result: Optional[EvaluationResult]
    ) -> EvaluationResult:
        """
        Stage 2: Cross-Encoder (ì •ë°€ ì¬í‰ê°€)
        
        í˜„ì¬: ê°„ì†Œí™” ë²„ì „ (ì‹¤ì œ Cross-EncoderëŠ” ë³„ë„ ëª¨ë¸ í•„ìš”)
        """
        logger.info("  [Stage 2] Cross-Encoder ì •ë°€ í‰ê°€")
        
        # Stage 1 ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì¬í‰ê°€
        if stage1_result:
            # Stage 1 ì ìˆ˜ ì¡°ì • (ë” ì—„ê²©í•˜ê²Œ)
            adjusted_score = stage1_result.score * 0.95
            
            # ì¬ë“±ê¸‰í™”
            if adjusted_score >= 0.80:
                grade = 'A'
                confidence = 0.90
            elif adjusted_score >= 0.65:
                grade = 'B'
                confidence = 0.87
            elif adjusted_score >= 0.45:
                grade = 'C'
                confidence = 0.83
            else:
                grade = 'D'
                confidence = 0.80
            
            logger.info(f"    ì¡°ì • ì ìˆ˜: {adjusted_score:.3f}, ë“±ê¸‰: {grade}")
            
            return EvaluationResult(
                grade=grade,
                score=adjusted_score,
                rationale=f"Stage 2 ì¬í‰ê°€: {stage1_result.rationale}",
                evidence_ids=stage1_result.evidence_ids,
                stage='stage_2',
                confidence=confidence
            )
        
        # Stage 1 ì—†ì´ ì§ì ‘ ì‹¤í–‰ ì‹œ
        return self._stage1_weighted_scoring(deliverable)
    
    def _stage3_llm_rae(self, deliverable: Dict[str, Any]) -> EvaluationResult:
        """
        Stage 3: LLM + RAE (ìµœì¢… íŒë‹¨)
        
        - RAE Indexì—ì„œ ìœ ì‚¬ í‰ê°€ ê²€ìƒ‰
        - LLMìœ¼ë¡œ ìµœì¢… íŒë‹¨
        - ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ
        """
        logger.info("  [Stage 3] LLM + RAE ìµœì¢… íŒë‹¨")
        
        deliverable_id = deliverable.get('id', 'unknown')
        deliverable_content = deliverable.get('content', '')
        
        # 1. RAE Indexì—ì„œ ìœ ì‚¬ í‰ê°€ ê²€ìƒ‰
        similar_evals = self.rae_memory.find_similar_evaluations(deliverable_content)
        
        # 2. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒ ê¸°íšŒ ê°€ì„¤ì„ í‰ê°€í•˜ì„¸ìš”.

ê°€ì„¤:
{deliverable_content[:500]}

í‰ê°€ ê¸°ì¤€:
1. ëª…í™•ì„±: ëª©í‘œì™€ ì „ëµì´ ëª…í™•í•œê°€?
2. ì‹¤í–‰ê°€ëŠ¥ì„±: ì‹¤ì œë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œê°€?
3. ê·¼ê±°: ì¶©ë¶„í•œ ê·¼ê±°ê°€ ìˆëŠ”ê°€?
4. ì •ëŸ‰í™”: ì‹œì¥ í¬ê¸° ë“±ì´ ì •ëŸ‰í™”ë˜ì—ˆëŠ”ê°€?

"""
        
        # ìœ ì‚¬ í‰ê°€ê°€ ìˆìœ¼ë©´ ì°¸ê³ 
        if similar_evals:
            prompt += "\nê³¼ê±° ìœ ì‚¬ í‰ê°€:\n"
            for eval_data in similar_evals[:2]:
                prompt += f"- {eval_data['deliverable_id']}: {eval_data['grade']} ({eval_data['rationale'][:50]}...)\n"
            prompt += "\nìœ„ í‰ê°€ì™€ ì¼ê´€ì„± ìˆê²Œ í‰ê°€í•˜ì„¸ìš”.\n"
        
        prompt += """
ì‘ë‹µ í˜•ì‹ (JSON):
{
  "grade": "A/B/C/D",
  "score": 0.0-1.0,
  "rationale": "í‰ê°€ ì‚¬ìœ  (í•œ ë¬¸ì¥)"
}
"""
        
        # 3. LLM í‰ê°€
        try:
            response = self.llm.invoke(prompt)
            
            # JSON íŒŒì‹± (ê°„ë‹¨íˆ)
            import json
            import re
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'\{[^}]+\}', response.content, re.DOTALL)
            if json_match:
                result_data = json.loads(json_match.group())
                
                logger.info(f"    LLM í‰ê°€: {result_data.get('grade')} ({result_data.get('score', 0):.2f})")
                
                return EvaluationResult(
                    grade=result_data.get('grade', 'C'),
                    score=result_data.get('score', 0.5),
                    rationale=result_data.get('rationale', 'LLM í‰ê°€'),
                    evidence_ids=deliverable.get('metadata', {}).get('evidence_ids', []),
                    stage='stage_3',
                    confidence=0.98  # LLM + RAE = ìµœê³  ì‹ ë¢°ë„
                )
        
        except Exception as e:
            logger.error(f"    âŒ LLM í‰ê°€ ì‹¤íŒ¨: {e}")
        
        # Fallback: Stage 1
        logger.warning("    âš ï¸  Stage 3 ì‹¤íŒ¨ â†’ Stage 1 Fallback")
        return self._stage1_weighted_scoring(deliverable)


# ì˜ˆì‹œ ì‚¬ìš©
if __name__ == "__main__":
    print("=" * 60)
    print("Three-Stage Evaluator í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    evaluator = ThreeStageEvaluator()
    
    # í…ŒìŠ¤íŠ¸ ê°€ì„¤
    test_deliverable = {
        'id': 'OPP-TEST-001',
        'content': '''
ìŒì•… ìŠ¤íŠ¸ë¦¬ë° ì‹œì¥ì—ì„œ Freemium + ê´‘ê³  ëª¨ë¸ ê¸°íšŒ

ëª©í‘œ: Spotifyì™€ ìœ ì‚¬í•œ êµ¬ë… + ê´‘ê³  ì´ì¤‘ ìˆ˜ìµí™” ëª¨ë¸
ì „ëµ: ë¬´ë£Œ ì‚¬ìš©ìëŠ” ê´‘ê³ , í”„ë¦¬ë¯¸ì—„ì€ êµ¬ë…ìœ¼ë¡œ ì „í™˜
ì‹œì¥ ê·œëª¨: ì—° $10B
ê·¼ê±°: Spotify ì‚¬ë¡€, YouTube Music ì„±ê³µ

ì‹¤í–‰ ê³„íš:
1. ë¬´ë£Œ ë²„ì „ ì¶œì‹œ
2. ê´‘ê³  íŒŒíŠ¸ë„ˆ í™•ë³´
3. í”„ë¦¬ë¯¸ì—„ ì „í™˜ ìœ ë„
        ''',
        'metadata': {
            'evidence_ids': ['CAN-spotify-001', 'CAN-youtube-002'],
            'has_examples': True
        }
    }
    
    # ê° Stage í…ŒìŠ¤íŠ¸
    print("\n[1] Stage 1: Weighted Scoring")
    result1 = evaluator.evaluate(test_deliverable, force_stage=1)
    print(f"  ë“±ê¸‰: {result1.grade}")
    print(f"  ì ìˆ˜: {result1.score:.3f}")
    print(f"  ì‹ ë¢°ë„: {result1.confidence:.2f}")
    print(f"  ì‚¬ìœ : {result1.rationale}")
    
    print("\n[2] Stage 2: Cross-Encoder")
    result2 = evaluator.evaluate(test_deliverable, force_stage=2)
    print(f"  ë“±ê¸‰: {result2.grade}")
    print(f"  ì ìˆ˜: {result2.score:.3f}")
    print(f"  ì‹ ë¢°ë„: {result2.confidence:.2f}")
    
    print("\n[3] Stage 3: LLM + RAE (ì‹¤ì œ LLM í˜¸ì¶œ)")
    result3 = evaluator.evaluate(test_deliverable, force_stage=3)
    print(f"  ë“±ê¸‰: {result3.grade}")
    print(f"  ì ìˆ˜: {result3.score:.3f}")
    print(f"  ì‹ ë¢°ë„: {result3.confidence:.2f}")
    print(f"  ì‚¬ìœ : {result3.rationale}")
    
    print("\n[4] ìë™ Stage ì„ íƒ")
    result_auto = evaluator.evaluate(test_deliverable)
    print(f"  ì„ íƒëœ Stage: {result_auto.stage}")
    print(f"  ìµœì¢… ë“±ê¸‰: {result_auto.grade}")
    
    print("\nâœ… Three-Stage Evaluator ì‘ë™ í™•ì¸")

