#!/usr/bin/env python3
"""
Model Config System í…ŒìŠ¤íŠ¸

config/model_configs.yaml ë° model_configs.py, model_router.py ê²€ì¦
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.core.model_configs import (
    model_config_manager,
    get_model_config,
    list_supported_models,
    is_pro_model
)
from umis_rag.core.model_router import select_model_with_config


def test_yaml_loading():
    """YAML ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 1: YAML ë¡œë”©")
    print("="*60)
    
    try:
        models = list_supported_models()
        print(f"âœ… ë¡œë“œëœ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ")
        print(f"   ëª¨ë¸ ëª©ë¡: {', '.join(models[:8])}...")
        return True
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")
        return False


def test_model_config_query():
    """ëª¨ë¸ ì„¤ì • ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 2: ëª¨ë¸ ì„¤ì • ì¡°íšŒ")
    print("="*60)
    
    test_models = ['o1-mini', 'gpt-5.1', 'gpt-5-pro', 'gpt-4.1-nano']
    success_count = 0
    
    for model_name in test_models:
        try:
            config = get_model_config(model_name)
            print(f"\nâœ… {model_name}:")
            print(f"   - API íƒ€ì…: {config.api_type}")
            print(f"   - Max tokens: {config.max_output_tokens}")
            print(f"   - Reasoning effort: {config.reasoning_effort_support}")
            if config.reasoning_effort_support:
                print(f"     - Levels: {config.reasoning_effort_levels}")
                print(f"     - Default: {config.reasoning_effort_default}")
                if config.reasoning_effort_fixed:
                    print(f"     - Fixed: {config.reasoning_effort_fixed}")
            print(f"   - Temperature: {config.temperature_support}")
            print(f"   - Notes: {config.notes[:50]}...")
            success_count += 1
        except Exception as e:
            print(f"âŒ {model_name} ì‹¤íŒ¨: {e}")
    
    print(f"\nê²°ê³¼: {success_count}/{len(test_models)} ì„±ê³µ")
    return success_count == len(test_models)


def test_api_params_building():
    """API íŒŒë¼ë¯¸í„° êµ¬ì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 3: API íŒŒë¼ë¯¸í„° ìë™ êµ¬ì„±")
    print("="*60)
    
    test_cases = [
        {
            'model': 'o1-mini',
            'reasoning_effort': 'medium',
            'expected_api': 'responses'
        },
        {
            'model': 'gpt-5.1',
            'reasoning_effort': 'high',
            'expected_api': 'responses'
        },
        {
            'model': 'gpt-4.1-nano',
            'reasoning_effort': None,
            'expected_api': 'chat'
        }
    ]
    
    success_count = 0
    
    for test in test_cases:
        try:
            config = get_model_config(test['model'])
            params = config.build_api_params(
                prompt="í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸",
                reasoning_effort=test['reasoning_effort']
            )
            
            print(f"\nâœ… {test['model']}:")
            print(f"   - API íƒ€ì…: {config.api_type}")
            print(f"   - model: {params.get('model')}")
            
            if config.api_type == 'responses':
                print(f"   - input: {params.get('input')[:30]}...")
                print(f"   - max_output_tokens: {params.get('max_output_tokens')}")
                if 'reasoning' in params:
                    print(f"   - reasoning.effort: {params['reasoning']['effort']}")
            else:
                print(f"   - messages: {len(params.get('messages', []))}ê°œ")
                print(f"   - max_tokens: {params.get('max_tokens')}")
                if 'temperature' in params:
                    print(f"   - temperature: {params['temperature']}")
            
            # ê²€ì¦
            assert config.api_type == test['expected_api'], f"API íƒ€ì… ë¶ˆì¼ì¹˜"
            assert params.get('model') == test['model'], f"ëª¨ë¸ ì´ë¦„ ë¶ˆì¼ì¹˜"
            
            success_count += 1
        except Exception as e:
            print(f"âŒ {test['model']} ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nê²°ê³¼: {success_count}/{len(test_cases)} ì„±ê³µ")
    return success_count == len(test_cases)


def test_pro_model_detection():
    """Pro ëª¨ë¸ ê°ì§€ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 4: Pro ëª¨ë¸ ê°ì§€")
    print("="*60)
    
    pro_models = ['gpt-5-pro', 'o1-pro', 'o1-pro-2025-03-19']
    non_pro_models = ['o1-mini', 'gpt-5.1', 'gpt-4.1-nano']
    
    success_count = 0
    total = len(pro_models) + len(non_pro_models)
    
    print("\nPro ëª¨ë¸ (Fast Mode ëŒ€ìƒ):")
    for model in pro_models:
        result = is_pro_model(model)
        print(f"  {model}: {result}")
        if result:
            success_count += 1
        else:
            print(f"    âŒ Pro ëª¨ë¸ì´ì§€ë§Œ False ë°˜í™˜")
    
    print("\nì¼ë°˜ ëª¨ë¸:")
    for model in non_pro_models:
        result = is_pro_model(model)
        print(f"  {model}: {result}")
        if not result:
            success_count += 1
        else:
            print(f"    âŒ ì¼ë°˜ ëª¨ë¸ì´ì§€ë§Œ True ë°˜í™˜")
    
    print(f"\nê²°ê³¼: {success_count}/{total} ì„±ê³µ")
    return success_count == total


def test_model_router_integration():
    """ModelRouter í†µí•© í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 5: ModelRouter select_model_with_config()")
    print("="*60)
    
    test_phases = [0, 1, 2, 3, 4]
    success_count = 0
    
    for phase in test_phases:
        try:
            model_name, config = select_model_with_config(phase)
            
            print(f"\nâœ… Phase {phase}:")
            print(f"   - ì„ íƒëœ ëª¨ë¸: {model_name}")
            print(f"   - API íƒ€ì…: {config.api_type}")
            print(f"   - Max tokens: {config.max_output_tokens}")
            print(f"   - Reasoning effort: {config.reasoning_effort_support}")
            
            # API íŒŒë¼ë¯¸í„° êµ¬ì„± í…ŒìŠ¤íŠ¸
            api_params = config.build_api_params(
                prompt=f"Phase {phase} í…ŒìŠ¤íŠ¸",
                reasoning_effort='medium'
            )
            print(f"   - API params keys: {list(api_params.keys())}")
            
            success_count += 1
        except Exception as e:
            print(f"âŒ Phase {phase} ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"\nê²°ê³¼: {success_count}/{len(test_phases)} ì„±ê³µ")
    return success_count == len(test_phases)


def test_prefix_fallback():
    """Prefix ê¸°ë°˜ í´ë°± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 6: Prefix ê¸°ë°˜ í´ë°±")
    print("="*60)
    
    test_cases = [
        ('o1-mini-2025-12-31', 'o1-mini'),  # ìƒˆ ë²„ì „ â†’ ê¸°ë³¸ ë²„ì „
        ('o3-mini-2025-99-99', 'o3-mini'),
        ('gpt-5.1-turbo', 'gpt-5.1'),
        ('unknown-model', 'default'),  # ì™„ì „ ë¯¸ì§€ì›
    ]
    
    success_count = 0
    
    for model_input, expected_base in test_cases:
        try:
            config = get_model_config(model_input)
            
            if expected_base == 'default':
                # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                assert config.api_type == 'chat', "ê¸°ë³¸ ì„¤ì • api_typeì€ chat"
                print(f"âœ… {model_input} â†’ default config")
            else:
                # ì˜ˆìƒ ë² ì´ìŠ¤ ëª¨ë¸ ì„¤ì • ì‚¬ìš©
                base_config = get_model_config(expected_base)
                assert config.api_type == base_config.api_type
                print(f"âœ… {model_input} â†’ {expected_base} (fallback)")
            
            success_count += 1
        except Exception as e:
            print(f"âŒ {model_input} ì‹¤íŒ¨: {e}")
    
    print(f"\nê²°ê³¼: {success_count}/{len(test_cases)} ì„±ê³µ")
    return success_count == len(test_cases)


def main():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "â”"*60)
    print("ğŸ§ª Model Config System í†µí•© í…ŒìŠ¤íŠ¸")
    print("â”"*60)
    
    tests = [
        ("YAML ë¡œë”©", test_yaml_loading),
        ("ëª¨ë¸ ì„¤ì • ì¡°íšŒ", test_model_config_query),
        ("API íŒŒë¼ë¯¸í„° êµ¬ì„±", test_api_params_building),
        ("Pro ëª¨ë¸ ê°ì§€", test_pro_model_detection),
        ("ModelRouter í†µí•©", test_model_router_integration),
        ("Prefix í´ë°±", test_prefix_fallback),
    ]
    
    results = []
    
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ {name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "â”"*60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("â”"*60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status}: {name}")
    
    print(f"\nì´ ê²°ê³¼: {passed}/{total} í†µê³¼ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return 0
    else:
        print(f"\nâš ï¸ {total - passed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return 1


if __name__ == "__main__":
    sys.exit(main())

