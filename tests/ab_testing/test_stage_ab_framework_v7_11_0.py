"""
v7.11.0 Stage ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

ëª©í‘œ:
1. Stage 1-4 ë¹„êµ (Evidence â†’ Prior â†’ Fermi â†’ Fusion)
2. Budget ê¸°ë°˜ íƒìƒ‰ ë¹„êµ (Standard vs Fast)
3. Certainty ì¸¡ì • (high/medium/low)
4. ì •í™•ë„, ì†ë„, Cost ë¹„êµ

ë§ˆì´ê·¸ë ˆì´ì…˜:
- v7.9.0 vs v7.10.0 â†’ v7.10.2 (Legacy) vs v7.11.0 (Fusion)
- phase/confidence â†’ source/certainty
- estimate_hybrid ì œê±° â†’ estimateë§Œ ì‚¬ìš©

ì‘ì„±ì¼: 2025-11-26
"""

import pytest
import time
import json
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path

import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from umis_rag.agents.estimator import EstimatorRAG
from umis_rag.agents.estimator.models import Context
from umis_rag.agents.estimator.common import EstimationResult, Budget, create_standard_budget, create_fast_budget


@dataclass
class TestCase:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    id: str
    question: str
    expected_value: Optional[float] = None
    expected_range: Optional[Tuple[float, float]] = None
    domain: Optional[str] = None
    region: Optional[str] = None
    project_data: Optional[Dict] = None
    tags: List[str] = field(default_factory=list)


@dataclass
class StageABResult:
    """Stage ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_id: str
    question: str

    # Standard Budget ê²°ê³¼
    standard_value: Optional[float] = None
    standard_source: str = ""
    standard_certainty: str = ""
    standard_time_ms: float = 0.0
    standard_llm_calls: int = 0
    standard_success: bool = False

    # Fast Budget ê²°ê³¼
    fast_value: Optional[float] = None
    fast_source: str = ""
    fast_certainty: str = ""
    fast_time_ms: float = 0.0
    fast_llm_calls: int = 0
    fast_success: bool = False

    # ë¹„êµ
    expected_value: Optional[float] = None
    expected_range: Optional[Tuple[float, float]] = None

    # ë¶„ì„
    standard_accuracy: Optional[float] = None
    fast_accuracy: Optional[float] = None
    winner: str = "tie"
    notes: str = ""


@dataclass
class StageABSummary:
    """Stage ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½"""
    total_tests: int = 0
    standard_wins: int = 0
    fast_wins: int = 0
    ties: int = 0

    # í‰ê·  ì§€í‘œ
    standard_avg_time_ms: float = 0.0
    fast_avg_time_ms: float = 0.0
    standard_avg_llm_calls: float = 0.0
    fast_avg_llm_calls: float = 0.0
    standard_success_rate: float = 0.0
    fast_success_rate: float = 0.0

    # Sourceë³„ ë¶„í¬ (Literal, Direct RAG, Validator, Prior, Fermi, Fusion)
    standard_source_dist: Dict[str, int] = field(default_factory=dict)
    fast_source_dist: Dict[str, int] = field(default_factory=dict)

    # Certainty ë¶„í¬ (high, medium, low)
    standard_certainty_dist: Dict[str, int] = field(default_factory=dict)
    fast_certainty_dist: Dict[str, int] = field(default_factory=dict)


class StageABTestFramework:
    """Stage ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""

    def __init__(self):
        self.estimator = EstimatorRAG()
        self.results: List[StageABResult] = []
        self.test_cases: List[TestCase] = []

    def add_test_case(self, test_case: TestCase):
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€"""
        self.test_cases.append(test_case)

    def add_test_cases(self, test_cases: List[TestCase]):
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¼ê´„ ì¶”ê°€"""
        self.test_cases.extend(test_cases)

    def run_single(self, test_case: TestCase) -> StageABResult:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        result = StageABResult(
            test_id=test_case.id,
            question=test_case.question,
            expected_value=test_case.expected_value,
            expected_range=test_case.expected_range
        )

        context = Context(
            domain=test_case.domain or "",
            region=test_case.region or ""
        )

        # Standard Budget
        try:
            start = time.time()
            standard_result = self.estimator.estimate(
                question=test_case.question,
                context=context,
                project_data=test_case.project_data
            )
            result.standard_time_ms = (time.time() - start) * 1000
            result.standard_value = standard_result.value
            result.standard_source = standard_result.source
            result.standard_certainty = standard_result.certainty
            result.standard_llm_calls = standard_result.cost.get('llm_calls', 0)
            result.standard_success = standard_result.is_successful()
        except Exception as e:
            result.notes += f"Standard error: {e}; "

        # Fast Budget (EstimatorRAGì— fast_mode íŒŒë¼ë¯¸í„° ì¶”ê°€ í•„ìš”, ì„ì‹œë¡œ ë™ì¼)
        try:
            start = time.time()
            fast_result = self.estimator.estimate(
                question=test_case.question,
                context=context,
                project_data=test_case.project_data
                # TODO: fast_mode=True íŒŒë¼ë¯¸í„° ì¶”ê°€
            )
            result.fast_time_ms = (time.time() - start) * 1000
            result.fast_value = fast_result.value
            result.fast_source = fast_result.source
            result.fast_certainty = fast_result.certainty
            result.fast_llm_calls = fast_result.cost.get('llm_calls', 0)
            result.fast_success = fast_result.is_successful()
        except Exception as e:
            result.notes += f"Fast error: {e}; "

        # ì •í™•ë„ ê³„ì‚°
        if result.expected_value:
            if result.standard_value:
                result.standard_accuracy = self._calculate_accuracy(
                    result.standard_value, result.expected_value, result.expected_range
                )
            if result.fast_value:
                result.fast_accuracy = self._calculate_accuracy(
                    result.fast_value, result.expected_value, result.expected_range
                )

        # Winner ê²°ì •
        result.winner = self._determine_winner(result)

        return result

    def _calculate_accuracy(self, value: float, expected: float, expected_range: Optional[Tuple[float, float]]) -> float:
        """ì •í™•ë„ ê³„ì‚° (0.0-1.0)"""
        if expected_range:
            low, high = expected_range
            if low <= value <= high:
                return 1.0
            else:
                # Range ë²—ì–´ë‚œ ì •ë„ì— ë”°ë¼ ì ìˆ˜ ê°ì†Œ
                distance = min(abs(value - low), abs(value - high))
                range_width = high - low
                return max(0.0, 1.0 - (distance / range_width))
        else:
            # Range ì—†ìœ¼ë©´ ìƒëŒ€ ì˜¤ì°¨ ì‚¬ìš©
            error = abs(value - expected) / expected
            return max(0.0, 1.0 - error)

    def _determine_winner(self, result: StageABResult) -> str:
        """Winner ê²°ì •"""
        if not result.standard_success and not result.fast_success:
            return "tie (both failed)"

        if not result.standard_success:
            return "fast"
        if not result.fast_success:
            return "standard"

        # ì •í™•ë„ ë¹„êµ
        if result.standard_accuracy and result.fast_accuracy:
            acc_diff = abs(result.standard_accuracy - result.fast_accuracy)
            if acc_diff < 0.1:
                # ì •í™•ë„ ë¹„ìŠ·í•˜ë©´ ì†ë„ ë¹„êµ
                if result.fast_time_ms < result.standard_time_ms * 0.7:
                    return "fast"
                else:
                    return "standard"
            elif result.standard_accuracy > result.fast_accuracy:
                return "standard"
            else:
                return "fast"

        # Certainty ë¹„êµ
        certainty_score = {"high": 3, "medium": 2, "low": 1}
        standard_cert = certainty_score.get(result.standard_certainty, 0)
        fast_cert = certainty_score.get(result.fast_certainty, 0)

        if standard_cert > fast_cert:
            return "standard"
        elif fast_cert > standard_cert:
            return "fast"
        else:
            return "tie"

    def run_all(self) -> StageABSummary:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.results = []
        for test_case in self.test_cases:
            result = self.run_single(test_case)
            self.results.append(result)

        return self.summarize()

    def summarize(self) -> StageABSummary:
        """ê²°ê³¼ ìš”ì•½"""
        summary = StageABSummary(total_tests=len(self.results))

        # ì§‘ê³„
        for result in self.results:
            if result.winner.startswith("standard"):
                summary.standard_wins += 1
            elif result.winner.startswith("fast"):
                summary.fast_wins += 1
            else:
                summary.ties += 1

            # í‰ê·  ê³„ì‚°ìš©
            summary.standard_avg_time_ms += result.standard_time_ms
            summary.fast_avg_time_ms += result.fast_time_ms
            summary.standard_avg_llm_calls += result.standard_llm_calls
            summary.fast_avg_llm_calls += result.fast_llm_calls

            if result.standard_success:
                summary.standard_success_rate += 1
            if result.fast_success:
                summary.fast_success_rate += 1

            # Source ë¶„í¬
            if result.standard_source:
                summary.standard_source_dist[result.standard_source] = \
                    summary.standard_source_dist.get(result.standard_source, 0) + 1
            if result.fast_source:
                summary.fast_source_dist[result.fast_source] = \
                    summary.fast_source_dist.get(result.fast_source, 0) + 1

            # Certainty ë¶„í¬
            if result.standard_certainty:
                summary.standard_certainty_dist[result.standard_certainty] = \
                    summary.standard_certainty_dist.get(result.standard_certainty, 0) + 1
            if result.fast_certainty:
                summary.fast_certainty_dist[result.fast_certainty] = \
                    summary.fast_certainty_dist.get(result.fast_certainty, 0) + 1

        # í‰ê·  ê³„ì‚°
        if summary.total_tests > 0:
            summary.standard_avg_time_ms /= summary.total_tests
            summary.fast_avg_time_ms /= summary.total_tests
            summary.standard_avg_llm_calls /= summary.total_tests
            summary.fast_avg_llm_calls /= summary.total_tests
            summary.standard_success_rate /= summary.total_tests
            summary.fast_success_rate /= summary.total_tests

        return summary

    def export_json(self, filename: str):
        """ê²°ê³¼ JSON ì €ì¥"""
        summary = self.summarize()
        data = {
            'timestamp': datetime.now().isoformat(),
            'summary': asdict(summary),
            'results': [asdict(r) for r in self.results]
        }

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def print_summary(self):
        """ìš”ì•½ ì¶œë ¥"""
        summary = self.summarize()

        print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
        print("ğŸ“Š Stage ê¸°ë°˜ A/B í…ŒìŠ¤íŠ¸ ìš”ì•½ (v7.11.0)")
        print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

        print(f"\nì „ì²´ í…ŒìŠ¤íŠ¸: {summary.total_tests}ê°œ")
        print(f"  - Standard Budget ìŠ¹ë¦¬: {summary.standard_wins}ê°œ")
        print(f"  - Fast Budget ìŠ¹ë¦¬: {summary.fast_wins}ê°œ")
        print(f"  - ë¬´ìŠ¹ë¶€: {summary.ties}ê°œ")

        print(f"\ní‰ê·  ì†ë„:")
        print(f"  - Standard: {summary.standard_avg_time_ms:.1f}ms")
        print(f"  - Fast: {summary.fast_avg_time_ms:.1f}ms")

        print(f"\ní‰ê·  LLM í˜¸ì¶œ:")
        print(f"  - Standard: {summary.standard_avg_llm_calls:.1f}íšŒ")
        print(f"  - Fast: {summary.fast_avg_llm_calls:.1f}íšŒ")

        print(f"\nì„±ê³µë¥ :")
        print(f"  - Standard: {summary.standard_success_rate*100:.1f}%")
        print(f"  - Fast: {summary.fast_success_rate*100:.1f}%")

        print(f"\nSource ë¶„í¬ (Standard):")
        for source, count in summary.standard_source_dist.items():
            print(f"  - {source}: {count}ê°œ")

        print(f"\nSource ë¶„í¬ (Fast):")
        for source, count in summary.fast_source_dist.items():
            print(f"  - {source}: {count}ê°œ")

        print(f"\nCertainty ë¶„í¬ (Standard):")
        for cert, count in summary.standard_certainty_dist.items():
            print(f"  - {cert}: {count}ê°œ")

        print(f"\nCertainty ë¶„í¬ (Fast):")
        for cert, count in summary.fast_certainty_dist.items():
            print(f"  - {cert}: {count}ê°œ")

        print("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")


# ============================================================
# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
# ============================================================

def get_standard_test_cases() -> List[TestCase]:
    """í‘œì¤€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
    return [
        TestCase(
            id="TC01",
            question="employees",
            project_data={'employees': 150},
            expected_value=150,
            tags=["literal", "definite"]
        ),
        TestCase(
            id="TC02",
            question="B2B SaaS í‰ê·  ARPUëŠ”?",
            domain="B2B_SaaS",
            expected_range=(50000, 200000),
            tags=["validator", "range"]
        ),
        TestCase(
            id="TC03",
            question="ì„œìš¸ ìŒì‹ì  ìˆ˜ëŠ”?",
            region="ì„œìš¸",
            expected_range=(80000, 120000),
            tags=["fermi", "decomposition"]
        ),
        TestCase(
            id="TC04",
            question="2025ë…„ AI ì±—ë´‡ ì„œë¹„ìŠ¤ í‰ê·  ARPUëŠ”?",
            domain="AI_Chatbot",
            expected_range=(10000, 50000),
            tags=["prior", "generative"]
        ),
        TestCase(
            id="TC05",
            question="ì„œìš¸ ì „ì²´ ìŒì‹ì  ë§¤ì¶œì€?",
            region="ì„œìš¸",
            expected_range=(5000000000000, 10000000000000),
            tags=["fermi", "complex"]
        ),
    ]


# ============================================================
# Pytest í…ŒìŠ¤íŠ¸
# ============================================================

class TestStageABFramework:
    """Stage ê¸°ë°˜ A/B í”„ë ˆì„ì›Œí¬ í…ŒìŠ¤íŠ¸"""

    @pytest.mark.skipif(
        not Path(__file__).parent.parent.parent.joinpath('.env').exists(),
        reason="API key required"
    )
    def test_ab_framework_basic(self):
        """ê¸°ë³¸ A/B í…ŒìŠ¤íŠ¸"""
        framework = StageABTestFramework()
        framework.add_test_cases(get_standard_test_cases())

        summary = framework.run_all()

        # ê¸°ë³¸ ê²€ì¦
        assert summary.total_tests == 5
        assert summary.standard_success_rate >= 0.6  # 60% ì´ìƒ ì„±ê³µ
        assert summary.fast_success_rate >= 0.6

        # ê²°ê³¼ ì¶œë ¥
        framework.print_summary()

    def test_ab_export_json(self, tmp_path):
        """JSON ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸"""
        framework = StageABTestFramework()
        framework.add_test_case(get_standard_test_cases()[0])  # TC01ë§Œ

        framework.run_all()

        output_file = tmp_path / "ab_results.json"
        framework.export_json(str(output_file))

        # JSON íŒŒì¼ ìƒì„± í™•ì¸
        assert output_file.exists()

        # JSON ë¡œë“œ í™•ì¸
        with open(output_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            assert 'summary' in data
            assert 'results' in data


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ
    framework = StageABTestFramework()
    framework.add_test_cases(get_standard_test_cases())
    framework.run_all()
    framework.print_summary()
    framework.export_json("stage_ab_results.json")

