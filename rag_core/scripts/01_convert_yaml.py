#!/usr/bin/env python3
"""
YAML â†’ RAG Chunks ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸

UMISì˜ YAML íŒŒì¼ì„ ì—ì´ì „íŠ¸ë³„ ê´€ì ìœ¼ë¡œ ì²­í‚¹í•˜ì—¬
RAG ì‹œìŠ¤í…œì— ìµœì í™”ëœ JSON Lines í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python scripts/01_convert_yaml.py

ì¶œë ¥:
    ../../data/chunks/explorer_chunks.jsonl  (Explorer ì „ìš© ì²­í¬)
    ../../data/chunks/observer_chunks.jsonl (í–¥í›„ í™•ì¥)
    ...
"""

import json
import sys
from pathlib import Path
from typing import Any, Dict, List

import yaml
from rich.console import Console
from rich.progress import track

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from umis_rag.utils.logger import logger

console = Console()


class UMISYAMLConverter:
    """
    UMIS YAML íŒŒì¼ì„ RAGìš© ì²­í¬ë¡œ ë³€í™˜í•˜ëŠ” ì»¨ë²„í„°
    
    ê°œë…:
    ------
    1. **ì²­í‚¹ ì „ëµ**: ì—ì´ì „íŠ¸ë³„ë¡œ ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ ê°™ì€ ë°ì´í„°ë¥¼ ì²­í‚¹
       - Explorer: ê¸°íšŒ/ì „ëµ ì¤‘ì‹¬
       - Observer: êµ¬ì¡°/íŒ¨í„´ ì¤‘ì‹¬ (í–¥í›„)
       - Quantifier: ì •ëŸ‰ ë°ì´í„° ì¤‘ì‹¬ (í–¥í›„)
    
    2. **ë©”íƒ€ë°ì´í„°**: ê° ì²­í¬ì— ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ë©”íƒ€ë°ì´í„° ì²¨ë¶€
       - agent: ì–´ëŠ ì—ì´ì „íŠ¸ìš©ì¸ê°€
       - pattern_id: ì–´ë–¤ íŒ¨í„´ì¸ê°€
       - keywords: ê²€ìƒ‰ í‚¤ì›Œë“œ
    
    3. **ì¶œë ¥ í˜•ì‹**: JSON Lines (.jsonl)
       - í•œ ì¤„ì— í•˜ë‚˜ì˜ ì²­í¬ (JSON ê°ì²´)
       - ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°€ëŠ¥
       - ëŒ€ìš©ëŸ‰ ë°ì´í„° íš¨ìœ¨ì 
    """
    
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.raw_dir = data_dir / "raw"
        self.chunks_dir = data_dir / "chunks"
        self.chunks_dir.mkdir(exist_ok=True)
        
        logger.info(f"Converter ì´ˆê¸°í™”: {self.raw_dir} â†’ {self.chunks_dir}")
    
    def load_yaml(self, filename: str) -> Dict[str, Any]:
        """YAML íŒŒì¼ ë¡œë“œ"""
        filepath = self.raw_dir / filename
        logger.info(f"YAML íŒŒì¼ ë¡œë”©: {filepath}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        logger.info(f"  âœ… {len(data)} ê°œ ìµœìƒìœ„ í‚¤ ë¡œë“œë¨")
        return data
    
    def convert_business_model_patterns_for_explorer(self) -> List[Dict[str, Any]]:
        """
        ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒ¨í„´ì„ Explorer ê´€ì ìœ¼ë¡œ ì²­í‚¹
        
        Explorerì˜ ê´€ì‹¬ì‚¬:
        - ì–´ë–¤ íŠ¸ë¦¬ê±° ì‹œê·¸ë„ì´ ì´ íŒ¨í„´ì„ ì‹œì‚¬í•˜ëŠ”ê°€?
        - ê¸°íšŒ êµ¬ì¡°ëŠ” ë¬´ì—‡ì¸ê°€?
        - ê²€ì¦ ë°©ë²•ì€?
        - ì„±ê³µ ì‚¬ë¡€ëŠ”?
        
        ì²­í‚¹ ì „ëµ:
        - 1ê°œ íŒ¨í„´ = ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
        - ì„¹ì…˜ë³„ë¡œ ë…ë¦½ ì²­í¬ (concept, triggers, structure, validation, cases)
        """
        logger.info("ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒ¨í„´ â†’ Explorer ì²­í¬ ë³€í™˜ ì‹œì‘")
        
        data = self.load_yaml("umis_business_model_patterns.yaml")
        chunks = []
        
        # 7ê°œ íŒ¨í„´ ID
        pattern_ids = [
            "platform_business_model",
            "subscription_model",
            "franchise_model",
            "direct_to_consumer_model",
            "advertising_model",
            "licensing_model",
            "freemium_model"
        ]
        
        for pattern_id in track(pattern_ids, description="íŒ¨í„´ ì²˜ë¦¬ ì¤‘..."):
            if pattern_id not in data:
                logger.warning(f"  âš ï¸  íŒ¨í„´ ì—†ìŒ: {pattern_id}")
                continue
            
            pattern = data[pattern_id]
            
            # ì²­í¬ 1: íŒ¨í„´ ê°œìš” (Concept + Triggers)
            chunks.append(self._create_pattern_overview_chunk(pattern_id, pattern))
            
            # ì²­í¬ 2: ê¸°íšŒ êµ¬ì¡° (Opportunity Structure)
            if "opportunity_structure" in pattern:
                chunks.append(self._create_opportunity_structure_chunk(pattern_id, pattern))
            
            # ì²­í¬ 3: ê²€ì¦ í”„ë ˆì„ì›Œí¬
            if "validation_framework" in pattern:
                chunks.append(self._create_validation_framework_chunk(pattern_id, pattern))
            
            # ì²­í¬ 4-N: ì„±ê³µ ì‚¬ë¡€ë“¤ (ê° ì‚¬ë¡€ë³„ë¡œ ë…ë¦½ ì²­í¬)
            if "success_case_library" in pattern:
                chunks.extend(self._create_case_chunks(pattern_id, pattern))
        
        logger.info(f"  âœ… ì´ {len(chunks)}ê°œ Explorer ì²­í¬ ìƒì„±")
        return chunks
    
    def _create_pattern_overview_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        íŒ¨í„´ ê°œìš” ì²­í¬ ìƒì„±
        
        í¬í•¨ ë‚´ìš©:
        - Concept (í•µì‹¬ ê°œë…)
        - Trigger Observations (íŠ¸ë¦¬ê±° ì‹œê·¸ë„)
        
        Explorerê°€ ì‚¬ìš©í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤:
        "Observerê°€ 'ë†’ì€ ì´ˆê¸° ë¹„ìš© + ì •ê¸° ìœ ì§€ê´€ë¦¬' ë°œê²¬"
        â†’ Explorerê°€ íŠ¸ë¦¬ê±° ê²€ìƒ‰
        â†’ subscription_model ë§¤ì¹­!
        """
        concept = pattern.get("concept", {})
        triggers = pattern.get("trigger_observations", {})
        
        # ì²­í¬ ì»¨í…ì¸  êµ¬ì„±
        content = f"""
## {concept.get('name', pattern_id)}

### í•µì‹¬ ê°œë…
- **ë³¸ì§ˆ**: {concept.get('essence', 'N/A')}
- **í•µì‹¬ ê°€ì¹˜**: {concept.get('core_value', 'N/A')}

### íŠ¸ë¦¬ê±° ì‹œê·¸ë„ (Observer ê´€ì°°ì—ì„œ ì°¾ì„ ì‹ í˜¸)
"""
        
        # íŠ¸ë¦¬ê±° ì‹œê·¸ë„ ì¶”ê°€
        if "signals" in triggers:
            for signal in triggers["signals"]:
                content += f"- {signal}\n"
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "chunk_id": f"{pattern_id}_overview",
            "chunk_type": "pattern_overview",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "business_model",
            "section": "concept_and_triggers",
            
            # ê²€ìƒ‰ ìµœì í™”
            "keywords": self._extract_keywords(concept),
            "triggers": triggers.get("signals", []) if isinstance(triggers.get("signals"), list) else [],
            
            # ë©”íƒ€ ì •ë³´
            "source_file": "umis_business_model_patterns.yaml",
            "token_count": len(content.split()),  # ëŒ€ëµì  í† í° ìˆ˜
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_opportunity_structure_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê¸°íšŒ êµ¬ì¡° ì²­í¬ ìƒì„±
        
        Explorerì˜ Phase 2 (ë‹¤ì°¨ì› ë¶„ì„)ì—ì„œ ì‚¬ìš©
        - ê°€ì¹˜ ì œì•ˆì€?
        - ìˆ˜ìµ ëª¨ë¸ì€?
        - êµ¬ì¡°ì  ìš”ê±´ì€?
        """
        opp_structure = pattern["opportunity_structure"]
        
        content = f"""
## {pattern.get('concept', {}).get('name', pattern_id)} - ê¸°íšŒ êµ¬ì¡°

### ê°€ì¹˜ ì œì•ˆ
"""
        
        # ê°€ì¹˜ ì œì•ˆ ì„¹ì…˜ ì¶”ê°€
        if "value_proposition" in opp_structure:
            vp = opp_structure["value_proposition"]
            for stakeholder, values in vp.items():
                content += f"\n**{stakeholder}**:\n"
                if isinstance(values, list):
                    for v in values:
                        content += f"- {v}\n"
        
        # ìˆ˜ìµ ëª¨ë¸
        if "revenue_models" in opp_structure:
            content += "\n### ìˆ˜ìµ ëª¨ë¸\n"
            for rev_model in opp_structure["revenue_models"]:
                if isinstance(rev_model, dict):
                    content += f"- **{rev_model.get('type', 'N/A')}**: {rev_model.get('mechanism', rev_model.get('structure', 'N/A'))}\n"
        
        # êµ¬ì¡°ì  ìš”ê±´
        if "structural_requirements" in opp_structure:
            content += "\n### êµ¬ì¡°ì  ìš”ê±´\n"
            content += yaml.dump(opp_structure["structural_requirements"], allow_unicode=True, default_flow_style=False)
        
        metadata = {
            "chunk_id": f"{pattern_id}_opportunity_structure",
            "chunk_type": "opportunity_structure",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "business_model",
            "section": "opportunity_structure",
            "source_file": "umis_business_model_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_validation_framework_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ê²€ì¦ í”„ë ˆì„ì›Œí¬ ì²­í¬
        
        Explorerê°€ ê°€ì„¤ ìƒì„± í›„ ê²€ì¦í•  ë•Œ ì‚¬ìš©
        - Quantifierì—ê²Œ ë­˜ ë¬¼ì–´ë´ì•¼ í•˜ë‚˜?
        - Validatorì—ê²Œ ë­˜ í™•ì¸í•´ì•¼ í•˜ë‚˜?
        - Observerì—ê²Œ ë­˜ ê²€ì¦ë°›ì•„ì•¼ í•˜ë‚˜?
        """
        val_framework = pattern["validation_framework"]
        
        content = f"""
## {pattern.get('concept', {}).get('name', pattern_id)} - ê²€ì¦ í”„ë ˆì„ì›Œí¬

### í˜‘ì—… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
"""
        
        content += yaml.dump(val_framework, allow_unicode=True, default_flow_style=False)
        
        metadata = {
            "chunk_id": f"{pattern_id}_validation",
            "chunk_type": "validation_framework",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "business_model",
            "section": "validation",
            
            # ê²€ì¦ í•„ìš” ì—ì´ì „íŠ¸ íƒœê·¸
            "validation_agents": ["quantifier", "validator", "observer"],
            
            "source_file": "umis_business_model_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_case_chunks(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        ì„±ê³µ ì‚¬ë¡€ë“¤ì„ ê°ê° ë…ë¦½ ì²­í¬ë¡œ ìƒì„±
        
        ì™œ ì‚¬ë¡€ë³„ë¡œ ë¶„í• ?
        - Explorerê°€ "ìœ ì‚¬í•œ ì‚°ì—… ì‚¬ë¡€" ê²€ìƒ‰ ì‹œ
        - íŠ¹ì • ì‚¬ë¡€ë§Œ ì •í™•í•˜ê²Œ ê²€ìƒ‰ ê°€ëŠ¥
        - ì˜ˆ: "ìŒì‹ ë°°ë‹¬" ê²€ìƒ‰ â†’ "ë°°ë‹¬ì˜ë¯¼ì¡±" ì²­í¬ë§Œ ë§¤ì¹­
        """
        cases_lib = pattern["success_case_library"]
        chunks = []
        
        for region in ["domestic", "global"]:
            if region not in cases_lib:
                continue
            
            cases = cases_lib[region]
            for company_name, case_data in cases.items():
                content = f"""
## ì„±ê³µ ì‚¬ë¡€: {company_name}

**íŒ¨í„´**: {pattern.get('concept', {}).get('name', pattern_id)}
**ì§€ì—­**: {region}

"""
                
                # ì‚¬ë¡€ ë°ì´í„°ë¥¼ YAMLë¡œ ì§ë ¬í™”
                content += yaml.dump(case_data, allow_unicode=True, default_flow_style=False)
                
                # í•µì‹¬ ì„±ê³µ ìš”ì¸ ì¶”ì¶œ
                csf = case_data.get("critical_success_factors", [])
                
                metadata = {
                    "chunk_id": f"{pattern_id}_case_{company_name}",
                    "chunk_type": "success_case",
                    "agent": "explorer",
                    "pattern_id": pattern_id,
                    "pattern_type": "business_model",
                    "section": "case_study",
                    
                    # ì‚¬ë¡€ íŠ¹í™” ë©”íƒ€ë°ì´í„°
                    "company": company_name,
                    "region": region,
                    "industry": case_data.get("market", "unknown"),
                    "critical_success_factors": csf if isinstance(csf, list) else [],
                    
                    "source_file": "umis_business_model_patterns.yaml",
                    "token_count": len(content.split()),
                }
                
                chunks.append({
                    "content": content.strip(),
                    "metadata": metadata
                })
        
        return chunks
    
    def convert_disruption_patterns_for_explorer(self) -> List[Dict[str, Any]]:
        """
        Disruption íŒ¨í„´ì„ Explorer ê´€ì ìœ¼ë¡œ ì²­í‚¹
        
        Explorerì˜ ê´€ì‹¬ì‚¬ (Disruption íŠ¹í™”):
        - 1ë“±ì˜ ì–´ë–¤ ì•½ì ì„ ê³µëµí•˜ë‚˜?
        - Counter-Positioning ë©”ì»¤ë‹ˆì¦˜ì€?
        - 1ë“±ì´ ëª» ë”°ë¼ì˜¤ëŠ” ì´ìœ ëŠ”?
        - ì‹¤ì œ ì¶”ì›” ì‚¬ë¡€ëŠ”?
        
        ì²­í‚¹ ì „ëµ:
        - 1ê°œ íŒ¨í„´ = ì—¬ëŸ¬ ì²­í¬
        - ì‚¬ë¡€ë³„ë¡œ ìƒì„¸ ì²­í‚¹ (ì‚¬ë¡€ê°€ ë§¤ìš° ì¤‘ìš”!)
        """
        logger.info("ğŸ”¥ Disruption íŒ¨í„´ â†’ Explorer ì²­í¬ ë³€í™˜ ì‹œì‘")
        
        data = self.load_yaml("umis_disruption_patterns.yaml")
        chunks = []
        
        # 5ê°œ Disruption íŒ¨í„´
        pattern_ids = [
            "innovation_disruption",
            "low_end_disruption",
            "channel_disruption",
            "experience_disruption",
            "continuous_innovation_disruption"
        ]
        
        for pattern_id in track(pattern_ids, description="Disruption íŒ¨í„´ ì²˜ë¦¬ ì¤‘..."):
            if pattern_id not in data:
                logger.warning(f"  âš ï¸  íŒ¨í„´ ì—†ìŒ: {pattern_id}")
                continue
            
            pattern = data[pattern_id]
            
            # ì²­í¬ 1: íŒ¨í„´ ê°œìš” + Incumbent Dilemma
            chunks.append(self._create_disruption_overview_chunk(pattern_id, pattern))
            
            # ì²­í¬ 2: Attacker Strategy
            if "attacker_strategy" in pattern:
                chunks.append(self._create_attacker_strategy_chunk(pattern_id, pattern))
            
            # ì²­í¬ 3: Validation Framework
            if "validation_framework" in pattern:
                chunks.append(self._create_disruption_validation_chunk(pattern_id, pattern))
            
            # ì²­í¬ 4-N: ì„±ê³µ ì‚¬ë¡€ë“¤ (Disruptionì€ ì‚¬ë¡€ê°€ í•µì‹¬!)
            if "success_case_library" in pattern:
                chunks.extend(self._create_disruption_case_chunks(pattern_id, pattern))
        
        logger.info(f"  âœ… ì´ {len(chunks)}ê°œ Disruption ì²­í¬ ìƒì„±")
        return chunks
    
    def _create_disruption_overview_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Disruption íŒ¨í„´ ê°œìš” ì²­í¬
        
        í•µì‹¬: Incumbent Dilemma (1ë“±ì˜ ë”œë ˆë§ˆ)
        """
        concept = pattern.get("concept", {})
        triggers = pattern.get("trigger_observations", {})
        incumbent_dilemma = pattern.get("incumbent_dilemma", {})
        
        content = f"""
## {concept.get('name', pattern_id)}

### í•µì‹¬ ê°œë…
- **ë³¸ì§ˆ**: {concept.get('essence', 'N/A')}
- **íƒœê·¸ë¼ì¸**: {concept.get('tagline', 'N/A')}

### íŠ¸ë¦¬ê±° ì‹œê·¸ë„ (1ë“±ì˜ ì•½ì  í¬ì°©)
"""
        
        # íŠ¸ë¦¬ê±° ì¶”ê°€
        if "incumbent_signals" in triggers:
            inc_signals = triggers["incumbent_signals"]
            for category, signals_list in inc_signals.items():
                content += f"\n**{category}**:\n"
                if isinstance(signals_list, list):
                    for signal in signals_list:
                        content += f"- {signal}\n"
                elif isinstance(signals_list, dict):
                    for key, value in signals_list.items():
                        if isinstance(value, list):
                            content += f"- {key}:\n"
                            for v in value:
                                content += f"  - {v}\n"
        
        # Incumbent Dilemma (í•µì‹¬!)
        content += "\n### 1ë“±ì˜ ë”œë ˆë§ˆ (ì™œ ëª» ë”°ë¼ì˜¤ë‚˜?)\n"
        if "why_they_cant_follow" in incumbent_dilemma:
            content += yaml.dump(
                incumbent_dilemma["why_they_cant_follow"], 
                allow_unicode=True, 
                default_flow_style=False
            )
        
        metadata = {
            "chunk_id": f"{pattern_id}_overview",
            "chunk_type": "disruption_overview",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "disruption",
            "section": "concept_and_dilemma",
            
            "keywords": self._extract_keywords(concept),
            "source_file": "umis_disruption_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_attacker_strategy_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Attacker ì „ëµ ì²­í¬ (ì–´ë–»ê²Œ ê³µëµí•˜ë‚˜?)"""
        strategy = pattern["attacker_strategy"]
        
        content = f"""
## {pattern.get('concept', {}).get('name', pattern_id)} - Attacker ì „ëµ

### ì‹¤í–‰ ë°©ë²•
"""
        content += yaml.dump(strategy, allow_unicode=True, default_flow_style=False)
        
        metadata = {
            "chunk_id": f"{pattern_id}_strategy",
            "chunk_type": "attacker_strategy",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "disruption",
            "section": "strategy",
            "source_file": "umis_disruption_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_disruption_validation_chunk(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Disruption ê²€ì¦ í”„ë ˆì„ì›Œí¬ (Counter-Positioning í…ŒìŠ¤íŠ¸!)"""
        val_framework = pattern["validation_framework"]
        
        content = f"""
## {pattern.get('concept', {}).get('name', pattern_id)} - ê²€ì¦ í”„ë ˆì„ì›Œí¬

### Counter-Positioning í…ŒìŠ¤íŠ¸
- ìš°ë¦¬ ì „ëµì„ 1ë“±ì´ ëª¨ë°©í•˜ë©´?
- 1ë“±ì—ê²Œ ë°œìƒí•  ì†í•´ëŠ”?
- ì†í•´ > ì´ìµì¸ê°€?

### ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
"""
        content += yaml.dump(val_framework, allow_unicode=True, default_flow_style=False)
        
        metadata = {
            "chunk_id": f"{pattern_id}_validation",
            "chunk_type": "disruption_validation",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "disruption",
            "section": "validation",
            "validation_agents": ["observer", "quantifier", "validator"],
            "source_file": "umis_disruption_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _create_disruption_case_chunks(
        self, 
        pattern_id: str, 
        pattern: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Disruption ì‚¬ë¡€ ì²­í¬ë“¤
        
        ì¤‘ìš”: Disruptionì€ ì‚¬ë¡€ê°€ ë§¤ìš° ìƒì„¸í•¨!
        - Incumbent vs Attacker êµ¬ì¡°
        - Counter-Positioning ë©”ì»¤ë‹ˆì¦˜
        - Timeline
        - ì‹¤ì œ ê²°ê³¼
        
        â†’ ê° ì‚¬ë¡€ë¥¼ 2-3ê°œ ì²­í¬ë¡œ ë¶„í• 
        """
        cases_lib = pattern.get("success_case_library", {})
        chunks = []
        
        for case_id, case_data in cases_lib.items():
            if case_id.startswith("case_"):
                # ë©”ì¸ ì‚¬ë¡€ ì²­í¬
                chunks.append(self._create_single_disruption_case(
                    pattern_id, 
                    case_id, 
                    case_data
                ))
        
        return chunks
    
    def _create_single_disruption_case(
        self, 
        pattern_id: str, 
        case_id: str, 
        case_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ Disruption ì‚¬ë¡€ ì²­í¬"""
        content = f"""
## Disruption ì‚¬ë¡€: {case_id}

**íŒ¨í„´**: {pattern_id}
**ì‹œì¥**: {case_data.get('market', 'N/A')}
**ê¸°ê°„**: {case_data.get('period', 'N/A')}
**ê²°ê³¼**: {case_data.get('outcome', 'N/A')}

### Counter-Positioning ë©”ì»¤ë‹ˆì¦˜
"""
        
        # Counter-Positioning ë©”ì»¤ë‹ˆì¦˜ (í•µì‹¬!)
        if "counter_positioning_mechanism" in case_data:
            content += yaml.dump(
                case_data["counter_positioning_mechanism"], 
                allow_unicode=True, 
                default_flow_style=False
            )
        
        # Timeline
        if "disruption_timeline" in case_data:
            content += "\n### Disruption Timeline\n"
            content += yaml.dump(
                case_data["disruption_timeline"], 
                allow_unicode=True, 
                default_flow_style=False
            )
        
        # Key Metrics
        if "key_metrics" in case_data:
            content += "\n### ì£¼ìš” ì§€í‘œ\n"
            content += yaml.dump(
                case_data["key_metrics"], 
                allow_unicode=True, 
                default_flow_style=False
            )
        
        # CSF
        csf = case_data.get("critical_success_factors", [])
        if csf:
            content += "\n### í•µì‹¬ ì„±ê³µ ìš”ì¸\n"
            for factor in csf:
                content += f"- {factor}\n"
        
        metadata = {
            "chunk_id": f"{pattern_id}_{case_id}",
            "chunk_type": "disruption_case",
            "agent": "explorer",
            "pattern_id": pattern_id,
            "pattern_type": "disruption",
            "section": "case_study",
            
            # ì‚¬ë¡€ ë©”íƒ€ë°ì´í„°
            "case_id": case_id,
            "market": case_data.get("market", "unknown"),
            "period": case_data.get("period", "unknown"),
            "outcome": case_data.get("outcome", "unknown"),
            "critical_success_factors": csf if isinstance(csf, list) else [],
            
            "source_file": "umis_disruption_patterns.yaml",
            "token_count": len(content.split()),
        }
        
        return {
            "content": content.strip(),
            "metadata": metadata
        }
    
    def _extract_keywords(self, concept: Dict[str, Any]) -> List[str]:
        """
        ì»¨ì…‰ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        ê°„ë‹¨í•œ ë°©ë²•: essenceì™€ core_valueì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        í–¥í›„ ê°œì„ : LLMìœ¼ë¡œ ìë™ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        keywords = []
        
        essence = concept.get("essence", "")
        core_value = concept.get("core_value", "")
        
        # ë‹¨ìˆœ ë‹¨ì–´ ë¶„í•  (í–¥í›„ í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ ê°œì„  ê°€ëŠ¥)
        text = f"{essence} {core_value}"
        words = text.replace(",", " ").split()
        
        # 2ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ
        keywords = [w.strip() for w in words if len(w.strip()) >= 2]
        
        return keywords[:10]  # ìµœëŒ€ 10ê°œ
    
    def save_chunks(self, chunks: List[Dict[str, Any]], filename: str) -> None:
        """
        ì²­í¬ë¥¼ JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥
        
        JSON Lines (.jsonl) í˜•ì‹:
        - í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON ê°ì²´
        - ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ê°€ëŠ¥
        - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— íš¨ìœ¨ì 
        
        ì˜ˆì‹œ:
        {"content": "...", "metadata": {...}}
        {"content": "...", "metadata": {...}}
        {"content": "...", "metadata": {...}}
        """
        filepath = self.chunks_dir / filename
        logger.info(f"ğŸ’¾ ì²­í¬ ì €ì¥ ì¤‘: {filepath}")
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for chunk in chunks:
                # í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON
                json.dump(chunk, f, ensure_ascii=False)
                f.write('\n')
        
        # í†µê³„ ì¶œë ¥
        total_tokens = sum(c["metadata"]["token_count"] for c in chunks)
        logger.info(f"  âœ… {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
        logger.info(f"  ğŸ“Š ì´ í† í° ìˆ˜: ~{total_tokens:,}")
        logger.info(f"  ğŸ“ íŒŒì¼ í¬ê¸°: {filepath.stat().st_size / 1024:.1f} KB")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    console.print("\n[bold blue]ğŸš€ UMIS YAML â†’ RAG Chunks ë³€í™˜[/bold blue]\n")
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ
    data_dir = project_root / "data"
    
    # ì»¨ë²„í„° ì´ˆê¸°í™”
    converter = UMISYAMLConverter(data_dir)
    
    # Phase 1: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒ¨í„´ â†’ Explorer ì²­í¬
    console.print("[yellow]ğŸ“Š Phase 1: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒ¨í„´ ë³€í™˜[/yellow]")
    explorer_bm_chunks = converter.convert_business_model_patterns_for_explorer()
    converter.save_chunks(explorer_bm_chunks, "explorer_business_models.jsonl")
    
    # Phase 2: Disruption íŒ¨í„´ â†’ Explorer ì²­í¬
    console.print("\n[yellow]ğŸ”¥ Phase 2: Disruption íŒ¨í„´ ë³€í™˜[/yellow]")
    explorer_dp_chunks = converter.convert_disruption_patterns_for_explorer()
    converter.save_chunks(explorer_dp_chunks, "explorer_disruption_patterns.jsonl")
    
    # TODO: Phase 3: Observer ê´€ì  ì²­í¬ (í–¥í›„)
    # TODO: Phase 4: Quantifier ê´€ì  ì²­í¬ (í–¥í›„)
    
    console.print("\n[bold green]âœ… ë³€í™˜ ì™„ë£Œ![/bold green]\n")
    console.print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {converter.chunks_dir}")
    console.print("\në‹¤ìŒ ë‹¨ê³„:")
    console.print("  python scripts/02_build_index.py --agent explorer")


if __name__ == "__main__":
    main()

