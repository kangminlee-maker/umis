#!/usr/bin/env python3
"""
Responses API ëª¨ë¸ ì „ì²´ í…ŒìŠ¤íŠ¸ + ê¸°ì¡´ ê²°ê³¼ ë³‘í•©
"""

import json
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark


def test_responses_api_full():
    """Responses API ëª¨ë¸ ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print("=" * 100)
    print("Responses API ëª¨ë¸ ì „ì²´ í…ŒìŠ¤íŠ¸")
    print("=" * 100)
    print()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    # Responses API ëª¨ë¸ë§Œ
    benchmark.models = {
        'openai_codex': ['gpt-5-codex', 'gpt-5.1-codex'],
        'openai_pro': ['gpt-5-pro'],
        'openai_thinking_pro': ['o1-pro']
    }
    
    print("âœ… Responses API ëª¨ë¸ ì „ì²´ í…ŒìŠ¤íŠ¸")
    print("   ëª¨ë¸: 4ê°œ")
    print("   ì‹œë‚˜ë¦¬ì˜¤: 7ê°œ (Phase 0-4)")
    print("   ì˜ˆìƒ ì†Œìš” ì‹œê°„: ~10ë¶„")
    print()
    
    try:
        benchmark.run_benchmark(category_filter=['openai_codex', 'openai_pro', 'openai_thinking_pro'])
        
        # ê²°ê³¼ ì €ì¥
        output_file = f"benchmark_responses_api_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        benchmark.save_results(output_file)
        
        # í†µê³„
        success = [r for r in benchmark.results if r.get('success', False)]
        print(f"\n{'='*100}")
        print("ğŸ“Š Responses API í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print(f"{'='*100}\n")
        print(f"ì´ í…ŒìŠ¤íŠ¸: {len(benchmark.results)}ê°œ")
        print(f"ì„±ê³µ: {len(success)}ê°œ ({len(success)/len(benchmark.results)*100:.1f}%)")
        print(f"ì‹¤íŒ¨: {len(benchmark.results) - len(success)}ê°œ")
        
        return output_file
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì¤‘ë‹¨ë¨")
        return None


def merge_with_previous(responses_file: str, previous_file: str = 'benchmark_merged_20251121_120819.json'):
    """Responses API ê²°ê³¼ì™€ ê¸°ì¡´ ê²°ê³¼ ë³‘í•©"""
    print(f"\n{'='*100}")
    print("ê²°ê³¼ ë³‘í•©")
    print(f"{'='*100}\n")
    
    # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ
    with open(previous_file, 'r') as f:
        previous_data = json.load(f)
    
    # Responses API ê²°ê³¼ ë¡œë“œ
    with open(responses_file, 'r') as f:
        responses_data = json.load(f)
    
    print(f"ê¸°ì¡´ ê²°ê³¼: {len(previous_data['results'])}ê°œ")
    print(f"Responses API ê²°ê³¼: {len(responses_data['results'])}ê°œ")
    
    # ë³‘í•© (Responses API ëª¨ë¸ì˜ ê¸°ì¡´ ì‹¤íŒ¨ ê²°ê³¼ë¥¼ ìƒˆ ê²°ê³¼ë¡œ êµì²´)
    results_dict = {}
    for r in previous_data['results']:
        key = (r.get('model'), r.get('scenario_id'))
        results_dict[key] = r
    
    # Responses API ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
    updated_count = 0
    for r in responses_data['results']:
        key = (r.get('model'), r.get('scenario_id'))
        if key in results_dict:
            results_dict[key] = r
            updated_count += 1
        else:
            results_dict[key] = r  # ìƒˆ ê²°ê³¼ ì¶”ê°€
    
    # ìµœì¢… ê²°ê³¼
    merged_results = list(results_dict.values())
    
    merged_data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(merged_results),
            'success_count': sum(1 for r in merged_results if r.get('success', False)),
            'previous_file': previous_file,
            'responses_file': responses_file,
            'updated_count': updated_count
        },
        'results': merged_results
    }
    
    # ì €ì¥
    output_file = f"benchmark_final_with_responses_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë³‘í•© ì™„ë£Œ: {output_file}")
    print(f"   ì´ ê²°ê³¼: {len(merged_results)}ê°œ")
    print(f"   ì„±ê³µ: {merged_data['metadata']['success_count']}ê°œ")
    print(f"   ì—…ë°ì´íŠ¸: {updated_count}ê°œ")
    
    # ìµœì¢… ë¦¬í¬íŠ¸
    generate_final_report(merged_data)
    
    return output_file


def generate_final_report(data):
    """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
    results = data['results']
    success = [r for r in results if r.get('success', False)]
    
    print(f"\n{'='*100}")
    print("ğŸ“Š ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸ (Responses API í¬í•¨)")
    print(f"{'='*100}\n")
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(success)}ê°œ ({len(success)/len(results)*100:.1f}%)")
    
    # Responses API ëª¨ë¸ ì„±ê³µë¥ 
    responses_models = ['gpt-5-codex', 'gpt-5.1-codex', 'gpt-5-pro', 'o1-pro']
    
    print(f"\n{'='*100}")
    print("Responses API ëª¨ë¸ ì„±ê³µë¥ ")
    print(f"{'='*100}\n")
    
    from collections import defaultdict
    model_stats = defaultdict(lambda: {'success': 0, 'total': 0})
    
    for r in results:
        model = r.get('model')
        if model in responses_models:
            model_stats[model]['total'] += 1
            if r.get('success', False):
                model_stats[model]['success'] += 1
    
    for model, stats in model_stats.items():
        success_rate = stats['success'] / stats['total'] * 100 if stats['total'] > 0 else 0
        status = "âœ…" if success_rate == 100 else "âš ï¸" if success_rate >= 50 else "âŒ"
        print(f"{status} {model:20s} | {stats['success']:2d}/{stats['total']:2d} | {success_rate:5.1f}%")
    
    # ê°€ì„±ë¹„ ë¶„ì„ (Responses API í¬í•¨)
    print(f"\n{'='*100}")
    print("ì „ì²´ ëª¨ë¸ ê°€ì„±ë¹„ TOP 15 (Responses API í¬í•¨)")
    print(f"{'='*100}\n")
    
    model_perf = defaultdict(lambda: {'costs': [], 'quality': []})
    
    for r in success:
        model = r['model']
        model_perf[model]['costs'].append(r.get('cost', 0))
        model_perf[model]['quality'].append(r.get('quality_score', {}).get('total_score', 0))
    
    model_avg = []
    for model, data_dict in model_perf.items():
        if not data_dict['costs']:
            continue
        
        avg_cost = sum(data_dict['costs']) / len(data_dict['costs'])
        avg_quality = sum(data_dict['quality']) / len(data_dict['quality'])
        efficiency = avg_quality / (avg_cost * 1000) if avg_cost > 0 else 0
        
        model_avg.append({
            'model': model,
            'avg_cost': avg_cost,
            'avg_quality': avg_quality,
            'efficiency': efficiency,
            'count': len(data_dict['costs'])
        })
    
    model_avg.sort(key=lambda x: x['efficiency'], reverse=True)
    
    for idx, m in enumerate(model_avg[:15], 1):
        print(f"   {idx:2d}. {m['model']:30s} | ê°€ì„±ë¹„: {m['efficiency']:7.1f} | í’ˆì§ˆ: {m['avg_quality']:5.1f} | ë¹„ìš©: ${m['avg_cost']:.6f}")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # 1. Responses API ëª¨ë¸ ì „ì²´ í…ŒìŠ¤íŠ¸
    responses_file = test_responses_api_full()
    
    if not responses_file:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ë˜ëŠ” ì¤‘ë‹¨ë¨")
        return
    
    # 2. ê¸°ì¡´ ê²°ê³¼ì™€ ë³‘í•©
    merge_with_previous(responses_file)
    
    print("\nğŸ‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

