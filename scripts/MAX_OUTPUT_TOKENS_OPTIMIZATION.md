# max_output_tokens 최적화 (2025-11-23)

## 📊 최적화 근거

### 실제 사용량 분석 (36개 테스트 케이스)
- **평균 출력**: 150-320 토큰
- **최대 출력**: 457 토큰
- **Mini 모델**: 150-220 토큰
- **Standard 모델**: 170-220 토큰
- **Premium 모델**: 200-320 토큰
- **Pro 모델**: 500-1,000 토큰 (예상)

---

## ✅ 최적화된 설정

| 모델 등급 | 이전 설정 | 새 설정 | 감소율 | 안전 버퍼 |
|-----------|-----------|---------|--------|-----------|
| **Mini** | 16,000 | 4,000 | -75% | 20배 ✅ |
| **Standard** | 16,000 | 8,000 | -50% | 35-40배 ✅ |
| **Premium** | 16,000 | 8,000-12,000 | -25~50% | 30-40배 ✅ |
| **Pro** | 100,000 | 32,000 | -68% | 32-64배 ✅ |
| **Ultra** | 272,000 | 48,000 | -82% | 48배 ✅ |

---

## 🎯 모델별 상세 설정

### Mini 모델 (4,000 토큰)
- `o1-mini`: 4,000 (실제 ~170토큰)
- `o3-mini`: 4,000 (실제 ~200토큰)
- `o3-mini-2025-01-31`: 4,000 (실제 ~170토큰)
- `gpt-4.1-mini`: 4,000 (실제 ~215토큰)

### Standard 모델 (8,000 토큰)
- `o1`: 8,000 (실제 ~170토큰)
- `o1-2024-12-17`: 8,000 (실제 ~190토큰)
- `o3`: 8,000 (실제 ~200토큰)
- `o3-2025-04-16`: 8,000 (실제 ~215토큰)
- `gpt-4.1`: 8,000 (실제 ~200토큰)

### Premium 모델 (8,000-12,000 토큰)
- `gpt-5.1`: 12,000 (실제 ~315토큰) ⭐
- `o4-mini`: 8,000 (실제 ~170토큰)
- `o4-mini-2025-04-16`: 8,000 (실제 ~160토큰)

### Pro 모델 (32,000 토큰)
- `o1-pro`: 32,000 (실제 500-1K 예상)
- `o1-pro-2025-03-19`: 32,000 (실제 500-1K 예상)

### Ultra Premium (48,000 토큰)
- `gpt-5-pro`: 48,000 (실제 500-1K 예상)
  - **중요**: 272K → 48K로 감소 (타임아웃 방지!)

---

## 💡 주요 개선 효과

### 1. ⚡ 타임아웃 방지
- **gpt-5-pro**: 272K → 48K
  - 이전: 10분 타임아웃 (Batch 3 문제 1)
  - 개선: 안정적 응답 예상
- **o1-pro**: 100K → 32K
  - 과도한 토큰 할당 제거
  - 더 빠른 응답

### 2. 🚀 응답 속도 개선
- 더 작은 토큰 제한 = 더 빠른 응답
- 불필요한 긴 응답 방지
- 모델이 핵심에 집중

### 3. 💰 비용 효율
- 실제 출력 토큰만 과금 (직접 영향 없음)
- 타임아웃 방지로 재시도 비용 절감
- 시간 절약 = 개발 효율 증가

### 4. 🎯 적절한 버퍼
```
최적값 = 실제 사용량 × 10-20배
```
- Mini: 20배 (충분)
- Standard: 35-40배 (안전)
- Premium: 30-40배 (균형)
- Pro: 32-64배 (여유)
- Ultra: 48배 (안정)

---

## 📈 실전 데이터

### Batch 1-5 테스트 결과
| 모델 | 평균 토큰 | 최대 토큰 | 새 설정 | 효율 |
|------|----------|----------|---------|------|
| gpt-5.1 | 315 | 457 | 12,000 | 2.6% → 3.8% ✅ |
| o3-2025-04-16 | 215 | 300 | 8,000 | 1.3% → 2.7% ✅ |
| gpt-4.1-mini | 213 | 279 | 4,000 | 1.3% → 5.3% ✅ |
| o1 | 172 | 217 | 8,000 | 1.1% → 2.2% ✅ |

**효율 개선**: 평균 1-2% → 2-5% (2-3배 향상)

---

## ⚠️ 주의사항

### 현재 실행 중인 테스트
- **Batch 3**: 이미 시작된 프로세스 (영향 없음)
- 기존 설정(272K, 100K)으로 계속 실행
- 다음 테스트부터 새 설정 적용

### 적용 범위
- `phase4_common.py` 수정
- 모든 Batch 스크립트 자동 적용:
  - test_phase4_batch1.py
  - test_phase4_batch2.py
  - test_phase4_batch3.py
  - test_phase4_batch4.py
  - test_phase4_batch5.py
  - test_phase4_batch6.py

---

## 🔄 롤백 방법

필요 시 `phase4_common.py`의 `MODEL_API_CONFIGS`에서 `max_output_tokens` 값만 수정:

```python
# 이전 설정으로 되돌리기
'o1-mini': {'max_output_tokens': 16000}  # 4000 → 16000
'gpt-5-pro': {'max_output_tokens': 272000}  # 48000 → 272000
```

---

## 📝 참고

### 최적화 원칙
1. 실제 사용량의 10-20배 할당 (버퍼)
2. Reasoning 토큰 고려 (이미 포함됨)
3. 너무 크면 타임아웃 위험
4. 실용적 범위: 4,000 ~ 48,000

### 벤치마크 기준
- 36개 테스트 케이스 (Batch 1-5)
- 10개 이상의 모델
- 3가지 effort 레벨 (low, medium, high)
- 실전 Fermi 추정 문제

---

**최종 업데이트**: 2025-11-23  
**적용 버전**: phase4_common.py v1.1
