#!/usr/bin/env python3
"""
GPT-5/5.1 Phase 0-4 ê³ ë‚œì´ë„ í…ŒìŠ¤íŠ¸
ì‹¤ì œ UMIS Estimator ìˆ˜ì¤€ì˜ ë¬¸í•­ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
"""

import sys
import os
import time
import json
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()


# =====================================
# Phase 0-4 í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ (ê³ ë‚œì´ë„)
# =====================================

PHASE_SCENARIOS = {
    'phase_0': {
        'name': 'Phase 0: ë³µì¡í•œ ë°ì´í„° ì¶”ì¶œ',
        'difficulty': 'Medium',
        'description': 'ì—¬ëŸ¬ ì¡°ê±´ì´ ì„ì¸ ë°ì´í„°ì—ì„œ ì •í™•í•œ ê°’ ì¶”ì¶œ',
        'prompt': '''ë‹¤ìŒ ì‹œì¥ ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS í‰ê·  ARPU (ì—°ê°„ êµ¬ë…)" ê°’ì„ ì¶”ì¶œí•˜ì„¸ìš”.

ë°ì´í„°:
- í•œêµ­ B2B SaaS ì›”ê°„ êµ¬ë… ARPU: 18,000ì›
- í•œêµ­ B2B SaaS ì—°ê°„ êµ¬ë… ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ì—°ê°„ êµ¬ë… ARPU: 70,000ì›
- ì¼ë³¸ B2B SaaS ì—°ê°„ êµ¬ë… ARPU: Â¥180,000
- í•œêµ­ B2B SaaS í‰ê·  í•´ì§€ìœ¨: 8.5%
- í•œêµ­ B2B SaaS í‰ê·  CAC: 450,000ì›

ì¡°ê±´:
1. B2Bë§Œ ì¶”ì¶œ (B2C ì œì™¸)
2. ì—°ê°„ êµ¬ë…ë§Œ ì¶”ì¶œ (ì›”ê°„ ì œì™¸)
3. í•œêµ­ë§Œ ì¶”ì¶œ (ì¼ë³¸ ì œì™¸)
4. ARPUë§Œ ì¶”ì¶œ (í•´ì§€ìœ¨, CAC ì œì™¸)

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"value": 200000, "unit": "ì›", "confidence": 1.0, "reasoning": "ì¶”ì¶œ ê·¼ê±°"}''',
        'expected': {
            'value': 200000,
            'unit': 'ì›',
            'confidence': 1.0
        }
    },
    
    'phase_1': {
        'name': 'Phase 1: ê°„ë‹¨í•œ ê³„ì‚°',
        'difficulty': 'Medium-High',
        'description': 'ì£¼ì–´ì§„ ê°’ë“¤ë¡œ ê°„ë‹¨í•œ ì‚°ìˆ  ê³„ì‚°',
        'prompt': '''ë‹¤ìŒ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ "í•œêµ­ B2B SaaS LTV"ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

ì£¼ì–´ì§„ ë°ì´í„°:
- í‰ê·  ARPU (ì—°ê°„): 200,000ì›
- í‰ê·  ê³ ê° ìœ ì§€ ê¸°ê°„: 3.2ë…„
- ì—°ê°„ í•´ì§€ìœ¨: 8.5%

ê³„ì‚°ì‹:
LTV = ARPU Ã— í‰ê·  ìœ ì§€ ê¸°ê°„
ë˜ëŠ”
LTV = ARPU / í•´ì§€ìœ¨ (ì—°ê°„)

ë‘ ë°©ë²• ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ê³„ì‚°í•˜ì„¸ìš”.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"value": <ê³„ì‚°ëœ_ê°’>, "unit": "ì›", "confidence": <0.7-1.0>, "method": "ì‚¬ìš©í•œ ê³„ì‚°ì‹", "reasoning": "ê³„ì‚° ê³¼ì •"}''',
        'expected': {
            'value_range': [640000, 2352941],  # ë‘ ë°©ë²•ì˜ ê²°ê³¼ ë²”ìœ„
            'unit': 'ì›',
            'min_confidence': 0.7
        }
    },
    
    'phase_2': {
        'name': 'Phase 2: ì§€ì‹ ê¸°ë°˜ ì¶”ë¡ ',
        'difficulty': 'High',
        'description': 'Validator RAG ìˆ˜ì¤€ì˜ ì •ì˜ ë° ë²¤ì¹˜ë§ˆí¬ í™œìš©',
        'prompt': '''ë‹¤ìŒ ìƒí™©ì—ì„œ "í•œêµ­ B2B í˜‘ì—… SaaS CAC"ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ìƒí™©:
- ì‚°ì—…: B2B í˜‘ì—… ë„êµ¬ (Slack, Notion ìœ ì‚¬)
- ì‹œì¥: í•œêµ­
- ê³ ê°: ì¤‘ì†Œê¸°ì—… (10-50ëª…)
- ë§ˆì¼€íŒ… ì±„ë„: ë””ì§€í„¸ ë§ˆì¼€íŒ… + Inside Sales

ì°¸ê³  ì •ë³´ (í™œìš© ê°€ëŠ¥):
- ë¯¸êµ­ B2B SaaS í‰ê·  CAC: $1,200 (ì•½ 1,560,000ì›)
- í•œêµ­ SaaS CACëŠ” ë¯¸êµ­ ëŒ€ë¹„ 30-40% ìˆ˜ì¤€
- B2B í˜‘ì—… ë„êµ¬ëŠ” ì¼ë°˜ SaaS ëŒ€ë¹„ CAC 20% ë‚®ìŒ (ë°”ì´ëŸ´ íš¨ê³¼)
- ì¤‘ì†Œê¸°ì—… íƒ€ê²Ÿì€ ëŒ€ê¸°ì—… ëŒ€ë¹„ CAC 40% ë‚®ìŒ

ê³„ì‚° ê³¼ì •:
1. ë¯¸êµ­ ê¸°ì¤€ê°’ ì„ íƒ
2. í•œêµ­ í• ì¸ ì ìš©
3. í˜‘ì—… ë„êµ¬ í• ì¸ ì ìš©
4. ì¤‘ì†Œê¸°ì—… í• ì¸ ì ìš©

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"value": <ì¶”ì •ê°’>, "unit": "ì›", "confidence": <0.5-0.8>, "method": "ê³„ì‚° ê³¼ì •", "assumptions": ["ê°€ì •1", "ê°€ì •2"]}''',
        'expected': {
            'value_range': [200000, 500000],  # í•©ë¦¬ì  ë²”ìœ„
            'unit': 'ì›',
            'min_confidence': 0.5
        }
    },
    
    'phase_3': {
        'name': 'Phase 3: ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ê³„ì‚°',
        'difficulty': 'High',
        'description': 'ì—¬ëŸ¬ ë³€ìˆ˜ë¥¼ ê³ ë ¤í•œ ì„±ì¥ë¥  ë¶„ì„',
        'prompt': '''ë‹¤ìŒ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ "3ë…„ í›„ í•œêµ­ B2B í˜‘ì—… SaaS ì‹œì¥ ê·œëª¨"ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

í˜„ì¬ ì‹œì¥ ì •ë³´ (2025ë…„):
- í˜„ì¬ ì‹œì¥ ê·œëª¨: 500ì–µì›
- í˜„ì¬ í™œì„± ê¸°ì—… ê³ ê°: 25,000ê°œ
- í‰ê·  ARPU: 200ë§Œì›/ë…„

ì„±ì¥ ìš”ì¸:
1. ì‹œì¥ ì„±ì¥ë¥ : ì—° 25% (íŒ¬ë°ë¯¹ ì´í›„ ê°€ì†)
2. ê³ ê° ì¦ê°€ìœ¨: ì—° 20%
3. ARPU ì¦ê°€ìœ¨: ì—° 5% (ê¸°ëŠ¥ ì¶”ê°€, ì—…ì…€ë§)

ê³„ì‚° ê³¼ì œ:
- ë‹¨ìˆœ ì„±ì¥ë¥  ì ìš© vs ë³µí•© ì„±ì¥ ê³ ë ¤
- 3ë…„ í›„ (2028ë…„) ì‹œì¥ ê·œëª¨ ì¶”ì •

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"value": <ì¶”ì •ê°’>, "unit": "ì–µì›", "confidence": <0.6-0.9>, "method": "ê³„ì‚°ì‹", "breakdown": {"ê³ ê°ìˆ˜": <ê°’>, "ARPU": <ê°’>}, "reasoning": "ê³„ì‚° ê·¼ê±°"}''',
        'expected': {
            'value_range': [900, 1200],  # ì–µì› ë‹¨ìœ„
            'unit': 'ì–µì›',
            'min_confidence': 0.6
        }
    },
    
    'phase_4': {
        'name': 'Phase 4: Fermi ì¶”ì • (ì™„ì „ ë¯¸ì§€ì˜ ê°’)',
        'difficulty': 'Very High',
        'description': 'ë°ì´í„° ì—†ì´ ìˆœìˆ˜ ë¶„í•´ì™€ ê°€ì •ìœ¼ë¡œ ì¶”ì •',
        'prompt': '''ë°ì´í„° ì—†ì´ "í•œêµ­ ê¸°ì—…ìš© í™”ìƒíšŒì˜ ì†”ë£¨ì…˜ TAM (Total Addressable Market)"ì„ ì¶”ì •í•˜ì„¸ìš”.

ì œì•½ ì¡°ê±´:
- ë°ì´í„°ëŠ” ì£¼ì–´ì§€ì§€ ì•ŠìŒ
- ìˆœìˆ˜ Fermi ë¶„í•´ ë°©ì‹ ì‚¬ìš©
- Top-down ë˜ëŠ” Bottom-up ì ‘ê·¼

ì¶”ì • ê°€ì´ë“œ (ì„ íƒ ì‚¬í•­):
1. Top-down: í•œêµ­ ì „ì²´ ê¸°ì—… ìˆ˜ â†’ ì ì¬ ê³ ê° â†’ ì§€ë¶ˆ ì˜í–¥ â†’ ê°€ê²©
2. Bottom-up: í‰ê·  ê¸°ì—… ê·œëª¨ â†’ ì‚¬ìš©ì ìˆ˜ â†’ ì¢Œì„ë‹¹ ê°€ê²© â†’ ì‹œì¥ ì¹¨íˆ¬ìœ¨

í•„ìˆ˜ í¬í•¨ ìš”ì†Œ:
- ëª…í™•í•œ ë¶„í•´ ë‹¨ê³„ (3ë‹¨ê³„ ì´ìƒ)
- ê° ë‹¨ê³„ë³„ ê°€ì •ê³¼ ê·¼ê±°
- ìµœì¢… ì¶”ì •ê°’ê³¼ ì‹ ë¢° êµ¬ê°„

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "value": <ì¶”ì •ê°’>,
    "unit": "ì–µì›",
    "confidence": <0.3-0.7>,
    "method": "top-down ë˜ëŠ” bottom-up",
    "decomposition": [
        {"step": "ë‹¨ê³„1", "assumption": "ê°€ì •", "value": <ê°’>},
        {"step": "ë‹¨ê³„2", "assumption": "ê°€ì •", "value": <ê°’>}
    ],
    "reasoning": "ì „ì²´ ì¶”ì • ë…¼ë¦¬",
    "confidence_range": {"min": <ìµœì†Œê°’>, "max": <ìµœëŒ€ê°’>}
}''',
        'expected': {
            'value_range': [500, 3000],  # ì–µì›, ë„“ì€ ë²”ìœ„ í—ˆìš©
            'unit': 'ì–µì›',
            'min_confidence': 0.3,
            'min_decomposition_steps': 3
        }
    }
}


def test_model_on_phase(client, model, api_type, phase_id, scenario, reasoning_config=None):
    """íŠ¹ì • Phaseì—ì„œ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    prompt = scenario['prompt']
    
    try:
        start = time.time()
        
        if api_type == 'responses':
            # Responses API
            api_params = {
                "model": model,
                "input": prompt,
            }
            
            if reasoning_config:
                api_params["reasoning"] = reasoning_config.get("reasoning", {"effort": "low"})
                api_params["text"] = reasoning_config.get("text", {"verbosity": "low"})
            else:
                api_params["reasoning"] = {"effort": "medium"}
                api_params["text"] = {"verbosity": "low"}
            
            response = client.responses.create(**api_params)
            
            # ì‘ë‹µ ì¶”ì¶œ
            if hasattr(response, 'output_text'):
                content = response.output_text
            elif hasattr(response, 'output'):
                content = response.output
            else:
                content = str(response)
            
            # í† í° ì •ë³´
            tokens = {
                'input': getattr(response, 'input_tokens', len(prompt) // 4),
                'output': getattr(response, 'output_tokens', len(content) // 4),
            }
            tokens['total'] = tokens['input'] + tokens['output']
            
        else:
            # Chat API
            api_params = {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
            }
            
            if reasoning_config and reasoning_config.get('reasoning_effort'):
                api_params["reasoning_effort"] = reasoning_config['reasoning_effort']
            
            response = client.chat.completions.create(**api_params)
            content = response.choices[0].message.content
            
            # í† í° ì •ë³´
            tokens = {
                'input': response.usage.prompt_tokens,
                'output': response.usage.completion_tokens,
                'total': response.usage.total_tokens
            }
            
            # reasoning_tokens ì¶”ê°€
            if hasattr(response.usage, 'completion_tokens_details'):
                details = response.usage.completion_tokens_details
                if hasattr(details, 'reasoning_tokens') and details.reasoning_tokens:
                    tokens['reasoning'] = details.reasoning_tokens
        
        elapsed = time.time() - start
        
        # JSON íŒŒì‹±
        import re
        
        try:
            if '```json' in content:
                json_start = content.find('```json') + 7
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()
            elif '```' in content:
                json_start = content.find('```') + 3
                json_end = content.find('```', json_start)
                content = content[json_start:json_end].strip()
            
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
            if json_match:
                content = json_match.group(0)
            
            parsed = json.loads(content)
        except Exception as e:
            parsed = {'raw': content[:200], 'parse_error': str(e)}
        
        # ë¹„ìš© ê³„ì‚°
        pricing = {
            'gpt-4.1-nano': {'input': 0.10, 'output': 0.40},
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
            'gpt-5': {'input': 1.25, 'output': 10.00},
            'gpt-5.1': {'input': 1.25, 'output': 10.00}
        }
        
        rates = pricing.get(model, {'input': 1.25, 'output': 10.00})
        cost = (tokens['input'] / 1_000_000 * rates['input'] + 
               tokens['output'] / 1_000_000 * rates['output'])
        
        # í’ˆì§ˆ í‰ê°€ (Phaseë³„ ê¸°ì¤€)
        quality_score = evaluate_phase_quality(phase_id, parsed, scenario['expected'])
        
        return {
            'success': True,
            'cost': cost,
            'elapsed_seconds': round(elapsed, 2),
            'quality_score': quality_score,
            'tokens': tokens,
            'response': parsed
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }


def evaluate_phase_quality(phase_id, response, expected):
    """Phaseë³„ í’ˆì§ˆ í‰ê°€ (100ì  ë§Œì )"""
    
    if 'parse_error' in response:
        return 0
    
    score = 0
    
    # ê¸°ë³¸ êµ¬ì¡° (25ì )
    if 'value' in response:
        score += 15
    if 'unit' in response:
        score += 5
    if 'confidence' in response:
        score += 5
    
    # Phase 0: ì •í™•í•œ ê°’ ì¶”ì¶œ (75ì )
    if phase_id == 'phase_0':
        if response.get('value') == expected['value']:
            score += 50  # ì •í™•í•œ ê°’
        if response.get('unit') == expected['unit']:
            score += 15
        if response.get('confidence') == expected['confidence']:
            score += 10
    
    # Phase 1: ê³„ì‚° ì •í™•ë„ (75ì )
    elif phase_id == 'phase_1':
        value = response.get('value', 0)
        if expected['value_range'][0] <= value <= expected['value_range'][1]:
            score += 40  # ë²”ìœ„ ë‚´
        if response.get('unit') == expected['unit']:
            score += 15
        if 'method' in response:
            score += 10
        if 'reasoning' in response:
            score += 10
    
    # Phase 2: ì¶”ë¡  í’ˆì§ˆ (75ì )
    elif phase_id == 'phase_2':
        value = response.get('value', 0)
        if expected['value_range'][0] <= value <= expected['value_range'][1]:
            score += 30
        if response.get('confidence', 0) >= expected['min_confidence']:
            score += 10
        if 'method' in response:
            score += 15
        if 'assumptions' in response and len(response.get('assumptions', [])) >= 2:
            score += 20
    
    # Phase 3: ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ (75ì )
    elif phase_id == 'phase_3':
        value = response.get('value', 0)
        if expected['value_range'][0] <= value <= expected['value_range'][1]:
            score += 30
        if 'method' in response:
            score += 15
        if 'breakdown' in response:
            score += 15
        if 'reasoning' in response:
            score += 15
    
    # Phase 4: Fermi ë¶„í•´ (75ì )
    elif phase_id == 'phase_4':
        value = response.get('value', 0)
        if expected['value_range'][0] <= value <= expected['value_range'][1]:
            score += 20
        
        decomp = response.get('decomposition', [])
        if len(decomp) >= expected['min_decomposition_steps']:
            score += 25
        
        if 'method' in response:
            score += 10
        if 'reasoning' in response:
            score += 10
        if 'confidence_range' in response:
            score += 10
    
    return min(score, 100)


def run_comprehensive_test():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("=" * 120)
    print("GPT-5/5.1 Phase 0-4 ê³ ë‚œì´ë„ í…ŒìŠ¤íŠ¸")
    print("=" * 120)
    print()
    
    client = OpenAI()
    
    # í…ŒìŠ¤íŠ¸ êµ¬ì„±
    test_configs = [
        # Chat API ëª¨ë¸
        ('gpt-4.1-nano', 'chat', None),
        ('gpt-4o-mini', 'chat', None),
        ('gpt-4.1-mini', 'chat', None),
        
        # Responses API - ìµœì  êµ¬ì„±
        ('gpt-5.1', 'responses', {
            'reasoning': {'effort': 'medium'},
            'text': {'verbosity': 'low'}
        }),
        
        # Chat API - reasoning effort
        ('gpt-5.1', 'chat', {'reasoning_effort': 'medium'}),
    ]
    
    results = []
    
    # Phaseë³„ í…ŒìŠ¤íŠ¸
    for phase_id, scenario in PHASE_SCENARIOS.items():
        print(f"\n{'='*120}")
        print(f"ğŸ”¬ {scenario['name']} (ë‚œì´ë„: {scenario['difficulty']})")
        print(f"{'='*120}")
        print(f"ğŸ“ {scenario['description']}")
        print()
        
        phase_results = []
        
        for model, api_type, config in test_configs:
            config_name = f"{model} ({api_type})"
            print(f"  í…ŒìŠ¤íŠ¸: {config_name}")
            
            result = test_model_on_phase(client, model, api_type, phase_id, scenario, config)
            
            result['model'] = model
            result['api_type'] = api_type
            result['phase'] = phase_id
            result['config_name'] = config_name
            
            phase_results.append(result)
            
            if result['success']:
                print(f"    âœ… ì„±ê³µ: í’ˆì§ˆ {result['quality_score']}/100, ë¹„ìš© ${result['cost']:.6f}, ì‹œê°„ {result['elapsed_seconds']}ì´ˆ")
            else:
                print(f"    âŒ ì‹¤íŒ¨: {result['error'][:50]}")
            
            time.sleep(2)
        
        results.extend(phase_results)
        
        # Phaseë³„ ìš”ì•½
        success_results = [r for r in phase_results if r['success']]
        if success_results:
            print(f"\n  ğŸ“Š {phase_id} ìš”ì•½:")
            print(f"  {'ëª¨ë¸':<20} | {'í’ˆì§ˆ':<8} | {'ë¹„ìš©':<12} | {'ì‹œê°„'}")
            print(f"  {'-'*60}")
            
            for r in sorted(success_results, key=lambda x: x['quality_score'], reverse=True):
                marker = "ğŸ†" if r['quality_score'] >= 90 else "â­" if r['quality_score'] >= 70 else "  "
                print(f"  {marker}{r['config_name']:<18} | {r['quality_score']:>6}/100 | ${r['cost']:<11.6f} | {r['elapsed_seconds']}ì´ˆ")
    
    # ì „ì²´ ë¶„ì„
    print(f"\n{'='*120}")
    print("ğŸ“Š ì „ì²´ ê²°ê³¼ ë¶„ì„")
    print(f"{'='*120}\n")
    
    success_results = [r for r in results if r['success']]
    
    # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
    from collections import defaultdict
    
    by_model = defaultdict(list)
    for r in success_results:
        by_model[r['config_name']].append(r)
    
    print("ğŸ† ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ (Phase 0-4 í‰ê· )\n")
    print(f"{'ìˆœìœ„':<4} | {'ëª¨ë¸':<25} | {'í‰ê·  í’ˆì§ˆ':<10} | {'í‰ê·  ë¹„ìš©':<12} | {'í‰ê·  ì‹œê°„':<10} | {'í•©ê³„'}")
    print("-" * 120)
    
    model_stats = []
    for model_name, model_results in by_model.items():
        avg_quality = sum(r['quality_score'] for r in model_results) / len(model_results)
        avg_cost = sum(r['cost'] for r in model_results) / len(model_results)
        avg_time = sum(r['elapsed_seconds'] for r in model_results) / len(model_results)
        total_cost = sum(r['cost'] for r in model_results)
        
        model_stats.append({
            'name': model_name,
            'avg_quality': avg_quality,
            'avg_cost': avg_cost,
            'avg_time': avg_time,
            'total_cost': total_cost,
            'count': len(model_results)
        })
    
    model_stats.sort(key=lambda x: x['avg_quality'], reverse=True)
    
    for i, stat in enumerate(model_stats, 1):
        marker = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
        print(f"{marker}{i:<3} | {stat['name']:<25} | {stat['avg_quality']:>8.1f}/100 | ${stat['avg_cost']:<11.6f} | {stat['avg_time']:>8.2f}ì´ˆ | ${stat['total_cost']:.6f}")
    
    # Phaseë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    print(f"\n\nğŸ¯ Phaseë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n")
    
    for phase_id, scenario in PHASE_SCENARIOS.items():
        phase_results = [r for r in success_results if r['phase'] == phase_id]
        if phase_results:
            best = max(phase_results, key=lambda x: x['quality_score'])
            print(f"{scenario['name']}")
            print(f"  ğŸ† {best['config_name']}: {best['quality_score']}/100 (ë¹„ìš©: ${best['cost']:.6f}, ì‹œê°„: {best['elapsed_seconds']}ì´ˆ)")
    
    # ì €ì¥
    output_file = f"benchmark_phase_0_4_advanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_type': 'phase_0_4_advanced',
                'total_tests': len(results),
                'success_count': len(success_results),
                'phases': list(PHASE_SCENARIOS.keys())
            },
            'scenarios': PHASE_SCENARIOS,
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\n\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    run_comprehensive_test()


