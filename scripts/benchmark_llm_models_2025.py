#!/usr/bin/env python3
"""
LLM ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (2025-11-20 ì—…ë°ì´íŠ¸)
OpenAI + Anthropic ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸
UMIS Estimator 5-Phase ì¶”ë¡  ëŠ¥ë ¥ í‰ê°€
"""

import os
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import OpenAI
import anthropic
import backoff

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


class LLMBenchmark2025:
    """
    LLM ëª¨ë¸ ì¢…í•© ë²¤ì¹˜ë§ˆí¬ (2025)
    """
    
    def __init__(self):
        self.openai_client = OpenAI()
        self.anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (2025-11-20 ìµœì‹ )
        self.models = {
            'openai_mini': [
                'gpt-4o-mini'
            ],
            'openai_standard': [
                'gpt-4o'
            ],
            'openai_thinking': [
                'o1-mini'
            ],
            'claude_haiku': [
                'claude-haiku-3.5'
            ],
            'claude_sonnet': [
                'claude-sonnet-3.5'
            ],
            'claude_opus': [
                'claude-opus-3'
            ]
        }
        
        # ê°€ê²© ì •ë³´ ($/1M í† í°) - 2025-11-20 ê¸°ì¤€
        self.pricing = {
            # OpenAI (Standard Tier) - í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-4o': {'input': 2.50, 'output': 10.00},
            'o1-mini': {'input': 1.10, 'output': 4.40},
            'o1': {'input': 15.00, 'output': 60.00},
            
            # Claude (Standard Tier) - í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸
            'claude-haiku-3.5': {'input': 0.80, 'output': 4.00},
            'claude-sonnet-3.5': {'input': 3.00, 'output': 15.00},
            'claude-opus-3': {'input': 15.00, 'output': 75.00}
        }
        
        # Claude API ì´ë¦„ ë§¤í•‘ (2025-11-21 ì—…ë°ì´íŠ¸)
        self.model_api_names = {
            # Claude
            'claude-haiku-3.5': 'claude-3-5-haiku-20241022',
            'claude-sonnet-3.5': 'claude-3-5-sonnet-20241022',
            'claude-sonnet-3.7': 'claude-3-7-sonnet-20250219',
            'claude-sonnet-4': 'claude-sonnet-4-20250514',
            'claude-sonnet-4.5': 'claude-sonnet-4-5-20250929',
            'claude-haiku-4.5': 'claude-haiku-4-5-20251001',
            'claude-opus-3': 'claude-3-opus-20240229',
            'claude-opus-4': 'claude-opus-4-20250514',
            'claude-opus-4.1': 'claude-opus-4-1-20250805'
        }
        
        # ê²°ê³¼ ì €ì¥
        self.results = []
    
    def run_full_benchmark(self, 
                          test_openai: bool = True,
                          test_claude: bool = True,
                          output_file: Optional[str] = None):
        """
        ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        """
        print("ğŸš€ LLM ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (2025-11-20)")
        print(f"   í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(self.get_test_scenarios())}ê°œ")
        
        total_models = 0
        if test_openai:
            total_models += sum(len(models) for cat, models in self.models.items() if 'openai' in cat)
        if test_claude:
            total_models += sum(len(models) for cat, models in self.models.items() if 'claude' in cat)
        
        print(f"   í…ŒìŠ¤íŠ¸ ëª¨ë¸: {total_models}ê°œ")
        print()
        
        scenarios = self.get_test_scenarios()
        
        for scenario_idx, scenario in enumerate(scenarios, 1):
            print(f"\n{'='*100}")
            print(f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx}/{len(scenarios)}: {scenario['name']}")
            print(f"{'='*100}")
            
            # OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸
            if test_openai:
                for category, models in self.models.items():
                    if 'openai' not in category:
                        continue
                    
                    print(f"\nğŸ“¦ {category}")
                    for model in models:
                        try:
                            result = self.test_openai_model(model, scenario)
                            self.results.append(result)
                            self._print_result(result)
                            
                            # Rate limiting: ë” ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš©
                            if model.startswith('o'):  # thinking ëª¨ë¸ì€ ë” ê¸´ ëŒ€ê¸°
                                time.sleep(3)
                            else:
                                time.sleep(1.5)
                        
                        except Exception as e:
                            print(f"   âŒ {model}: ì˜¤ë¥˜ - {str(e)}")
                            self.results.append({
                                'provider': 'openai',
                                'model': model,
                                'scenario': scenario['name'],
                                'error': str(e),
                                'timestamp': datetime.now().isoformat(),
                                'success': False
                            })
                            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ê¸´ ëŒ€ê¸°
                            time.sleep(3)
            
            # Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸
            if test_claude:
                for category, models in self.models.items():
                    if 'claude' not in category:
                        continue
                    
                    print(f"\nğŸ“¦ {category}")
                    for model in models:
                        try:
                            result = self.test_claude_model(model, scenario)
                            self.results.append(result)
                            self._print_result(result)
                            
                            # Rate limiting: Claudeë„ ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš©
                            time.sleep(2)
                        
                        except Exception as e:
                            print(f"   âŒ {model}: ì˜¤ë¥˜ - {str(e)}")
                            self.results.append({
                                'provider': 'claude',
                                'model': model,
                                'scenario': scenario['name'],
                                'error': str(e),
                                'timestamp': datetime.now().isoformat(),
                                'success': False
                            })
                            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ê¸´ ëŒ€ê¸°
                            time.sleep(3)
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(output_file)
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()
    
    def get_test_scenarios(self) -> List[Dict]:
        """
        UMIS Estimator 5-Phase í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        """
        return [
            {
                'id': 'phase0_literal',
                'name': 'Phase 0 (Literal Lookup)',
                'phase': 0,
                'category': 'simple',
                'prompt': '''ë‹¤ìŒ ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS ì›” ARPU"ë¥¼ ì°¾ì•„ ë°˜í™˜í•˜ì„¸ìš”:

ë°ì´í„°:
- ë¯¸êµ­ B2C SaaS ARPU: $50
- í•œêµ­ B2B SaaS ì›” ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ARPU: 70,000ì›

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 1.0, "source": "literal"}''',
                'expected': {
                    'value': 200000,
                    'unit': 'ì›',
                    'confidence': 1.0
                }
            },
            
            {
                'id': 'phase1_direct_rag',
                'name': 'Phase 1 (Direct RAG)',
                'phase': 1,
                'category': 'simple',
                'prompt': '''ë‹¤ìŒì€ RAG ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì½”ì›¨ì´ ë Œíƒˆ ARPUë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:

[ê²€ìƒ‰ ê²°ê³¼]
ì½”ì›¨ì´ 2024ë…„ ë Œíƒˆ ì‚¬ì—… ì‹¤ì :
- ì›” ë Œíƒˆë£Œ: 33,000ì›
- êµ¬ë…ì ìˆ˜: 720ë§Œ ëª…
- ë Œíƒˆ ë§¤ì¶œ: 2.85ì¡°ì›

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 1.0, "source": "rag"}''',
                'expected': {
                    'value': 33000,
                    'unit': 'ì›',
                    'confidence': 1.0
                }
            },
            
            {
                'id': 'phase2_validator_search',
                'name': 'Phase 2 (Validator Search + Calculation)',
                'phase': 2,
                'category': 'simple',
                'prompt': '''ë‹¤ìŒ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬ LTVë¥¼ ê³„ì‚°í•˜ì„¸ìš”:

ê³µì‹: LTV = ARPU / Churn_Rate

ì£¼ì–´ì§„ ê°’:
- ARPU: 80,000ì›
- Churn_Rate: 0.05 (ì›” 5%)

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "formula": "ARPU/Churn", "confidence": 1.0}''',
                'expected': {
                    'value': 1600000,
                    'confidence': 1.0
                }
            },
            
            {
                'id': 'phase3_template',
                'name': 'Phase 3 (Guestimation - Template)',
                'phase': 3,
                'category': 'medium',
                'prompt': '''B2B SaaS í•œêµ­ ì‹œì¥ ARPUë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  í…œí”Œë¦¿:
- ê¸€ë¡œë²Œ B2B SaaS ARPU: $100
- í•œêµ­ GDP per capita: ê¸€ë¡œë²Œ ëŒ€ë¹„ 60%
- B2B premium: B2C ëŒ€ë¹„ 3ë°°

ì¶”ì • ëª¨í˜•:
1. ê¸€ë¡œë²Œ ê¸°ì¤€ ì¡°ì •: $100 Ã— 0.6 = $60
2. B2B premium ì ìš©: $60 Ã— 3 = $180
3. í™˜ìœ¨ ì ìš©: $180 Ã— 1,300 = 234,000ì›
4. ë°˜ì˜¬ë¦¼: 200,000ì›

ì´ í…œí”Œë¦¿ì„ ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "reasoning": "í•œ ë¬¸ì¥", "models": ["model1"]}''',
                'expected': {
                    'value_range': [150000, 250000],
                    'confidence_min': 0.70
                }
            },
            
            {
                'id': 'phase3_no_template',
                'name': 'Phase 3 (Guestimation - No Template)',
                'phase': 3,
                'category': 'medium',
                'prompt': '''í•œêµ­ ì˜¨ë¼ì¸ ì„±ì¸ ì·¨ë¯¸ êµìœ¡ í”Œë«í¼ì˜ ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤ ì‚¬í•­:
- íƒ€ê²Ÿ: ì§ì¥ì¸, 30-40ëŒ€
- ê²½ìŸì‚¬: í´ë˜ìŠ¤101, íƒˆì‰ ë“±
- ì½˜í…ì¸ : ì•…ê¸°, ë¯¸ìˆ , ìš”ë¦¬ ë“±

ì°½ì˜ì ìœ¼ë¡œ ëª¨í˜•ì„ ë§Œë“¤ì–´ ë‹µë³€í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "reasoning": "ìš”ì•½", "models": ["model1", "model2"]}''',
                'expected': {
                    'value_range': [10000, 50000],
                    'confidence_min': 0.60
                }
            },
            
            {
                'id': 'phase4_simple_fermi',
                'name': 'Phase 4 (Simple Fermi Decomposition)',
                'phase': 4,
                'category': 'complex',
                'prompt': '''ì„œìš¸ì˜ í”¼ì•„ë…¸ í•™ì› ìˆ˜ë¥¼ Fermi ë¶„í•´ë¡œ ì¶”ì •í•˜ì„¸ìš”.

ë‹¨ê³„:
1. ì–´ë–¤ ë³€ìˆ˜ê°€ í•„ìš”í•œê°€?
2. ê° ë³€ìˆ˜ë¥¼ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?
3. ì–´ë–¤ ëª¨í˜•ì„ ì‚¬ìš©í• ê¹Œ?
4. ê²°ê³¼ê°€ í•©ë¦¬ì ì¸ê°€?

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ê°œ", "confidence": 0.0-1.0, "decomposition": {"var1": ê°’, "var2": ê°’}, "models": [...], "reasoning": "ìš”ì•½"}''',
                'expected': {
                    'value_range': [1500, 4000],
                    'confidence_min': 0.60
                }
            },
            
            {
                'id': 'phase4_complex_fermi',
                'name': 'Phase 4 (Complex Fermi - Multi-layer)',
                'phase': 4,
                'category': 'very_complex',
                'prompt': '''í•œêµ­ ì„±ì¸ í”¼ì•„ë…¸ í•™ìŠµìì˜ ì—°ê°„ ì´ ì§€ì¶œì•¡ì„ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤í•  ìš”ì†Œ:
- í•™ìŠµì ìˆ˜ (ì–´ë–»ê²Œ ì¶”ì •?)
- í•™ì›ë¹„ (ì›” í‰ê· )
- êµì¬ë¹„ (ì—°ê°„)
- ì•…ê¸° êµ¬ë§¤/ë Œíƒˆ (ë¹„ìœ¨)
- ê¸°íƒ€ ë¹„ìš© (ì¡°ìœ¨, ì•¡ì„¸ì„œë¦¬ ë“±)

ì—¬ëŸ¬ ëª¨í˜•ì„ ì‹œë„í•˜ê³ , ì¬ê·€ì ìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "decomposition": {...}, "models": [...], "recursive_estimates": {...}, "reasoning_detail": "ìƒì„¸ ì„¤ëª…"}''',
                'expected': {
                    'value_range': [50000000000, 500000000000],  # 500ì–µ-5000ì–µ
                    'confidence_min': 0.50
                }
            },
            
            {
                'id': 'phase4_creative_synthesis',
                'name': 'Phase 4 (Creative Synthesis)',
                'phase': 4,
                'category': 'very_complex',
                'prompt': '''í•œêµ­ì—ì„œ "êµ¬ë…í˜• í”¼ì•„ë…¸ ë Œíƒˆ + ì˜¨ë¼ì¸ ë ˆìŠ¨" ê²°í•© ì„œë¹„ìŠ¤ì˜ ì ì • ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤ ì‚¬í•­:
- ê¸°ì¡´ í”¼ì•„ë…¸ ë Œíƒˆë¹„
- ê¸°ì¡´ ëŒ€ë©´ ë ˆìŠ¨ë¹„
- ì˜¨ë¼ì¸ í• ì¸ìœ¨
- ê²°í•© í• ì¸
- WTP (ì§€ë¶ˆ ì˜í–¥)
- ê²½ìŸ ëŒ€ì•ˆ

ì°½ì˜ì ìœ¼ë¡œ ì—¬ëŸ¬ ì ‘ê·¼ì„ ì‹œë„í•˜ê³ , ìµœì¢… ê°’ì— ìˆ˜ë ´í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "models": [...], "creative_approaches": [...], "final_reasoning": "ì¢…í•© íŒë‹¨"}''',
                'expected': {
                    'value_range': [80000, 200000],
                    'confidence_min': 0.55
                }
            }
        ]
    
    @backoff.on_exception(
        backoff.expo,
        (Exception),
        max_tries=3,
        max_time=30,
        giveup=lambda e: "429" not in str(e) and "rate limit" not in str(e).lower() and "timeout" not in str(e).lower()
    )
    def _call_openai_with_retry(self, api_params: Dict) -> Any:
        """OpenAI API í˜¸ì¶œ with retry"""
        return self.openai_client.chat.completions.create(**api_params)
    
    def test_openai_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """OpenAI ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # ëª¨ë¸ íƒ€ì… êµ¬ë¶„
            is_o_series = model.startswith(('o1', 'o3', 'o4'))  # o1/o3/o4 ì‹œë¦¬ì¦ˆ
            is_gpt5 = model.startswith('gpt-5')  # gpt-5 ì‹œë¦¬ì¦ˆ
            is_reasoning_model = is_o_series or is_gpt5
            
            messages = [{"role": "user", "content": scenario['prompt']}]
            
            if not is_reasoning_model:
                messages.insert(0, {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."
                })
            
            # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            api_params = {
                "model": model,
                "messages": messages
            }
            
            # íŒŒë¼ë¯¸í„° ì¶”ê°€ (ëª¨ë¸ë³„ ì°¨ë³„í™”)
            if is_reasoning_model:
                # o1/o3/o4: low/medium/high, gpt-5: minimal/low/medium/high
                if is_o_series:
                    api_params["reasoning_effort"] = "medium"  # o ì‹œë¦¬ì¦ˆ ê¸°ë³¸ê°’
                else:  # gpt-5
                    api_params["reasoning_effort"] = "low"  # gpt-5 ê· í˜•ì¡íŒ ì„¤ì •
            else:
                # ì¼ë°˜ ëª¨ë¸: temperature ì‚¬ìš©
                api_params["temperature"] = 0.2
                api_params["response_format"] = {"type": "json_object"}
            
            # API í˜¸ì¶œ with retry
            response = self._call_openai_with_retry(api_params)
            
            elapsed = time.time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ ì‹œë„ (```json ... ``` ë¸”ë¡ ë˜ëŠ” ì¼ë°˜ JSON)
            try:
                # ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                parsed = json.loads(content)
            except json.JSONDecodeError:
                parsed = {'raw_response': content, 'parse_error': True}
            
            # ë¹„ìš© ê³„ì‚°
            usage = response.usage
            cost = self._calculate_cost(
                model,
                usage.prompt_tokens,
                usage.completion_tokens
            )
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = self._evaluate_quality(parsed, scenario.get('expected', {}))
            
            return {
                'provider': 'openai',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'category': scenario['category'],
                'response': parsed,
                'expected': scenario.get('expected'),
                'quality_score': quality_score,
                'tokens': {
                    'input': usage.prompt_tokens,
                    'output': usage.completion_tokens,
                    'total': usage.total_tokens
                },
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            elapsed = time.time() - start_time
            return {
                'provider': 'openai',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'error': str(e),
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
    
    @backoff.on_exception(
        backoff.expo,
        (Exception),
        max_tries=3,
        max_time=30,
        giveup=lambda e: "429" not in str(e) and "rate limit" not in str(e).lower() and "timeout" not in str(e).lower()
    )
    def _call_claude_with_retry(self, api_params: Dict) -> Any:
        """Claude API í˜¸ì¶œ with retry"""
        return self.anthropic_client.messages.create(**api_params)
    
    def test_claude_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # API ëª¨ë¸ ì´ë¦„ ë³€í™˜
            api_model = self.model_api_names.get(model, model)
            
            # API í˜¸ì¶œ íŒŒë¼ë¯¸í„° êµ¬ì„±
            api_params = {
                "model": api_model,
                "max_tokens": 2048,
                "temperature": 0.2,
                "system": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.",
                "messages": [
                    {"role": "user", "content": scenario['prompt']}
                ]
            }
            
            # API í˜¸ì¶œ with retry
            response = self._call_claude_with_retry(api_params)
            
            elapsed = time.time() - start_time
            
            # refusal ì¤‘ì§€ ì´ìœ  ì²˜ë¦¬ (Claude 4.5 ìš”êµ¬ì‚¬í•­)
            if response.stop_reason == "refusal":
                return {
                    'provider': 'claude',
                    'model': model,
                    'scenario_id': scenario['id'],
                    'scenario_name': scenario['name'],
                    'phase': scenario['phase'],
                    'category': scenario['category'],
                    'error': 'Model refused to respond (safety/policy)',
                    'stop_reason': 'refusal',
                    'elapsed_seconds': round(elapsed, 2),
                    'timestamp': datetime.now().isoformat(),
                    'success': False
                }
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.content[0].text
            
            # JSON ì¶”ì¶œ ì‹œë„ (```json ... ``` ë¸”ë¡ ë˜ëŠ” ì¼ë°˜ JSON)
            try:
                # ì½”ë“œ ë¸”ë¡ ë‚´ JSON ì¶”ì¶œ
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                
                parsed = json.loads(content)
            except json.JSONDecodeError:
                parsed = {'raw_response': content, 'parse_error': True}
            
            # ë¹„ìš© ê³„ì‚°
            cost = self._calculate_cost(
                model,
                response.usage.input_tokens,
                response.usage.output_tokens
            )
            
            # í’ˆì§ˆ í‰ê°€
            quality_score = self._evaluate_quality(parsed, scenario.get('expected', {}))
            
            return {
                'provider': 'claude',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'category': scenario['category'],
                'response': parsed,
                'expected': scenario.get('expected'),
                'quality_score': quality_score,
                'tokens': {
                    'input': response.usage.input_tokens,
                    'output': response.usage.output_tokens,
                    'total': response.usage.input_tokens + response.usage.output_tokens
                },
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            elapsed = time.time() - start_time
            return {
                'provider': 'claude',
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'error': str(e),
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        if model not in self.pricing:
            return 0.0
        
        rates = self.pricing[model]
        cost = (input_tokens / 1_000_000 * rates['input'] +
                output_tokens / 1_000_000 * rates['output'])
        return round(cost, 6)
    
    def _evaluate_quality(self, response: Dict, expected: Dict) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€"""
        score = {
            'has_value': 'value' in response,
            'has_confidence': 'confidence' in response,
            'has_reasoning': 'reasoning' in response or 'reasoning_detail' in response,
            'has_models': 'models' in response or 'decomposition' in response,
            'json_valid': 'parse_error' not in response,
            'value_in_range': False,
            'confidence_sufficient': False
        }
        
        # ê°’ ë²”ìœ„ ì²´í¬
        if score['has_value'] and 'value_range' in expected:
            value = response.get('value')
            if isinstance(value, (int, float)):
                min_val, max_val = expected['value_range']
                score['value_in_range'] = min_val <= value <= max_val
        elif score['has_value'] and 'value' in expected:
            score['value_in_range'] = response.get('value') == expected['value']
        
        # ì‹ ë¢°ë„ ì²´í¬
        if score['has_confidence'] and 'confidence_min' in expected:
            confidence = response.get('confidence', 0)
            score['confidence_sufficient'] = confidence >= expected['confidence_min']
        
        # ì´ì  ê³„ì‚° (0-100)
        total_score = 0
        if score['json_valid']: total_score += 20
        if score['has_value']: total_score += 20
        if score['has_confidence']: total_score += 15
        if score['has_reasoning']: total_score += 15
        if score['has_models']: total_score += 10
        if score['value_in_range']: total_score += 15
        if score['confidence_sufficient']: total_score += 5
        
        score['total_score'] = total_score
        
        return score
    
    def _print_result(self, result: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        if not result['success']:
            print(f"   âŒ {result['model']}: {result.get('error', 'Unknown error')}")
            return
        
        response = result['response']
        quality = result['quality_score']
        
        print(f"\n   âœ… {result['model']}")
        print(f"      ë¹„ìš©: ${result['cost']:.6f}")
        print(f"      ì‹œê°„: {result['elapsed_seconds']}ì´ˆ")
        print(f"      í† í°: {result['tokens']['total']} ({result['tokens']['input']}â†’{result['tokens']['output']})")
        print(f"      í’ˆì§ˆ: {quality['total_score']}/100")
        
        if 'value' in response:
            print(f"      ë‹µë³€: {response.get('value')} {response.get('unit', '')}")
            print(f"      ì‹ ë¢°ë„: {response.get('confidence', 'N/A')}")
        
        if 'reasoning' in response:
            reasoning = response['reasoning'][:80]
            print(f"      ê·¼ê±°: {reasoning}...")
    
    def save_results(self, output_file: Optional[str] = None):
        """ê²°ê³¼ ì €ì¥"""
        if output_file is None:
            output_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_tests': len(self.results),
                    'success_count': sum(1 for r in self.results if r['success']),
                    'pricing_date': '2025-11-20'
                },
                'results': self.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"   ì´ {len(self.results)}ê°œ í…ŒìŠ¤íŠ¸")
    
    def generate_report(self):
        """ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n{'='*100}")
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ (2025-11-20)")
        print(f"{'='*100}\n")
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_results = [r for r in self.results if r['success']]
        total_count = len(self.results)
        success_count = len(success_results)
        
        print(f"ì´ í…ŒìŠ¤íŠ¸: {total_count}ê°œ")
        print(f"ì„±ê³µ: {success_count}ê°œ ({success_count/total_count*100:.1f}%)")
        print(f"ì‹¤íŒ¨: {total_count - success_count}ê°œ")
        
        # Phaseë³„ ì„±ëŠ¥
        print(f"\n{'='*100}")
        print("Phaseë³„ ìµœì  ëª¨ë¸")
        print(f"{'='*100}\n")
        
        phases = {}
        for result in success_results:
            phase = result.get('phase', -1)
            if phase not in phases:
                phases[phase] = []
            phases[phase].append(result)
        
        for phase in sorted(phases.keys()):
            results = phases[phase]
            print(f"\nğŸ”¹ Phase {phase}")
            
            # í’ˆì§ˆ ì ìˆ˜ ìˆœ ì •ë ¬
            sorted_by_quality = sorted(results, key=lambda r: r['quality_score']['total_score'], reverse=True)
            
            print(f"   TOP 3 (í’ˆì§ˆ ê¸°ì¤€):")
            for idx, result in enumerate(sorted_by_quality[:3], 1):
                model = result['model']
                score = result['quality_score']['total_score']
                cost = result['cost']
                elapsed = result['elapsed_seconds']
                print(f"   {idx}ìœ„: {model:25s} | í’ˆì§ˆ: {score:3d}/100 | ${cost:.6f} | {elapsed:4.1f}ì´ˆ")
            
            # ë¹„ìš© ìˆœ ì •ë ¬
            sorted_by_cost = sorted(results, key=lambda r: r['cost'])
            
            print(f"\n   TOP 3 (ë¹„ìš© ê¸°ì¤€):")
            for idx, result in enumerate(sorted_by_cost[:3], 1):
                model = result['model']
                score = result['quality_score']['total_score']
                cost = result['cost']
                print(f"   {idx}ìœ„: {model:25s} | ${cost:.6f} | í’ˆì§ˆ: {score:3d}/100")
        
        # ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥
        print(f"\n{'='*100}")
        print("ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥")
        print(f"{'='*100}\n")
        
        model_stats = {}
        for result in success_results:
            model = result['model']
            if model not in model_stats:
                model_stats[model] = {
                    'provider': result['provider'],
                    'costs': [],
                    'times': [],
                    'quality_scores': []
                }
            
            model_stats[model]['costs'].append(result['cost'])
            model_stats[model]['times'].append(result['elapsed_seconds'])
            model_stats[model]['quality_scores'].append(result['quality_score']['total_score'])
        
        # í‰ê·  ê³„ì‚°
        model_averages = []
        for model, stats in model_stats.items():
            if not stats['costs']:
                continue
            
            avg_cost = sum(stats['costs']) / len(stats['costs'])
            avg_time = sum(stats['times']) / len(stats['times'])
            avg_quality = sum(stats['quality_scores']) / len(stats['quality_scores'])
            
            # ê°€ì„±ë¹„ ì ìˆ˜ (í’ˆì§ˆ/ë¹„ìš©)
            cost_efficiency = avg_quality / (avg_cost * 1000) if avg_cost > 0 else 0
            
            model_averages.append({
                'model': model,
                'provider': stats['provider'],
                'avg_cost': avg_cost,
                'avg_time': avg_time,
                'avg_quality': avg_quality,
                'cost_efficiency': cost_efficiency,
                'test_count': len(stats['costs'])
            })
        
        # ê°€ì„±ë¹„ ìˆœ ì •ë ¬
        model_averages.sort(key=lambda m: m['cost_efficiency'], reverse=True)
        
        print(f"{'ëª¨ë¸':30s} | {'ì œê³µì‚¬':10s} | {'í‰ê·  ë¹„ìš©':12s} | {'í‰ê·  í’ˆì§ˆ':10s} | {'ê°€ì„±ë¹„':10s} | {'í…ŒìŠ¤íŠ¸'}")
        print("-" * 100)
        
        for avg in model_averages:
            provider_emoji = "ğŸ”µ" if avg['provider'] == 'openai' else "ğŸŸ£"
            print(f"{provider_emoji} {avg['model']:27s} | {avg['provider']:10s} | ${avg['avg_cost']:.6f}   | {avg['avg_quality']:6.1f}/100 | {avg['cost_efficiency']:8.1f}   | {avg['test_count']}ê°œ")
        
        # ìµœì¢… ê¶Œì¥
        print(f"\n{'='*100}")
        print("ğŸ† ìµœì¢… ê¶Œì¥ (UMISìš©)")
        print(f"{'='*100}\n")
        
        print("ğŸ’ ìµœê³  ê°€ì„±ë¹„ TOP 5:")
        for idx, avg in enumerate(model_averages[:5], 1):
            provider_emoji = "ğŸ”µ" if avg['provider'] == 'openai' else "ğŸŸ£"
            print(f"   {idx}ìœ„: {provider_emoji} {avg['model']:25s} | ê°€ì„±ë¹„: {avg['cost_efficiency']:6.1f} | í’ˆì§ˆ: {avg['avg_quality']:5.1f}/100 | ë¹„ìš©: ${avg['avg_cost']:.6f}")
        
        # ë¹„ìš© ê¸°ì¤€
        print("\nğŸ’° ìµœì € ë¹„ìš© TOP 3:")
        sorted_by_cost = sorted(model_averages, key=lambda m: m['avg_cost'])
        for idx, avg in enumerate(sorted_by_cost[:3], 1):
            provider_emoji = "ğŸ”µ" if avg['provider'] == 'openai' else "ğŸŸ£"
            print(f"   {idx}ìœ„: {provider_emoji} {avg['model']:25s} ${avg['avg_cost']:.6f}/ì‘ì—… | í’ˆì§ˆ: {avg['avg_quality']:5.1f}/100")
        
        # í’ˆì§ˆ ê¸°ì¤€
        print("\nğŸ¯ ìµœê³  í’ˆì§ˆ TOP 3:")
        sorted_by_quality = sorted(model_averages, key=lambda m: m['avg_quality'], reverse=True)
        for idx, avg in enumerate(sorted_by_quality[:3], 1):
            provider_emoji = "ğŸ”µ" if avg['provider'] == 'openai' else "ğŸŸ£"
            print(f"   {idx}ìœ„: {provider_emoji} {avg['model']:25s} {avg['avg_quality']:.1f}/100 | ë¹„ìš©: ${avg['avg_cost']:.6f}/ì‘ì—…")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 100)
    print("LLM ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (2025-11-20)")
    print("=" * 100)
    print()
    
    # API í‚¤ í™•ì¸
    openai_key = os.getenv('OPENAI_API_KEY')
    anthropic_key = os.getenv('ANTHROPIC_API_KEY')
    
    test_openai = bool(openai_key)
    test_claude = bool(anthropic_key)
    
    if not test_openai and not test_claude:
        print("âŒ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì— OPENAI_API_KEY ë˜ëŠ” ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return
    
    if test_openai:
        print(f"âœ… OpenAI API í‚¤ í™•ì¸ë¨")
    if test_claude:
        print(f"âœ… Anthropic API í‚¤ í™•ì¸ë¨")
    print()
    
    # ì‚¬ìš©ì ì„ íƒ
    print("í…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. ì „ì²´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (ëŠë¦¼, ë¹„ìŒˆ)")
    print("2. í•µì‹¬ ëª¨ë¸ë§Œ í…ŒìŠ¤íŠ¸ (ê¶Œì¥)")
    print("3. ì»¤ìŠ¤í…€ ì„ íƒ")
    
    choice = input("\nì„ íƒ (1-3): ").strip()
    
    benchmark = LLMBenchmark2025()
    
    if choice == '2':
        # í•µì‹¬ ëª¨ë¸ë§Œ (ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸)
        benchmark.models = {
            'openai_mini': ['gpt-4o-mini'],
            'openai_standard': ['gpt-4o'],
            'openai_thinking': ['o1-mini'],
            'claude_haiku': ['claude-haiku-3.5'],
            'claude_sonnet': ['claude-sonnet-3.5'],
            'claude_opus': ['claude-opus-3']
        }
    
    try:
        benchmark.run_full_benchmark(
            test_openai=test_openai,
            test_claude=test_claude
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"   í˜„ì¬ê¹Œì§€ {len(benchmark.results)}ê°œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        if benchmark.results:
            save = input("\nê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if save.lower() == 'y':
                benchmark.save_results('benchmark_results_partial.json')
    
    print("\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

