#!/usr/bin/env python3
"""
Responses API í†µí•© í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì˜ˆì¸¡ í¬í•¨)
ëª¨ë“  Responses API ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark


# Phase 0 ì‹¤ì¸¡ ë°ì´í„° (ì´ˆ)
PHASE0_TIMES = {
    'gpt-5-codex': 1.85,
    'gpt-5.1-codex': 1.44,
    'gpt-5': 8.71,
    'gpt-5.1': 1.84,
    'gpt-5-pro': 73.69,
    'o1-pro': 30.22
}

# Rate limiting (ì´ˆ)
RATE_LIMITING = {
    'codex': 2,
    'pro': 3,
    'thinking': 3,
    'standard': 2
}


def estimate_test_time(models, num_scenarios=1):
    """í…ŒìŠ¤íŠ¸ ì†Œìš” ì‹œê°„ ì˜ˆì¸¡"""
    total_time = 0
    details = []
    
    for model in models:
        # Phase 0 ì‹¤ì¸¡ ì‹œê°„
        base_time = PHASE0_TIMES.get(model, 2.0)  # ê¸°ë³¸ê°’ 2ì´ˆ
        
        # Rate limiting ê²°ì •
        if 'codex' in model:
            rl = RATE_LIMITING['codex']
        elif 'o1-pro' in model or 'o3-pro' in model:
            rl = RATE_LIMITING['thinking']
        elif 'pro' in model:
            rl = RATE_LIMITING['pro']
        else:
            rl = RATE_LIMITING['standard']
        
        # ì‹œë‚˜ë¦¬ì˜¤ë‹¹ ì‹œê°„
        time_per_scenario = base_time + rl
        
        # ì „ì²´ ì‹œê°„
        model_total = time_per_scenario * num_scenarios
        total_time += model_total
        
        details.append({
            'model': model,
            'per_test': base_time,
            'rate_limit': rl,
            'per_scenario': time_per_scenario,
            'total': model_total,
            'minutes': model_total / 60,
            'percentage': 0  # ë‚˜ì¤‘ì— ê³„ì‚°
        })
    
    # ë¹„ìœ¨ ê³„ì‚°
    for d in details:
        d['percentage'] = (d['total'] / total_time * 100) if total_time > 0 else 0
    
    return total_time, details


def print_time_estimate(models, num_scenarios=1):
    """ì˜ˆìƒ ì‹œê°„ ì¶œë ¥"""
    total_time, details = estimate_test_time(models, num_scenarios)
    
    print(f"\n{'='*80}")
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: {total_time:.0f}ì´ˆ = {total_time/60:.1f}ë¶„")
    print(f"{'='*80}\n")
    
    print(f"{'ëª¨ë¸':<20} | {'í…ŒìŠ¤íŠ¸':<8} | {'ëŒ€ê¸°':<6} | {'ì‹œë‚˜ë¦¬ì˜¤ë‹¹':<10} | {'ì „ì²´':<12} | {'ë¹„ìœ¨'}")
    print("-" * 80)
    
    for d in sorted(details, key=lambda x: x['total'], reverse=True):
        bar_length = int(d['percentage'] / 5)  # 20ì¹¸ ë°”
        bar = 'â–ˆ' * bar_length + 'â–‘' * (20 - bar_length)
        
        print(f"{d['model']:<20} | {d['per_test']:>6.2f}ì´ˆ | {d['rate_limit']:>4d}ì´ˆ | "
              f"{d['per_scenario']:>8.2f}ì´ˆ | {d['total']:>6.0f}ì´ˆ ({d['minutes']:>4.1f}ë¶„) | "
              f"{bar} {d['percentage']:>4.1f}%")
    
    print()
    
    # ë³‘ëª© êµ¬ê°„ ê²½ê³ 
    slow_models = [d for d in details if d['per_test'] > 10]
    if slow_models:
        print("âš ï¸  ëŠë¦° ëª¨ë¸ ê²½ê³ :")
        for d in slow_models:
            print(f"   - {d['model']}: {d['per_test']:.1f}ì´ˆ/í…ŒìŠ¤íŠ¸ "
                  f"(ì „ì²´ì˜ {d['percentage']:.0f}% ì°¨ì§€)")
        print()
    
    return total_time


def test_responses_api_unified():
    """Responses API í†µí•© í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("Responses API í†µí•© í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    # í…ŒìŠ¤íŠ¸í•  ëª¨ë“  Responses API ëª¨ë¸
    all_models = [
        'gpt-5-codex',
        'gpt-5.1-codex',
        'gpt-5',
        'gpt-5.1',
        'gpt-5-pro',
        'o1-pro'
    ]
    
    # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    benchmark.responses_api_models = all_models
    
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ëª¨ë¸: 6ê°œ")
    for i, model in enumerate(all_models, 1):
        status = ""
        if 'pro' in model:
            status = "âš ï¸ ëŠë¦¼"
        elif model == 'gpt-5.1':
            status = "â­ ê¶Œì¥"
        print(f"   {i}. {model:<20} {status}")
    print()
    
    # í…ŒìŠ¤íŠ¸ ì˜µì…˜ ì„ íƒ
    print("í…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. Phase 0ë§Œ (ë¹ ë¥¸ ê²€ì¦, ~2.5ë¶„)")
    print("2. ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ (ì™„ì „í•œ í‰ê°€, ~15ë¶„)")
    print("3. gpt-5.1ë§Œ (ì‹¤ìš©ì , ~13ì´ˆ)")
    print("4. Pro ëª¨ë¸ ì œì™¸ (ì¶”ì²œ, ~0.5ë¶„)")
    print()
    
    choice = input("ì„ íƒ (1-4, ê¸°ë³¸=1): ").strip() or "1"
    
    if choice == '2':
        num_scenarios = 7
        test_models = all_models
    elif choice == '3':
        num_scenarios = 1
        test_models = ['gpt-5.1']
    elif choice == '4':
        num_scenarios = 1
        test_models = ['gpt-5-codex', 'gpt-5.1-codex', 'gpt-5', 'gpt-5.1']
    else:
        num_scenarios = 1
        test_models = all_models
    
    # ì˜ˆìƒ ì‹œê°„ ì¶œë ¥
    estimated_time = print_time_estimate(test_models, num_scenarios)
    
    # í™•ì¸
    if estimated_time > 300:  # 5ë¶„ ì´ìƒ
        confirm = input(f"\nâš ï¸  ì˜ˆìƒ ì‹œê°„ì´ {estimated_time/60:.1f}ë¶„ì…ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if confirm.lower() != 'y':
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì·¨ì†Œë¨")
            return
    
    print(f"\nğŸš€ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    scenarios = benchmark.get_test_scenarios()[:num_scenarios]
    results = []
    
    start_time = time.time()
    
    for scenario_idx, scenario in enumerate(scenarios, 1):
        print(f"\n{'='*80}")
        print(f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx}/{num_scenarios}: {scenario['name']}")
        print(f"{'='*80}\n")
        
        for model_idx, model in enumerate(test_models, 1):
            print(f"[{model_idx}/{len(test_models)}] í…ŒìŠ¤íŠ¸: {model}")
            
            try:
                result = benchmark.test_openai_model(model, scenario)
                results.append(result)
                
                if result['success']:
                    print(f"   âœ… ì„±ê³µ!")
                    print(f"      API: {result.get('api_type', 'chat')}")
                    print(f"      ë¹„ìš©: ${result['cost']:.6f}")
                    print(f"      ì‹œê°„: {result['elapsed_seconds']:.2f}ì´ˆ")
                    print(f"      í’ˆì§ˆ: {result['quality_score']['total_score']}/100")
                else:
                    error = result.get('error', '')[:80]
                    print(f"   âŒ ì˜¤ë¥˜: {error}")
                
                # Rate limiting
                if 'pro' in model:
                    time.sleep(3)
                else:
                    time.sleep(2)
            
            except Exception as e:
                print(f"   âŒ ì˜ˆì™¸: {str(e)[:80]}")
                results.append({
                    'model': model,
                    'scenario_id': scenario['id'],
                    'error': str(e),
                    'success': False
                })
                time.sleep(2)
    
    elapsed_time = time.time() - start_time
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*80}")
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}\n")
    
    success = [r for r in results if r.get('success', False)]
    failed = [r for r in results if not r.get('success', False)]
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(success)}ê°œ ({len(success)/len(results)*100:.1f}%)")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
    print(f"ì‹¤ì œ ì†Œìš” ì‹œê°„: {elapsed_time:.0f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)")
    print(f"ì˜ˆìƒ ì‹œê°„: {estimated_time:.0f}ì´ˆ ({estimated_time/60:.1f}ë¶„)")
    print(f"ì˜¤ì°¨: {abs(elapsed_time - estimated_time):.0f}ì´ˆ ({abs(elapsed_time - estimated_time)/estimated_time*100:.1f}%)")
    print()
    
    if success:
        print("âœ… ì„±ê³µí•œ ëª¨ë¸:")
        
        # ê°€ì„±ë¹„ ìˆœ ì •ë ¬
        success_sorted = sorted(success, 
                               key=lambda r: r['quality_score']['total_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0,
                               reverse=True)
        
        print(f"\n{'ëª¨ë¸':<20} | {'ë¹„ìš©':<12} | {'ì‹œê°„':<10} | {'í’ˆì§ˆ':<8} | {'ê°€ì„±ë¹„'}")
        print("-" * 70)
        
        for r in success_sorted:
            efficiency = r['quality_score']['total_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0
            marker = "â­" if r['model'] == 'gpt-5.1' else "  "
            print(f"{marker} {r['model']:<18} | ${r['cost']:<11.6f} | "
                  f"{r['elapsed_seconds']:<9.2f}ì´ˆ | "
                  f"{r['quality_score']['total_score']:>6}/100 | {efficiency:>8.1f}")
        print()
    
    if failed:
        print("âŒ ì‹¤íŒ¨í•œ ëª¨ë¸:")
        for r in failed:
            error = r.get('error', '')[:60]
            print(f"   - {r['model']}: {error}")
        print()
    
    # ì €ì¥
    output_file = f"benchmark_responses_unified_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    import json
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'total_tests': len(results),
                'success_count': len(success),
                'elapsed_time': elapsed_time,
                'estimated_time': estimated_time
            },
            'results': results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥: {output_file}")
    print()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_responses_api_unified()

