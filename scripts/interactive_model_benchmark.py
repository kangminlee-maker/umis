#!/usr/bin/env python3
"""
OpenAI ëª¨ë¸ ì¸í„°ë™í‹°ë¸Œ ë²¤ì¹˜ë§ˆí¬
ì‚¬ìš©ìê°€ ê° ì‘ë‹µì„ í‰ê°€í•˜ë©° ì§„í–‰
"""

import os
import json
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import OpenAI

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


class InteractiveBenchmark:
    """
    ì¸í„°ë™í‹°ë¸Œ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    """
    
    def __init__(self):
        # API í‚¤ë§Œ .envì—ì„œ ë¡œë“œ, ëª¨ë¸ì€ ì§ì ‘ ì§€ì •
        self.client = OpenAI()  # OPENAI_API_KEY ìë™ ë¡œë“œ
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.models_to_test = [
            # Tier 1: ì´ˆì €ê°€ (ìµœìš°ì„  í…ŒìŠ¤íŠ¸)
            'gpt-5-nano',
            'gpt-4.1-nano',
            'gpt-4o-mini',
            
            # Tier 2: ì¤‘ê¸‰
            'gpt-5-mini',
            'gpt-4.1-mini',
            'gpt-4o',
            'gpt-4.1',
            
            # Tier 3: Thinking
            'o1-mini',
            'o3-mini',
            'o3'
        ]
        
        # ê°€ê²© ì •ë³´
        self.pricing = {
            'gpt-5-nano': {'input': 0.05, 'output': 0.40},
            'gpt-4.1-nano': {'input': 0.10, 'output': 0.40},
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-5-mini': {'input': 0.25, 'output': 2.00},
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
            'gpt-4o': {'input': 2.50, 'output': 10.00},
            'gpt-4.1': {'input': 2.00, 'output': 8.00},
            'gpt-5.1': {'input': 1.25, 'output': 10.00},
            'o1-mini': {'input': 1.10, 'output': 4.40},
            'o3-mini': {'input': 1.10, 'output': 4.40},
            'o4-mini': {'input': 1.10, 'output': 4.40},
            'o3': {'input': 2.00, 'output': 8.00}
        }
        
        self.results = []
    
    def get_test_scenarios(self) -> List[Dict]:
        """UMIS í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤"""
        return [
            {
                'id': 'SC-001',
                'name': 'Phase 0 (Literal - í™•ì • ë°ì´í„° ì¡°íšŒ)',
                'phase': 0,
                'complexity': 'very_simple',
                'prompt': '''ë‹¤ìŒ ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS ARPU"ë¥¼ ì°¾ì•„ ë°˜í™˜í•˜ì„¸ìš”:

ë°ì´í„°:
- ë¯¸êµ­ B2C SaaS ARPU: $50
- í•œêµ­ B2B SaaS ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ARPU: 70,000ì›

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 1.0}''',
                'expected_answer': '200,000ì›',
                'evaluation_criteria': [
                    'ì •í™•í•œ ê°’ (200,000)',
                    'JSON í˜•ì‹ ì¤€ìˆ˜',
                    'ë¹ ë¥¸ ì‘ë‹µ (<2ì´ˆ)'
                ]
            },
            
            {
                'id': 'SC-002',
                'name': 'Phase 2 (ê³µì‹ ê³„ì‚°)',
                'phase': 2,
                'complexity': 'simple',
                'prompt': '''ë‹¤ìŒ ê³µì‹ì„ ê³„ì‚°í•˜ì„¸ìš”:

LTV = ARPU / Churn_Rate

ì£¼ì–´ì§„ ê°’:
- ARPU: 80,000ì›
- Churn_Rate: 0.05 (5%)

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "formula": "ARPU / Churn_Rate", "confidence": 1.0}''',
                'expected_answer': '1,600,000ì›',
                'evaluation_criteria': [
                    'ì •í™•í•œ ê³„ì‚° (1,600,000)',
                    'ê³µì‹ ì´í•´',
                    'JSON í˜•ì‹'
                ]
            },
            
            {
                'id': 'SC-003',
                'name': 'Phase 3 (í…œí”Œë¦¿ ìˆìŒ - ë²¤ì¹˜ë§ˆí¬ ì¡°ì •)',
                'phase': 3,
                'complexity': 'medium',
                'prompt': '''B2B SaaS í•œêµ­ ì‹œì¥ í‰ê·  ARPUë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  ì˜ˆì‹œ:
ì§ˆë¬¸: "B2B SaaS ë¯¸êµ­ ARPUëŠ”?"
ë‹¨ê³„:
1. ê¸€ë¡œë²Œ ë²¤ì¹˜ë§ˆí¬: $100 (ì•Œë ¤ì§)
2. ê²°ê³¼: $100

ì´ì œ í•œêµ­ ì‹œì¥ì„ ê°™ì€ ë°©ì‹ìœ¼ë¡œ:

íŒíŠ¸:
- ê¸€ë¡œë²Œ B2B SaaS ARPU: ~$100
- í•œêµ­ vs ê¸€ë¡œë²Œ GDP per capita: ì•½ 60%
- B2B vs B2C ë°°ìˆ˜: ì•½ 3ë°°

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.6-0.8, "reasoning": "í•œ ë¬¸ì¥ ìš”ì•½"}''',
                'expected_answer': '150,000-250,000ì› (Â±30%)',
                'evaluation_criteria': [
                    'í•©ë¦¬ì  ë²”ìœ„ (15-25ë§Œì›)',
                    'ë…¼ë¦¬ì  ê·¼ê±° ì œì‹œ',
                    'Confidence ì ì ˆ (0.6-0.8)',
                    'ë‹¨ê³„ë³„ ê³„ì‚°'
                ]
            },
            
            {
                'id': 'SC-004',
                'name': 'Phase 3 (í…œí”Œë¦¿ ì—†ìŒ - ì°½ì˜ì  ì¶”ì •)',
                'phase': 3,
                'complexity': 'complex',
                'prompt': '''í•œêµ­ ì„±ì¸ ì˜¨ë¼ì¸ í”¼ì•„ë…¸ ê°•ì¢Œì˜ ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤ ì‚¬í•­:
- íƒ€ê²Ÿ: ì„±ì¸ ì·¨ë¯¸ í•™ìŠµì
- ê²½ìŸì‚¬: í´ë˜ìŠ¤101, íƒˆì‰ ë“±
- ì‚¬ìš©ì: ì§ì¥ì¸, 30-40ëŒ€
- ì‹œì¥: í•œêµ­

ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  JSONìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "reasoning": "ìš”ì•½"}''',
                'expected_answer': '20,000-50,000ì›',
                'evaluation_criteria': [
                    'í•©ë¦¬ì  ë²”ìœ„ (2-5ë§Œì›)',
                    'ì‹œì¥ ì´í•´ë„',
                    'ê²½ìŸì‚¬ ê³ ë ¤',
                    'ë…¼ë¦¬ì  ê·¼ê±°'
                ]
            },
            
            {
                'id': 'SC-005',
                'name': 'Phase 4 (ë‹¨ìˆœ Fermi - í…œí”Œë¦¿ í™œìš© ê°€ëŠ¥)',
                'phase': 4,
                'complexity': 'complex',
                'prompt': '''ì„œìš¸ì˜ í”¼ì•„ë…¸ í•™ì› ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ë‹¤ìŒ ì ‘ê·¼ì„ ê³ ë ¤í•˜ì„¸ìš”:
1. Top-down: ì¸êµ¬ ê¸°ë°˜ (ì„œìš¸ ì¸êµ¬ 1000ë§Œ)
2. Bottom-up: í•™ìƒ ìˆ˜ ê¸°ë°˜ (ì´ˆì¤‘ê³ ìƒ 100ë§Œ)

ê° ëª¨í˜•ì„ ì‹¤í–‰í•˜ê³  í‰ê· ì„ êµ¬í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "models": [{"name": "ëª¨í˜•1", "result": ê°’}, {"name": "ëª¨í˜•2", "result": ê°’}], "confidence": 0.5-0.7, "reasoning": "ìš”ì•½"}''',
                'expected_answer': '2,000-3,500ê°œ',
                'evaluation_criteria': [
                    'í•©ë¦¬ì  ë²”ìœ„ (2000-3500)',
                    '2ê°œ ì´ìƒ ëª¨í˜• ì‚¬ìš©',
                    'ê° ëª¨í˜• ë…¼ë¦¬ ëª…í™•',
                    'ê²€ì¦ ë‹¨ê³„ í¬í•¨'
                ]
            },
            
            {
                'id': 'SC-006',
                'name': 'Phase 4 (ë³µì¡ Fermi - ì°½ì˜ì  ë¶„í•´)',
                'phase': 4,
                'complexity': 'very_complex',
                'prompt': '''í•œêµ­ ì„±ì¸ í”¼ì•„ë…¸ í•™ìŠµ ì‹œì¥ì˜ ì—°ê°„ ì´ ë§¤ì¶œì„ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤í•  ìš”ì†Œ:
- í•™ìŠµì ìˆ˜ (í•™ì› + ì˜¨ë¼ì¸ + ê°œì¸êµìŠµ)
- ê° ì±„ë„ë³„ ê°€ê²©
- êµì¬, ì•…ê¸° êµ¬ë§¤ ë“± ë¶€ê°€ ì§€ì¶œ

ì°½ì˜ì ìœ¼ë¡œ ëª¨í˜•ì„ ë§Œë“¤ê³ , ì—¬ëŸ¬ ì ‘ê·¼ì„ ì‹œë„í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "models": [...], "confidence": 0.4-0.6, "reasoning_detail": "ìƒì„¸ ê·¼ê±°"}''',
                'expected_answer': '1,000ì–µ-5,000ì–µì›',
                'evaluation_criteria': [
                    'ì°½ì˜ì  ëª¨í˜• ìƒì„±',
                    'ë‹¤ê°ë„ ì ‘ê·¼ (3ê°œ ì´ìƒ ëª¨í˜•)',
                    'ë³€ìˆ˜ ê°„ ê´€ê³„ ì´í•´',
                    'í•©ë¦¬ì  ê²°ê³¼'
                ]
            }
        ]
    
    def test_single_scenario(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ (.env ì„¤ì • ë¬´ì‹œ)
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
        print(f"ğŸ¤– ëª¨ë¸: {model}")
        print(f"{'='*80}\n")
        
        # í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        print("ğŸ“‹ í”„ë¡¬í”„íŠ¸:")
        print("-" * 80)
        print(scenario['prompt'])
        print("-" * 80)
        print()
        
        # API í˜¸ì¶œ
        print(f"â³ {model} í˜¸ì¶œ ì¤‘...\n")
        start_time = time.time()
        
        try:
            # ëª¨ë¸ë³„ ë¶„ê¸° (o1/o3/o4ëŠ” ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°)
            if model.startswith('o1') or model.startswith('o3') or model.startswith('o4'):
                # Thinking ëª¨ë¸ (system, temperature, response_format ë¯¸ì§€ì›)
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": scenario['prompt']}
                    ]
                )
            elif 'nano' in model or 'gpt-5' in model:
                # nano/gpt-5 ëª¨ë¸ (temperature ê¸°ë³¸ê°’ë§Œ ì§€ì›)
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": scenario['prompt']}
                    ],
                    response_format={"type": "json_object"}
                )
            else:
                # ì¼ë°˜ ëª¨ë¸ (ëª¨ë“  íŒŒë¼ë¯¸í„° ì§€ì›)
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•­ìƒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": scenario['prompt']}
                    ],
                    temperature=0.2,
                    response_format={"type": "json_object"}
                )
            
            elapsed = time.time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            
            try:
                parsed = json.loads(content)
            except json.JSONDecodeError:
                parsed = {'raw_response': content, 'parse_error': True}
            
            # ë¹„ìš© ê³„ì‚°
            usage = response.usage
            cost = self._calculate_cost(model, usage.prompt_tokens, usage.completion_tokens)
            
            # ì‘ë‹µ ì¶œë ¥
            print("âœ… ì‘ë‹µ ë°›ìŒ")
            print(f"   ë¹„ìš©: ${cost:.6f}")
            print(f"   ì‹œê°„: {elapsed:.2f}ì´ˆ")
            print(f"   í† í°: {usage.total_tokens} ({usage.prompt_tokens}â†’{usage.completion_tokens})")
            print()
            print("ğŸ“„ ì‘ë‹µ ë‚´ìš©:")
            print("-" * 80)
            print(json.dumps(parsed, ensure_ascii=False, indent=2))
            print("-" * 80)
            print()
            
            # ê¸°ëŒ€ ë‹µë³€ ì¶œë ¥
            print("ğŸ¯ ê¸°ëŒ€ ë‹µë³€:")
            print(f"   {scenario['expected_answer']}")
            print()
            
            # í‰ê°€ ê¸°ì¤€ ì¶œë ¥
            print("ğŸ“Š í‰ê°€ ê¸°ì¤€:")
            for idx, criterion in enumerate(scenario['evaluation_criteria'], 1):
                print(f"   {idx}. {criterion}")
            print()
            
            # ì‚¬ìš©ì í‰ê°€ ë°›ê¸° (auto_modeì—ì„œëŠ” ê±´ë„ˆë›°ê¸°)
            if hasattr(self, 'auto_mode') and self.auto_mode:
                quality_score = {'quality_score': None, 'auto_mode': True}
            else:
                quality_score = self._get_user_evaluation(scenario)
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'phase': scenario['phase'],
                'complexity': scenario['complexity'],
                'response': parsed,
                'expected': scenario['expected_answer'],
                'tokens': {
                    'input': usage.prompt_tokens,
                    'output': usage.completion_tokens,
                    'total': usage.total_tokens
                },
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'user_evaluation': quality_score,
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
            
            return result
        
        except Exception as e:
            elapsed = time.time() - start_time
            
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print()
            
            return {
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'error': str(e),
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': False,
                'user_evaluation': {'quality': 0, 'reason': 'API ì˜¤ë¥˜'}
            }
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        if model not in self.pricing:
            return 0.0
        
        rates = self.pricing[model]
        cost = (input_tokens / 1_000_000 * rates['input'] +
                output_tokens / 1_000_000 * rates['output'])
        return round(cost, 8)
    
    def _get_user_evaluation(self, scenario: Dict) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì í‰ê°€ ì…ë ¥
        """
        print("="*80)
        print("ğŸ‘¤ ì‚¬ìš©ì í‰ê°€")
        print("="*80)
        print()
        
        # í’ˆì§ˆ ì ìˆ˜ (0-100)
        while True:
            try:
                quality_input = input("í’ˆì§ˆ ì ìˆ˜ (0-100, Enter=ê±´ë„ˆë›°ê¸°): ").strip()
                
                if quality_input == '':
                    quality = None
                    break
                
                quality = int(quality_input)
                
                if 0 <= quality <= 100:
                    break
                else:
                    print("âš ï¸ 0-100 ì‚¬ì´ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”.")
            except ValueError:
                print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        
        # ì„¸ë¶€ í‰ê°€
        if quality is not None:
            print()
            print("ì„¸ë¶€ í‰ê°€ (ê° í•­ëª© y/n):")
            
            evaluations = {}
            for idx, criterion in enumerate(scenario['evaluation_criteria'], 1):
                while True:
                    answer = input(f"  {idx}. {criterion}? (y/n/Enter=skip): ").strip().lower()
                    if answer in ['y', 'n', '']:
                        evaluations[f'criterion_{idx}'] = answer if answer else 'skip'
                        break
            
            # ì½”ë©˜íŠ¸
            print()
            comment = input("ì½”ë©˜íŠ¸ (ì„ íƒ, Enter=ê±´ë„ˆë›°ê¸°): ").strip()
            
            return {
                'quality_score': quality,
                'evaluations': evaluations,
                'comment': comment if comment else None,
                'timestamp': datetime.now().isoformat()
            }
        else:
            return {
                'quality_score': None,
                'skipped': True,
                'timestamp': datetime.now().isoformat()
            }
    
    def run_interactive_benchmark(
        self,
        models: List[str] = None,
        scenarios: List[str] = None,
        output_file: str = None,
        auto_mode: bool = False
    ):
        """
        ì¸í„°ë™í‹°ë¸Œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        
        Args:
            models: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (None=ì „ì²´)
            scenarios: í…ŒìŠ¤íŠ¸í•  ì‹œë‚˜ë¦¬ì˜¤ ID (None=ì „ì²´)
            output_file: ê²°ê³¼ íŒŒì¼ëª…
            auto_mode: ìë™ ëª¨ë“œ (í™•ì¸ ê±´ë„ˆë›°ê¸°, í‰ê°€ ê±´ë„ˆë›°ê¸°)
        """
        print("="*80)
        print("ğŸ¯ OpenAI ëª¨ë¸ ì¸í„°ë™í‹°ë¸Œ ë²¤ì¹˜ë§ˆí¬")
        print("="*80)
        print()
        
        # API í‚¤ í™•ì¸
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… API í‚¤ í™•ì¸: {api_key[:20]}...\n")
        
        # í…ŒìŠ¤íŠ¸ ëŒ€ìƒ
        test_models = models if models else self.models_to_test
        all_scenarios = self.get_test_scenarios()
        
        if scenarios:
            test_scenarios = [s for s in all_scenarios if s['id'] in scenarios]
        else:
            test_scenarios = all_scenarios
        
        print(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸: {len(test_models)}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(test_scenarios)}ê°œ")
        print(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {len(test_models) * len(test_scenarios)}ê°œ")
        
        if auto_mode:
            print("âš¡ ìë™ ëª¨ë“œ: í‰ê°€ ì—†ì´ ì‘ë‹µë§Œ ìˆ˜ì§‘")
        
        print()
        
        # í™•ì¸ (auto_modeì—ì„œëŠ” ê±´ë„ˆë›°ê¸°)
        if not auto_mode:
            proceed = input("ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if proceed != 'y':
                print("ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
        
        self.auto_mode = auto_mode
        
        # í…ŒìŠ¤íŠ¸ ì§„í–‰
        total_tests = len(test_models) * len(test_scenarios)
        test_count = 0
        
        for model in test_models:
            print(f"\n{'#'*80}")
            print(f"# ëª¨ë¸: {model}")
            print(f"# ê°€ê²©: ${self.pricing.get(model, {}).get('input', 0)}/1M ì…ë ¥, ${self.pricing.get(model, {}).get('output', 0)}/1M ì¶œë ¥")
            print(f"{'#'*80}")
            
            for scenario in test_scenarios:
                test_count += 1
                print(f"\nì§„í–‰: {test_count}/{total_tests}")
                
                result = self.test_single_scenario(model, scenario)
                self.results.append(result)
                
                # ì¤‘ê°„ ì €ì¥
                if test_count % 5 == 0:
                    self._save_intermediate()
                
                # ê³„ì† ì§„í–‰ í™•ì¸ (auto_modeì—ì„œëŠ” ê±´ë„ˆë›°ê¸°)
                if test_count < total_tests and not self.auto_mode:
                    print()
                    cont = input("ê³„ì† ì§„í–‰? (y/Enter=yes, n=ì¤‘ë‹¨, s=ì €ì¥ í›„ ì¤‘ë‹¨): ").strip().lower()
                    
                    if cont == 'n':
                        print("\nâš ï¸ ë²¤ì¹˜ë§ˆí¬ ì¤‘ë‹¨")
                        break
                    elif cont == 's':
                        print("\nâš ï¸ ì €ì¥ í›„ ì¤‘ë‹¨")
                        self.save_final_results(output_file)
                        return
                else:
                    cont = 'y'  # auto_modeì—ì„œëŠ” í•­ìƒ ê³„ì†
            
            if test_count < total_tests and cont == 'n':
                break
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        self.save_final_results(output_file)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()
    
    def _save_intermediate(self):
        """ì¤‘ê°„ ì €ì¥"""
        filename = f"benchmark_intermediate_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ì¤‘ê°„ ì €ì¥: {filename}")
    
    def save_final_results(self, output_file: Optional[str] = None):
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        if output_file is None:
            output_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'total_tests': len(self.results),
                    'successful_tests': sum(1 for r in self.results if r['success'])
                },
                'results': self.results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ìµœì¢… ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"   ì´ {len(self.results)}ê°œ í…ŒìŠ¤íŠ¸")
    
    def generate_report(self):
        """ë¦¬í¬íŠ¸ ìƒì„±"""
        print(f"\n{'='*80}")
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸")
        print(f"{'='*80}\n")
        
        # ê¸°ë³¸ í†µê³„
        success_results = [r for r in self.results if r['success']]
        evaluated_results = [r for r in success_results if r.get('user_evaluation', {}).get('quality_score') is not None]
        
        print(f"ì´ í…ŒìŠ¤íŠ¸: {len(self.results)}ê°œ")
        print(f"ì„±ê³µ: {len(success_results)}ê°œ")
        print(f"ì‚¬ìš©ì í‰ê°€: {len(evaluated_results)}ê°œ")
        print()
        
        # Auto ëª¨ë“œì¼ ë•Œ ê¸°ë³¸ ë¦¬í¬íŠ¸
        if not evaluated_results:
            print("âš ï¸ í‰ê°€ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (ìë™ ëª¨ë“œ)")
            print()
            print("ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ!")
            print(f"ê²°ê³¼ íŒŒì¼ì—ì„œ ê° ëª¨ë¸ì˜ ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.")
            print()
            
            # ë¹„ìš©/ì‹œê°„ í†µê³„ë§Œ ì¶œë ¥
            print(f"{'='*80}")
            print("ëª¨ë¸ë³„ ë¹„ìš©/ì‹œê°„ í†µê³„")
            print(f"{'='*80}\n")
            
            model_stats = {}
            for result in success_results:
                model = result['model']
                if model not in model_stats:
                    model_stats[model] = {'costs': [], 'times': []}
                
                model_stats[model]['costs'].append(result['cost'])
                model_stats[model]['times'].append(result['elapsed_seconds'])
            
            print(f"{'ëª¨ë¸':20s} | {'í‰ê·  ë¹„ìš©':12s} | {'í‰ê·  ì‹œê°„':10s} | í…ŒìŠ¤íŠ¸ ìˆ˜")
            print("-" * 70)
            
            for model, stats in model_stats.items():
                avg_cost = sum(stats['costs']) / len(stats['costs'])
                avg_time = sum(stats['times']) / len(stats['times'])
                
                print(f"{model:20s} | ${avg_cost:.6f}   | {avg_time:6.2f}ì´ˆ   | {len(stats['costs'])}ê°œ")
            
            return
        
        # ëª¨ë¸ë³„ í†µê³„
        print(f"{'='*80}")
        print("ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
        print(f"{'='*80}\n")
        
        model_stats = {}
        for result in evaluated_results:
            model = result['model']
            
            if model not in model_stats:
                model_stats[model] = {
                    'quality_scores': [],
                    'costs': [],
                    'times': [],
                    'phases': {}
                }
            
            quality = result['user_evaluation']['quality_score']
            model_stats[model]['quality_scores'].append(quality)
            model_stats[model]['costs'].append(result['cost'])
            model_stats[model]['times'].append(result['elapsed_seconds'])
            
            phase = result.get('phase', 0)
            if phase not in model_stats[model]['phases']:
                model_stats[model]['phases'][phase] = []
            model_stats[model]['phases'][phase].append(quality)
        
        # ëª¨ë¸ë³„ í‰ê· 
        print(f"{'ëª¨ë¸':20s} | {'í‰ê·  í’ˆì§ˆ':10s} | {'í‰ê·  ë¹„ìš©':12s} | {'í‰ê·  ì‹œê°„':10s} | ê°€ì„±ë¹„")
        print("-" * 80)
        
        model_summaries = []
        for model, stats in model_stats.items():
            avg_quality = sum(stats['quality_scores']) / len(stats['quality_scores'])
            avg_cost = sum(stats['costs']) / len(stats['costs'])
            avg_time = sum(stats['times']) / len(stats['times'])
            
            # ê°€ì„±ë¹„ = í’ˆì§ˆ / ë¹„ìš©
            value_score = avg_quality / (avg_cost * 1000000) if avg_cost > 0 else 0
            
            model_summaries.append({
                'model': model,
                'avg_quality': avg_quality,
                'avg_cost': avg_cost,
                'avg_time': avg_time,
                'value_score': value_score,
                'test_count': len(stats['quality_scores'])
            })
            
            print(f"{model:20s} | {avg_quality:8.1f}ì   | ${avg_cost:.6f}   | {avg_time:6.2f}ì´ˆ   | {value_score:8.0f}")
        
        # ê°€ì„±ë¹„ TOP 3
        print(f"\n{'='*80}")
        print("ğŸ† ê°€ì„±ë¹„ TOP 3")
        print(f"{'='*80}\n")
        
        model_summaries.sort(key=lambda m: m['value_score'], reverse=True)
        
        for idx, summary in enumerate(model_summaries[:3], 1):
            print(f"{idx}ìœ„: {summary['model']}")
            print(f"   í’ˆì§ˆ: {summary['avg_quality']:.1f}ì ")
            print(f"   ë¹„ìš©: ${summary['avg_cost']:.6f}/ì‘ì—…")
            print(f"   ì‹œê°„: {summary['avg_time']:.2f}ì´ˆ")
            print(f"   ê°€ì„±ë¹„: {summary['value_score']:.0f}")
            print()
        
        # Phaseë³„ ë¶„ì„
        print(f"{'='*80}")
        print("Phaseë³„ ì„±ëŠ¥ ë¶„ì„")
        print(f"{'='*80}\n")
        
        for model, stats in model_stats.items():
            if len(stats['phases']) > 1:
                print(f"{model}:")
                for phase, scores in sorted(stats['phases'].items()):
                    avg_score = sum(scores) / len(scores)
                    print(f"   Phase {phase}: {avg_score:.1f}ì  ({len(scores)}ê°œ í…ŒìŠ¤íŠ¸)")
                print()
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        if model not in self.pricing:
            return 0.0
        
        rates = self.pricing[model]
        cost = (input_tokens / 1_000_000 * rates['input'] +
                output_tokens / 1_000_000 * rates['output'])
        return cost


def quick_test_mode(auto: bool = False):
    """
    ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (nano ëª¨ë¸ë§Œ, ê°„ë‹¨í•œ ì‹œë‚˜ë¦¬ì˜¤ë§Œ)
    
    Args:
        auto: ìë™ ëª¨ë“œ (í‰ê°€ ì—†ì´ ì‘ë‹µë§Œ ìˆ˜ì§‘)
    """
    print("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (nano ëª¨ë¸ + Phase 0-3)")
    if auto:
        print("âš¡ ìë™ ëª¨ë“œ: í‰ê°€ ê±´ë„ˆë›°ê¸°, ì‘ë‹µë§Œ ìˆ˜ì§‘")
    print()
    
    benchmark = InteractiveBenchmark()
    
    # nano ëª¨ë¸ë§Œ
    test_models = ['gpt-5-nano', 'gpt-4.1-nano', 'gpt-4o-mini']
    
    # Phase 0-3ë§Œ
    test_scenarios = ['SC-001', 'SC-002', 'SC-003']
    
    benchmark.run_interactive_benchmark(
        models=test_models,
        scenarios=test_scenarios,
        output_file='benchmark_nano_quick.json',
        auto_mode=auto
    )


def phase_by_phase_mode():
    """
    Phaseë³„ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    """
    print("ğŸ“Š Phaseë³„ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print()
    print("í…ŒìŠ¤íŠ¸í•  Phaseë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("  0: Phase 0 (Literal)")
    print("  1: Phase 2 (ê³„ì‚°)")
    print("  2: Phase 3 (í…œí”Œë¦¿ ìˆìŒ)")
    print("  3: Phase 3 (í…œí”Œë¦¿ ì—†ìŒ)")
    print("  4: Phase 4 (ë‹¨ìˆœ)")
    print("  5: Phase 4 (ë³µì¡)")
    print()
    
    phase_choice = input("ì„ íƒ (0-5): ").strip()
    
    scenario_map = {
        '0': ['SC-001'],
        '1': ['SC-002'],
        '2': ['SC-003'],
        '3': ['SC-004'],
        '4': ['SC-005'],
        '5': ['SC-006']
    }
    
    if phase_choice not in scenario_map:
        print("âš ï¸ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    # ëª¨ë¸ ì„ íƒ
    print()
    print("í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("  1: nano ëª¨ë¸ (gpt-5-nano, gpt-4.1-nano, gpt-4o-mini)")
    print("  2: mini ëª¨ë¸ (gpt-5-mini, gpt-4.1-mini, gpt-4o-mini)")
    print("  3: standard ëª¨ë¸ (gpt-4o, gpt-4.1, gpt-5.1)")
    print("  4: thinking ëª¨ë¸ (o1-mini, o3-mini, o3)")
    print("  5: ì „ì²´")
    print()
    
    model_choice = input("ì„ íƒ (1-5): ").strip()
    
    benchmark = InteractiveBenchmark()
    
    model_groups = {
        '1': benchmark.models['nano'],
        '2': benchmark.models['mini'],
        '3': benchmark.models['standard'],
        '4': benchmark.models['thinking'],
        '5': benchmark.models_to_test
    }
    
    if model_choice not in model_groups:
        print("âš ï¸ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    test_models = model_groups[model_choice]
    test_scenarios = scenario_map[phase_choice]
    
    print()
    print(f"âœ… í…ŒìŠ¤íŠ¸ êµ¬ì„±:")
    print(f"   ëª¨ë¸: {len(test_models)}ê°œ")
    print(f"   ì‹œë‚˜ë¦¬ì˜¤: {len(test_scenarios)}ê°œ")
    print(f"   ì´: {len(test_models) * len(test_scenarios)}ê°œ í…ŒìŠ¤íŠ¸")
    print()
    
    benchmark.run_interactive_benchmark(
        models=test_models,
        scenarios=test_scenarios,
        output_file=f'benchmark_phase{phase_choice}.json'
    )


def main():
    """
    ë©”ì¸ ë©”ë‰´
    """
    import sys
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì í™•ì¸
    if len(sys.argv) > 1:
        mode = sys.argv[1]
        
        if mode == 'quick' or mode == '1':
            print("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ìë™ ì‹¤í–‰)")
            print()
            quick_test_mode(auto=True)
            return
        elif mode == 'quick-interactive':
            print("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¸í„°ë™í‹°ë¸Œ)")
            print()
            quick_test_mode(auto=False)
            return
        elif mode == 'phase':
            phase_by_phase_mode()
            return
        elif mode == 'full':
            benchmark = InteractiveBenchmark()
            benchmark.run_interactive_benchmark()
            return
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œ: {mode}")
            print("ì‚¬ìš©ë²•: python interactive_model_benchmark.py [quick|phase|full]")
            return
    
    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print("="*80)
    print("OpenAI ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬")
    print("="*80)
    print()
    print("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    print("  1: ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (nano ëª¨ë¸ + Phase 0-3, 9ê°œ í…ŒìŠ¤íŠ¸)")
    print("  2: Phaseë³„ í…ŒìŠ¤íŠ¸ (ì„ íƒì )")
    print("  3: ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (ëª¨ë“  ëª¨ë¸ + ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤)")
    print()
    
    choice = input("ì„ íƒ (1-3): ").strip()
    
    if choice == '1':
        quick_test_mode()
    elif choice == '2':
        phase_by_phase_mode()
    elif choice == '3':
        benchmark = InteractiveBenchmark()
        benchmark.run_interactive_benchmark()
    else:
        print("âš ï¸ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

