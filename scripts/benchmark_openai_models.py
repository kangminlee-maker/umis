#!/usr/bin/env python3
"""
OpenAI ëª¨ë¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (UMIS ì‘ì—… ê¸°ì¤€)
ê° ëª¨ë¸ì„ ì‹¤ì œ UMIS ì‘ì—…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ë¹„ìš©-ì„±ëŠ¥ ë¹„êµ
"""

import os
import json
import time
from typing import Dict, List, Any
from datetime import datetime
from openai import OpenAI

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


class OpenAIBenchmark:
    """
    OpenAI ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬
    """
    
    def __init__(self):
        self.client = OpenAI()
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ (Standard Tier)
        self.models = {
            'nano': [
                'gpt-5-nano',
                'gpt-4.1-nano'
            ],
            'mini': [
                'gpt-4o-mini',
                'gpt-5-mini',
                'gpt-4.1-mini'
            ],
            'standard': [
                'gpt-4o',
                'gpt-4.1',
                'gpt-5.1'
            ],
            'thinking': [
                'o1-mini',
                'o3-mini',
                'o4-mini',
                'o3'
            ]
        }
        
        # ê°€ê²© ì •ë³´ ($/1M í† í°)
        self.pricing = {
            'gpt-5-nano': {'input': 0.05, 'output': 0.40},
            'gpt-4.1-nano': {'input': 0.10, 'output': 0.40},
            'gpt-4o-mini': {'input': 0.15, 'output': 0.60},
            'gpt-5-mini': {'input': 0.25, 'output': 2.00},
            'gpt-4.1-mini': {'input': 0.40, 'output': 1.60},
            'gpt-4o': {'input': 2.50, 'output': 10.00},
            'gpt-4.1': {'input': 2.00, 'output': 8.00},
            'gpt-5.1': {'input': 1.25, 'output': 10.00},
            'o1-mini': {'input': 1.10, 'output': 4.40},
            'o3-mini': {'input': 1.10, 'output': 4.40},
            'o4-mini': {'input': 1.10, 'output': 4.40},
            'o3': {'input': 2.00, 'output': 8.00}
        }
        
        # ê²°ê³¼ ì €ì¥
        self.results = []
    
    def run_full_benchmark(self, output_file: str = 'benchmark_results.json'):
        """
        ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        """
        print("ğŸš€ OpenAI ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        print(f"   í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {len(self.get_test_scenarios())}ê°œ")
        print(f"   í…ŒìŠ¤íŠ¸ ëª¨ë¸: {sum(len(models) for models in self.models.values())}ê°œ")
        print()
        
        scenarios = self.get_test_scenarios()
        
        for scenario_idx, scenario in enumerate(scenarios, 1):
            print(f"\n{'='*80}")
            print(f"ì‹œë‚˜ë¦¬ì˜¤ {scenario_idx}/{len(scenarios)}: {scenario['name']}")
            print(f"{'='*80}")
            
            # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
            for category, models in self.models.items():
                for model in models:
                    try:
                        result = self.test_model(model, scenario)
                        self.results.append(result)
                        
                        # ê²°ê³¼ ì¶œë ¥
                        self._print_result(result)
                        
                        # Rate limit ë°©ì§€
                        time.sleep(1)
                    
                    except Exception as e:
                        print(f"   âŒ {model}: ì˜¤ë¥˜ - {str(e)}")
                        self.results.append({
                            'model': model,
                            'scenario': scenario['name'],
                            'error': str(e),
                            'timestamp': datetime.now().isoformat()
                        })
        
        # ê²°ê³¼ ì €ì¥
        self.save_results(output_file)
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()
    
    def get_test_scenarios(self) -> List[Dict]:
        """
        UMIS í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        """
        return [
            {
                'id': 'phase0',
                'name': 'Phase 0 (Literal)',
                'category': 'simple',
                'prompt': '''ë‹¤ìŒ ë°ì´í„°ì—ì„œ "í•œêµ­ B2B SaaS ARPU"ë¥¼ ì°¾ì•„ ë°˜í™˜í•˜ì„¸ìš”:

ë°ì´í„°:
- ë¯¸êµ­ B2C SaaS ARPU: $50
- í•œêµ­ B2B SaaS ARPU: 200,000ì›
- í•œêµ­ B2C SaaS ARPU: 70,000ì›

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 1.0}''',
                'expected': {
                    'value': 200000,
                    'unit': 'ì›',
                    'confidence': 1.0
                }
            },
            
            {
                'id': 'phase2_calculation',
                'name': 'Phase 2 (ê³„ì‚°)',
                'category': 'simple',
                'prompt': '''ë‹¤ìŒ ê³µì‹ì„ ê³„ì‚°í•˜ì„¸ìš”:

LTV = ARPU / Churn_Rate

ì£¼ì–´ì§„ ê°’:
- ARPU: 80,000ì›
- Churn_Rate: 0.05

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "formula": "ê³µì‹", "confidence": 1.0}''',
                'expected': {
                    'value': 1600000,
                    'confidence': 1.0
                }
            },
            
            {
                'id': 'phase3_template',
                'name': 'Phase 3 (í…œí”Œë¦¿ ìˆìŒ)',
                'category': 'medium',
                'prompt': '''B2B SaaS í•œêµ­ ì‹œì¥ ARPUë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ì°¸ê³  ì˜ˆì‹œ:
- ê¸€ë¡œë²Œ B2B SaaS ARPU: $100
- í•œêµ­ GDP per capita: ê¸€ë¡œë²Œ ëŒ€ë¹„ 60%
- B2B vs B2C ë°°ìˆ˜: 3ë°°

ëª¨í˜•:
1. ê¸€ë¡œë²Œ ê¸°ì¤€ ì¡°ì •: $100 Ã— 0.6 Ã— 3 = $180
2. í™˜ìœ¨ ì ìš©: $180 Ã— 1,300 = 234,000ì›
3. ë°˜ì˜¬ë¦¼: 200,000ì›

ì´ì œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "reasoning": "í•œ ë¬¸ì¥"}''',
                'expected': {
                    'value_range': [150000, 250000],
                    'confidence_min': 0.65
                }
            },
            
            {
                'id': 'phase3_no_template',
                'name': 'Phase 3 (í…œí”Œë¦¿ ì—†ìŒ)',
                'category': 'complex',
                'prompt': '''í•œêµ­ ì˜¨ë¼ì¸ êµìœ¡ í”Œë«í¼ì˜ ì›” êµ¬ë…ë£Œë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤ ì‚¬í•­:
- íƒ€ê²Ÿ: ì„±ì¸ ì·¨ë¯¸ êµìœ¡
- ê²½ìŸì‚¬: í´ë˜ìŠ¤101, íƒˆì‰ ë“±
- ì‚¬ìš©ì: ì§ì¥ì¸, 30-40ëŒ€

ë‹¨ê³„ë³„ë¡œ ìƒê°í•˜ê³  JSONìœ¼ë¡œ ë‹µë³€:
{"value": ìˆ«ì, "unit": "ì›", "confidence": 0.0-1.0, "reasoning": "ìš”ì•½"}''',
                'expected': {
                    'value_range': [10000, 50000],
                    'confidence_min': 0.60
                }
            },
            
            {
                'id': 'phase4_simple',
                'name': 'Phase 4 (ë‹¨ìˆœ Fermi)',
                'category': 'complex',
                'prompt': '''ì„œìš¸ì˜ í”¼ì•„ë…¸ í•™ì› ìˆ˜ë¥¼ ì¶”ì •í•˜ì„¸ìš”.

ë‹¤ìŒ ë‹¨ê³„ë¡œ ìƒê°í•˜ì„¸ìš”:
1. ì–´ë–¤ ë³€ìˆ˜ê°€ í•„ìš”í•œê°€?
2. ê° ë³€ìˆ˜ë¥¼ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?
3. ì–´ë–¤ ëª¨í˜•ì„ ì‚¬ìš©í• ê¹Œ?
4. ê²°ê³¼ê°€ í•©ë¦¬ì ì¸ê°€?

JSON í˜•ì‹:
{"value": ìˆ«ì, "models": [ëª¨í˜•1ê²°ê³¼, ëª¨í˜•2ê²°ê³¼], "confidence": 0.0-1.0, "reasoning": "ìš”ì•½"}''',
                'expected': {
                    'value_range': [1500, 4000],
                    'confidence_min': 0.60
                }
            },
            
            {
                'id': 'phase4_complex',
                'name': 'Phase 4 (ë³µì¡ Fermi)',
                'category': 'very_complex',
                'prompt': '''í•œêµ­ ì„±ì¸ í”¼ì•„ë…¸ í•™ìŠµìì˜ ì—°ê°„ ì´ ì§€ì¶œì•¡ì„ ì¶”ì •í•˜ì„¸ìš”.

ê³ ë ¤í•  ìš”ì†Œ:
- í•™ìŠµì ìˆ˜
- í•™ì›ë¹„
- êµì¬ë¹„
- ì•…ê¸° êµ¬ë§¤/ë Œíƒˆ
- ê¸°íƒ€ ë¹„ìš©

ì°½ì˜ì ìœ¼ë¡œ ëª¨í˜•ì„ ìƒì„±í•˜ê³ , ì—¬ëŸ¬ ì ‘ê·¼ì„ ì‹œë„í•œ í›„ ë‹µë³€í•˜ì„¸ìš”.

JSON í˜•ì‹:
{"value": ìˆ«ì, "unit": "ì›", "models": [...], "confidence": 0.0-1.0, "reasoning_detail": {...}}''',
                'expected': {
                    'value_range': [50000000000, 500000000000],  # 500ì–µ-5000ì–µ
                    'confidence_min': 0.50
                }
            }
        ]
    
    def test_model(self, model: str, scenario: Dict) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        """
        start_time = time.time()
        
        try:
            # ëª¨ë¸ íƒ€ì… êµ¬ë¶„
            is_o_series = model.startswith(('o1', 'o3', 'o4'))
            is_gpt5 = model.startswith('gpt-5')
            is_reasoning = is_o_series or is_gpt5
            
            # API í˜¸ì¶œ (ëª¨ë¸ë³„ ë¶„ê¸°)
            if is_reasoning:
                # reasoning ëª¨ë¸ (system ë©”ì‹œì§€ ë¯¸ì§€ì›, reasoning_effort ì‚¬ìš©)
                api_params = {
                    "model": model,
                    "messages": [{"role": "user", "content": scenario['prompt']}]
                }
                if is_o_series:
                    api_params["reasoning_effort"] = "medium"
                else:  # gpt-5
                    api_params["reasoning_effort"] = "low"
                
                response = self.client.chat.completions.create(**api_params)
            else:
                # ì¼ë°˜ ëª¨ë¸
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."},
                        {"role": "user", "content": scenario['prompt']}
                    ],
                    temperature=0.2,
                    response_format={"type": "json_object"}
                )
            
            elapsed = time.time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ
            try:
                if '```json' in content:
                    json_start = content.find('```json') + 7
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                elif '```' in content:
                    json_start = content.find('```') + 3
                    json_end = content.find('```', json_start)
                    content = content[json_start:json_end].strip()
                parsed = json.loads(content)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
                parsed = {'raw_response': content, 'parse_error': True}
            
            # ë¹„ìš© ê³„ì‚°
            usage = response.usage
            cost = self._calculate_cost(
                model,
                usage.prompt_tokens,
                usage.completion_tokens
            )
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'category': scenario['category'],
                'response': parsed,
                'expected': scenario.get('expected'),
                'tokens': {
                    'input': usage.prompt_tokens,
                    'output': usage.completion_tokens,
                    'total': usage.total_tokens
                },
                'cost': cost,
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': True
            }
        
        except Exception as e:
            elapsed = time.time() - start_time
            return {
                'model': model,
                'scenario_id': scenario['id'],
                'scenario_name': scenario['name'],
                'error': str(e),
                'elapsed_seconds': round(elapsed, 2),
                'timestamp': datetime.now().isoformat(),
                'success': False
            }
    
    def _calculate_cost(self, model: str, input_tokens: int, output_tokens: int) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        if model not in self.pricing:
            return 0.0
        
        rates = self.pricing[model]
        cost = (input_tokens / 1_000_000 * rates['input'] +
                output_tokens / 1_000_000 * rates['output'])
        return round(cost, 6)
    
    def _print_result(self, result: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        if not result['success']:
            print(f"   âŒ {result['model']}: {result['error']}")
            return
        
        response = result['response']
        
        print(f"\n   âœ… {result['model']}")
        print(f"      ë¹„ìš©: ${result['cost']:.6f}")
        print(f"      ì‹œê°„: {result['elapsed_seconds']}ì´ˆ")
        print(f"      í† í°: {result['tokens']['total']} ({result['tokens']['input']}â†’{result['tokens']['output']})")
        
        if 'value' in response:
            print(f"      ë‹µë³€: {response.get('value')} {response.get('unit', '')}")
            print(f"      ì‹ ë¢°ë„: {response.get('confidence', 'N/A')}")
        
        if 'reasoning' in response:
            print(f"      ê·¼ê±°: {response['reasoning'][:100]}...")
    
    def save_results(self, output_file: str):
        """ê²°ê³¼ ì €ì¥"""
        output_path = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_path}")
        print(f"   ì´ {len(self.results)}ê°œ í…ŒìŠ¤íŠ¸")
    
    def generate_report(self):
        """
        ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        """
        print(f"\n{'='*80}")
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸")
        print(f"{'='*80}\n")
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„
        success_count = sum(1 for r in self.results if r['success'])
        total_count = len(self.results)
        
        print(f"ì´ í…ŒìŠ¤íŠ¸: {total_count}ê°œ")
        print(f"ì„±ê³µ: {success_count}ê°œ ({success_count/total_count*100:.1f}%)")
        print(f"ì‹¤íŒ¨: {total_count - success_count}ê°œ")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ í†µê³„
        print(f"\n{'='*80}")
        print("ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥")
        print(f"{'='*80}\n")
        
        scenarios = {}
        for result in self.results:
            if not result['success']:
                continue
            
            scenario = result['scenario_name']
            if scenario not in scenarios:
                scenarios[scenario] = []
            scenarios[scenario].append(result)
        
        for scenario_name, results in scenarios.items():
            print(f"\nğŸ“Œ {scenario_name}")
            print(f"   í…ŒìŠ¤íŠ¸ ëª¨ë¸: {len(results)}ê°œ\n")
            
            # ë¹„ìš© ìˆœ ì •ë ¬
            results_sorted = sorted(results, key=lambda r: r['cost'])
            
            for idx, result in enumerate(results_sorted[:5], 1):  # Top 5ë§Œ
                model = result['model']
                cost = result['cost']
                elapsed = result['elapsed_seconds']
                
                # ì‘ë‹µ í’ˆì§ˆ (ê°„ë‹¨ í‰ê°€)
                response = result.get('response', {})
                has_value = 'value' in response
                has_confidence = 'confidence' in response
                
                quality_score = "âœ…" if (has_value and has_confidence) else "âš ï¸"
                
                print(f"   {idx}ìœ„: {model:20s} | ${cost:.6f} | {elapsed:4.1f}ì´ˆ | {quality_score}")
        
        # ëª¨ë¸ë³„ í‰ê·  í†µê³„
        print(f"\n{'='*80}")
        print("ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥")
        print(f"{'='*80}\n")
        
        model_stats = {}
        for result in self.results:
            if not result['success']:
                continue
            
            model = result['model']
            if model not in model_stats:
                model_stats[model] = {
                    'costs': [],
                    'times': [],
                    'total_tokens': []
                }
            
            model_stats[model]['costs'].append(result['cost'])
            model_stats[model]['times'].append(result['elapsed_seconds'])
            model_stats[model]['total_tokens'].append(result['tokens']['total'])
        
        # í‰ê·  ê³„ì‚° ë° ì •ë ¬
        model_averages = []
        for model, stats in model_stats.items():
            if not stats['costs']:
                continue
            
            avg_cost = sum(stats['costs']) / len(stats['costs'])
            avg_time = sum(stats['times']) / len(stats['times'])
            avg_tokens = sum(stats['total_tokens']) / len(stats['total_tokens'])
            
            model_averages.append({
                'model': model,
                'avg_cost': avg_cost,
                'avg_time': avg_time,
                'avg_tokens': avg_tokens,
                'test_count': len(stats['costs'])
            })
        
        # ë¹„ìš© ìˆœ ì •ë ¬
        model_averages.sort(key=lambda m: m['avg_cost'])
        
        print(f"{'ëª¨ë¸':20s} | {'í‰ê·  ë¹„ìš©':12s} | {'í‰ê·  ì‹œê°„':10s} | {'í‰ê·  í† í°':10s} | í…ŒìŠ¤íŠ¸ ìˆ˜")
        print("-" * 80)
        
        for avg in model_averages:
            print(f"{avg['model']:20s} | ${avg['avg_cost']:.6f}   | {avg['avg_time']:6.2f}ì´ˆ   | {avg['avg_tokens']:8.0f}   | {avg['test_count']}ê°œ")
        
        # ê°€ì„±ë¹„ TOP 3
        print(f"\n{'='*80}")
        print("ğŸ† ê°€ì„±ë¹„ TOP 3 (ë¹„ìš©/ì‹œê°„ ê¸°ì¤€)")
        print(f"{'='*80}\n")
        
        # ë¹„ìš© ê¸°ì¤€
        print("ğŸ’° ìµœì € ë¹„ìš©:")
        for idx, avg in enumerate(model_averages[:3], 1):
            print(f"   {idx}ìœ„: {avg['model']:20s} ${avg['avg_cost']:.6f}/ì‘ì—…")
        
        # ì†ë„ ê¸°ì¤€
        print("\nâš¡ ìµœê³  ì†ë„:")
        model_averages_speed = sorted(model_averages, key=lambda m: m['avg_time'])
        for idx, avg in enumerate(model_averages_speed[:3], 1):
            print(f"   {idx}ìœ„: {avg['model']:20s} {avg['avg_time']:.2f}ì´ˆ/ì‘ì—…")


def main():
    """
    ë©”ì¸ ì‹¤í–‰
    """
    print("=" * 80)
    print("OpenAI ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ (UMIS ì‘ì—… ê¸°ì¤€)")
    print("=" * 80)
    print()
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print(f"âœ… API í‚¤ í™•ì¸ë¨: {api_key[:20]}...")
    print()
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = OpenAIBenchmark()
    
    try:
        benchmark.run_full_benchmark()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"   í˜„ì¬ê¹Œì§€ {len(benchmark.results)}ê°œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        if benchmark.results:
            save = input("\nê²°ê³¼ë¥¼ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            if save.lower() == 'y':
                benchmark.save_results('benchmark_results_partial.json')
    
    print("\nğŸ‰ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!")


if __name__ == "__main__":
    main()




