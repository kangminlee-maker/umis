#!/usr/bin/env python3
"""
Comprehensive ë²¤ì¹˜ë§ˆí¬ API ì—°ê²° í…ŒìŠ¤íŠ¸
"""

import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark

def test_single_scenario():
    """
    ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ë¡œ ê°œì„  ì‚¬í•­ í™•ì¸
    """
    print("=" * 80)
    print("Comprehensive ë²¤ì¹˜ë§ˆí¬ - API ì—°ê²° ê°œì„  í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    print()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    # ê°€ì¥ ì €ë ´í•œ ëª¨ë¸ë“¤ë§Œ í…ŒìŠ¤íŠ¸
    benchmark.models = {
        'openai_nano': ['gpt-4.1-nano'],
        'claude_standard': ['claude-haiku-3.5']
    }
    
    print("âœ… ì¬ì‹œë„ ë¡œì§ í…ŒìŠ¤íŠ¸")
    print("   - OpenAI: gpt-4.1-nano")
    print("   - Claude: claude-haiku-3.5")
    print("   - Phase 0 ì‹œë‚˜ë¦¬ì˜¤ë§Œ ì‹¤í–‰")
    print("   - Exponential backoff í™œì„±í™”")
    print()
    
    try:
        scenarios = benchmark.get_test_scenarios()[:1]  # Phase 0ë§Œ
        
        for scenario in scenarios:
            print(f"ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            print()
            
            # OpenAI í…ŒìŠ¤íŠ¸
            print("ğŸ“¦ OpenAI nano")
            for model in benchmark.models['openai_nano']:
                try:
                    result = benchmark.test_openai_model(model, scenario)
                    
                    if result['success']:
                        print(f"   âœ… {model}")
                        print(f"      ë¹„ìš©: ${result['cost']:.6f} | ì‹œê°„: {result['elapsed_seconds']}ì´ˆ")
                        print(f"      í’ˆì§ˆ: {result['quality_score']['total_score']}/100")
                    else:
                        print(f"   âŒ {model}: {result.get('error')}")
                except Exception as e:
                    print(f"   âŒ {model}: ì˜ˆì™¸ - {str(e)}")
            
            # Claude í…ŒìŠ¤íŠ¸
            print("\nğŸ“¦ Claude standard")
            for model in benchmark.models['claude_standard']:
                try:
                    result = benchmark.test_claude_model(model, scenario)
                    
                    if result['success']:
                        print(f"   âœ… {model}")
                        print(f"      ë¹„ìš©: ${result['cost']:.6f} | ì‹œê°„: {result['elapsed_seconds']}ì´ˆ")
                        print(f"      í’ˆì§ˆ: {result['quality_score']['total_score']}/100")
                    else:
                        print(f"   âŒ {model}: {result.get('error')}")
                except Exception as e:
                    print(f"   âŒ {model}: ì˜ˆì™¸ - {str(e)}")
        
        print()
        print("=" * 80)
        print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì¬ì‹œë„ ë¡œì§ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        print("=" * 80)
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_single_scenario()

