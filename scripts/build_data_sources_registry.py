"""
Data Sources Registry êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

Validatorê°€ í™•ì • ë°ì´í„° ê²€ìƒ‰ì— ì‚¬ìš©í•  data_sources_registry êµ¬ì¶•
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import yaml
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

from umis_rag.core.config import settings
from umis_rag.utils.logger import logger


def load_yaml_data(yaml_path: Path) -> dict:
    """YAML íŒŒì¼ ë¡œë“œ"""
    with open(yaml_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def create_documents(data: dict) -> list[Document]:
    """
    YAML ë°ì´í„°ë¥¼ Chroma Documentë¡œ ë³€í™˜
    
    ê° ë°ì´í„° ì†ŒìŠ¤ë¥¼:
    - content: ì„¤ëª… í…ìŠ¤íŠ¸ (ê²€ìƒ‰ìš©)
    - metadata: ê°’, ì¶œì²˜, ì •ì˜ ë“± (ì¶”ì¶œìš©)
    """
    documents = []
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
    categories = [
        'official_statistics',
        'industry_benchmarks',
        'market_data',
        'constants'
    ]
    
    for category in categories:
        if category not in data:
            continue
        
        category_data = data[category]
        
        for key, source_info in category_data.items():
            # Content ìƒì„± (ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸)
            content_parts = []
            
            # ë°ì´í„° í¬ì¸íŠ¸
            data_point = source_info.get('data_point', key)
            content_parts.append(f"ë°ì´í„°: {data_point}")
            
            # ì¶œì²˜
            source_name = source_info.get('source_name', 'Unknown')
            content_parts.append(f"ì¶œì²˜: {source_name}")
            
            # ì¹´í…Œê³ ë¦¬
            cat = source_info.get('category', category)
            content_parts.append(f"ë¶„ë¥˜: {cat}")
            
            # ì •ì˜
            if 'definition' in source_info:
                content_parts.append(f"ì •ì˜: {source_info['definition']}")
            
            # ê´€ë ¨ ì§ˆë¬¸ë“¤ (ì¤‘ìš”!)
            if 'related_queries' in source_info:
                queries = ", ".join(source_info['related_queries'])
                content_parts.append(f"ê´€ë ¨ ì§ˆë¬¸: {queries}")
            
            # ë…¸íŠ¸
            if 'notes' in source_info:
                notes = ", ".join(source_info['notes'])
                content_parts.append(f"ì°¸ê³ : {notes}")
            
            content = "\n".join(content_parts)
            
            # Metadata ìƒì„± (ê°’ ì¶”ì¶œìš©)
            metadata = {
                'source_id': source_info.get('source_id', f"{category.upper()}-{key}"),
                'source_name': source_name,
                'category': cat,
                'data_point': data_point,
                
                # ê°’ (í•µì‹¬!)
                'value': source_info.get('value'),
                'unit': source_info.get('unit', ''),
                'definition': source_info.get('definition', ''),
                
                # ë©”íƒ€ ì •ë³´
                'year': source_info.get('metadata', {}).get('year', ''),
                'reliability': source_info.get('metadata', {}).get('reliability', 'medium'),
                'access_method': source_info.get('metadata', {}).get('access_method', ''),
            }
            
            # Derived ê°’ (ì¼ì¼ íŒë§¤ëŸ‰ ë“±)
            if 'derived' in source_info:
                for derived_key, derived_val in source_info['derived'].items():
                    # Derived ê°’ì„ ë³„ë„ Documentë¡œ (ê²€ìƒ‰ ê°•í™”)
                    derived_content_parts = [
                        f"íŒŒìƒ ë°ì´í„°: {derived_key}",
                        f"ì›ë³¸: {data_point}",
                        f"ì¶œì²˜: {source_name}",
                        f"ê³„ì‚°: {derived_val.get('formula', '')}",
                        f"ê°’: {derived_val.get('value')} {derived_val.get('unit', '')}"
                    ]
                    
                    # ì›ë³¸ related_queriesë„ í¬í•¨ (ê²€ìƒ‰ ê°•í™”!)
                    if 'related_queries' in source_info:
                        derived_content_parts.append(f"ê´€ë ¨ ì§ˆë¬¸: {', '.join(source_info['related_queries'])}")
                    
                    derived_content = "\n".join(derived_content_parts)
                    
                    derived_metadata = metadata.copy()
                    derived_metadata['source_id'] = f"{metadata['source_id']}_derived_{derived_key}"
                    derived_metadata['value'] = derived_val.get('value')
                    derived_metadata['unit'] = derived_val.get('unit', '')
                    derived_metadata['formula'] = derived_val.get('formula', '')
                    derived_metadata['is_derived'] = True
                    derived_metadata['data_point'] = f"{data_point} ({derived_key})"
                    
                    documents.append(Document(
                        page_content=derived_content,
                        metadata=derived_metadata
                    ))
            
            # Distribution ì¶”ê°€ (ë²”ìœ„ ì •ë³´)
            if 'distribution' in source_info:
                metadata['has_distribution'] = True
                dist = source_info['distribution']
                for k, v in dist.items():
                    metadata[f'dist_{k}'] = v
            
            # Range ì¶”ê°€
            if 'range' in source_info:
                range_val = source_info['range']
                if isinstance(range_val, dict):
                    for k, v in range_val.items():
                        metadata[f'range_{k}'] = v
            
            # None ê°’ ì œê±° (Chroma ìš”êµ¬ì‚¬í•­)
            metadata = {k: v for k, v in metadata.items() if v is not None}
            
            # Document ìƒì„±
            documents.append(Document(
                page_content=content,
                metadata=metadata
            ))
    
    return documents


def build_index(yaml_path: Path, collection_name: str = "data_sources_registry"):
    """
    Data Sources Registry ì¸ë±ìŠ¤ êµ¬ì¶•
    
    Args:
        yaml_path: YAML íŒŒì¼ ê²½ë¡œ
        collection_name: Chroma collection ì´ë¦„
    """
    logger.info(f"[Build] {collection_name} êµ¬ì¶• ì‹œì‘")
    logger.info(f"  YAML: {yaml_path}")
    
    # 1. YAML ë¡œë“œ
    data = load_yaml_data(yaml_path)
    logger.info(f"  âœ… YAML ë¡œë“œ ì™„ë£Œ")
    
    # 2. Documents ìƒì„±
    documents = create_documents(data)
    logger.info(f"  âœ… {len(documents)}ê°œ Document ìƒì„±")
    
    # 3. Embeddings ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings(
        model=settings.embedding_model,
        openai_api_key=settings.openai_api_key
    )
    logger.info(f"  âœ… Embeddings ì¤€ë¹„")
    
    # 4. Chroma ì´ˆê¸°í™” (ê¸°ì¡´ ì‚­ì œ í›„ ì¬êµ¬ì¶•)
    try:
        import chromadb
        
        client = chromadb.PersistentClient(
            path=str(settings.chroma_persist_dir)
        )
        
        # ê¸°ì¡´ collection ì‚­ì œ
        try:
            client.delete_collection(collection_name)
            logger.info(f"  âœ… ê¸°ì¡´ collection ì‚­ì œ")
        except Exception:
            pass
        
        # ìƒˆ collection ìƒì„±
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name=collection_name,
            persist_directory=str(settings.chroma_persist_dir)
        )
        
        logger.info(f"  âœ… Chroma Collection ìƒì„±")
        logger.info(f"  âœ… {len(documents)}ê°œ ì²­í¬ ìƒ‰ì¸í™”")
        
    except Exception as e:
        logger.error(f"  âŒ Chroma êµ¬ì¶• ì‹¤íŒ¨: {e}")
        raise
    
    # 5. ê²€ì¦
    collection = vectorstore._collection
    count = collection.count()
    
    logger.info(f"\nâœ… êµ¬ì¶• ì™„ë£Œ!")
    logger.info(f"  Collection: {collection_name}")
    logger.info(f"  ì´ ì²­í¬: {count}ê°œ")
    
    # ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    logger.info(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
    
    test_queries = [
        "í•œêµ­ ì¸êµ¬ëŠ”?",
        "ë‹´ë°° íŒë§¤ëŸ‰ì€?",
        "SaaS ì´íƒˆë¥ ì€?"
    ]
    
    for query in test_queries:
        results = vectorstore.similarity_search_with_score(query, k=1)
        
        if results:
            doc, score = results[0]
            source = doc.metadata.get('source_name', 'Unknown')
            value = doc.metadata.get('value', 'N/A')
            
            logger.info(f"  '{query}'")
            logger.info(f"    â†’ {source}: {value} (ìœ ì‚¬ë„: {score:.3f})")
        else:
            logger.info(f"  '{query}' â†’ ê²°ê³¼ ì—†ìŒ")
    
    return vectorstore


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 80)
    print("Data Sources Registry êµ¬ì¶•")
    print("=" * 80)
    print()
    
    # YAML ê²½ë¡œ
    yaml_path = project_root / "data" / "raw" / "data_sources_registry.yaml"
    
    if not yaml_path.exists():
        print(f"âŒ YAML íŒŒì¼ ì—†ìŒ: {yaml_path}")
        sys.exit(1)
    
    # êµ¬ì¶•
    try:
        vectorstore = build_index(yaml_path)
        
        print()
        print("=" * 80)
        print("âœ… êµ¬ì¶• ì™„ë£Œ!")
        print("=" * 80)
        print()
        print("ì‚¬ìš© ì˜ˆì‹œ:")
        print("  from umis_rag.agents.validator import get_validator_rag")
        print("  validator = get_validator_rag()")
        print("  result = validator.search_definite_data('í•œêµ­ ì¸êµ¬ëŠ”?')")
        print()
        
    except Exception as e:
        print(f"âŒ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

