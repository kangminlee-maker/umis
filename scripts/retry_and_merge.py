#!/usr/bin/env python3
"""
ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¬ì‹œë„ ë° ê²°ê³¼ ë³‘í•©
"""

import json
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark


def load_previous_results(filename: str):
    """ì´ì „ ê²°ê³¼ ë¡œë“œ"""
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)


def identify_failed_cases(results):
    """ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ ì‹ë³„"""
    failed = []
    for r in results:
        if not r.get('success', False):
            model = r.get('model')
            scenario_id = r.get('scenario_id')
            error = r.get('error', '')
            
            # 404 ì—ëŸ¬ëŠ” ì œì™¸ (ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
            if '404' in error:
                continue
            
            # codex, pro ëª¨ë¸ì€ ì œì™¸ (ì ‘ê·¼ ë¶ˆê°€)
            if 'codex' in model or 'pro' in model:
                continue
            
            failed.append({
                'model': model,
                'scenario_id': scenario_id,
                'error': error
            })
    
    return failed


def retry_failed_cases(benchmark, failed_cases):
    """ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ë“¤ë§Œ ì¬ì‹œë„"""
    print(f"\nğŸ”„ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¬ì‹œë„ ì‹œì‘")
    print(f"   ì¬ì‹œë„í•  ì¼€ì´ìŠ¤: {len(failed_cases)}ê°œ")
    print()
    
    retry_results = []
    scenarios = {s['id']: s for s in benchmark.get_test_scenarios()}
    
    for idx, case in enumerate(failed_cases, 1):
        model = case['model']
        scenario_id = case['scenario_id']
        
        print(f"\n[{idx}/{len(failed_cases)}] {model} @ {scenario_id}")
        
        scenario = scenarios.get(scenario_id)
        if not scenario:
            print(f"   âš ï¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            continue
        
        try:
            if 'claude' in model.lower():
                result = benchmark.test_claude_model(model, scenario)
            else:
                result = benchmark.test_openai_model(model, scenario)
            
            retry_results.append(result)
            
            if result['success']:
                print(f"   âœ… ì„±ê³µ! í’ˆì§ˆ: {result['quality_score']['total_score']}/100")
            else:
                print(f"   âŒ ì—¬ì „íˆ ì‹¤íŒ¨: {result.get('error', '')[:80]}")
            
            import time
            time.sleep(2)  # Rate limiting
        
        except Exception as e:
            print(f"   âŒ ì˜ˆì™¸ ë°œìƒ: {str(e)[:80]}")
            retry_results.append({
                'model': model,
                'scenario_id': scenario_id,
                'error': str(e),
                'success': False
            })
    
    return retry_results


def merge_results(original_data, retry_results):
    """ê²°ê³¼ ë³‘í•© (ì¬ì‹œë„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸)"""
    print(f"\nğŸ“Š ê²°ê³¼ ë³‘í•© ì¤‘...")
    
    # ì›ë³¸ ê²°ê³¼ë¥¼ dictë¡œ ë³€í™˜ (model + scenario_idë¥¼ í‚¤ë¡œ)
    results_dict = {}
    for r in original_data['results']:
        key = (r.get('model'), r.get('scenario_id'))
        results_dict[key] = r
    
    # ì¬ì‹œë„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
    updated_count = 0
    for r in retry_results:
        key = (r.get('model'), r.get('scenario_id'))
        if key in results_dict:
            results_dict[key] = r
            updated_count += 1
    
    # ìµœì¢… ê²°ê³¼ ìƒì„±
    merged_results = list(results_dict.values())
    
    print(f"   ì—…ë°ì´íŠ¸ëœ ì¼€ì´ìŠ¤: {updated_count}ê°œ")
    print(f"   ìµœì¢… ê²°ê³¼ ìˆ˜: {len(merged_results)}ê°œ")
    
    return {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'total_tests': len(merged_results),
            'success_count': sum(1 for r in merged_results if r.get('success', False)),
            'original_file': 'benchmark_comprehensive_20251121_114452.json',
            'retry_count': len(retry_results),
            'updated_count': updated_count
        },
        'results': merged_results
    }


def generate_comprehensive_report(data):
    """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
    results = data['results']
    success = [r for r in results if r.get('success', False)]
    failed = [r for r in results if not r.get('success', False)]
    
    print(f"\n{'='*100}")
    print("ğŸ“Š ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸")
    print(f"{'='*100}\n")
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(success)}ê°œ ({len(success)/len(results)*100:.1f}%)")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ ({len(failed)/len(results)*100:.1f}%)")
    
    # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ë¥˜
    print(f"\nì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜:")
    
    error_types = {}
    for r in failed:
        error = r.get('error', 'unknown')
        if '404' in error:
            error_type = '404 (ëª¨ë¸ ì—†ìŒ)'
        elif 'codex' in r.get('model', ''):
            error_type = 'Codex (ì ‘ê·¼ ë¶ˆê°€)'
        elif 'pro' in r.get('model', ''):
            error_type = 'Pro (ì ‘ê·¼ ë¶ˆê°€)'
        else:
            error_type = error[:50]
        
        error_types[error_type] = error_types.get(error_type, 0) + 1
    
    for error_type, count in sorted(error_types.items(), key=lambda x: -x[1]):
        print(f"   - {error_type}: {count}ê°œ")
    
    # ëª¨ë¸ë³„ ì„±ê³µë¥ 
    print(f"\n{'='*100}")
    print("ëª¨ë¸ë³„ ì„±ê³µë¥ ")
    print(f"{'='*100}\n")
    
    from collections import defaultdict
    model_stats = defaultdict(lambda: {'success': 0, 'total': 0})
    
    for r in results:
        model = r.get('model')
        model_stats[model]['total'] += 1
        if r.get('success', False):
            model_stats[model]['success'] += 1
    
    # ì„±ê³µë¥  ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_models = sorted(
        model_stats.items(),
        key=lambda x: (x[1]['success'] / x[1]['total'], x[1]['success']),
        reverse=True
    )
    
    print(f"{'ëª¨ë¸':30s} | {'ì„±ê³µ':>6s} / {'ì´ê³„':>6s} | {'ì„±ê³µë¥ ':>8s}")
    print("-" * 60)
    
    for model, stats in sorted_models:
        success_rate = stats['success'] / stats['total'] * 100
        status = "âœ…" if success_rate == 100 else "âš ï¸" if success_rate >= 50 else "âŒ"
        print(f"{status} {model:27s} | {stats['success']:6d} / {stats['total']:6d} | {success_rate:7.1f}%")
    
    # ê°€ì„±ë¹„ TOP 10
    print(f"\n{'='*100}")
    print("ğŸ† ìµœê³  ê°€ì„±ë¹„ TOP 10 (ì„±ê³µí•œ ëª¨ë¸ë§Œ)")
    print(f"{'='*100}\n")
    
    model_perf = defaultdict(lambda: {'costs': [], 'quality': [], 'times': []})
    
    for r in success:
        model = r['model']
        model_perf[model]['costs'].append(r.get('cost', 0))
        model_perf[model]['quality'].append(r.get('quality_score', {}).get('total_score', 0))
        model_perf[model]['times'].append(r.get('elapsed_seconds', 0))
    
    model_avg = []
    for model, data in model_perf.items():
        if not data['costs']:
            continue
        
        avg_cost = sum(data['costs']) / len(data['costs'])
        avg_quality = sum(data['quality']) / len(data['quality'])
        avg_time = sum(data['times']) / len(data['times'])
        efficiency = avg_quality / (avg_cost * 1000) if avg_cost > 0 else 0
        
        model_avg.append({
            'model': model,
            'avg_cost': avg_cost,
            'avg_quality': avg_quality,
            'avg_time': avg_time,
            'efficiency': efficiency,
            'count': len(data['costs'])
        })
    
    model_avg.sort(key=lambda x: x['efficiency'], reverse=True)
    
    for idx, m in enumerate(model_avg[:10], 1):
        print(f"   {idx:2d}. {m['model']:30s} | ê°€ì„±ë¹„: {m['efficiency']:7.1f} | í’ˆì§ˆ: {m['avg_quality']:5.1f} | ë¹„ìš©: ${m['avg_cost']:.6f}")
    
    # Phaseë³„ ìµœì  ëª¨ë¸
    print(f"\n{'='*100}")
    print("Phaseë³„ ìµœì  ëª¨ë¸")
    print(f"{'='*100}\n")
    
    phase_results = defaultdict(list)
    for r in success:
        phase = r.get('phase', -1)
        phase_results[phase].append(r)
    
    for phase in sorted(phase_results.keys()):
        results_list = phase_results[phase]
        
        # ê°€ì„±ë¹„ ìˆœ ì •ë ¬
        sorted_results = sorted(
            results_list,
            key=lambda r: (r['quality_score']['total_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0),
            reverse=True
        )
        
        print(f"ğŸ”¹ Phase {phase}")
        print(f"   TOP 3 (ê°€ì„±ë¹„):")
        
        for idx, r in enumerate(sorted_results[:3], 1):
            efficiency = r['quality_score']['total_score'] / (r['cost'] * 1000) if r['cost'] > 0 else 0
            print(f"   {idx}. {r['model']:30s} | ê°€ì„±ë¹„: {efficiency:7.1f} | í’ˆì§ˆ: {r['quality_score']['total_score']:3d}/100 | ${r['cost']:.6f}")
        print()


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 100)
    print("ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¬ì‹œë„ ë° ê²°ê³¼ ë³‘í•©")
    print("=" * 100)
    
    # 1. ì´ì „ ê²°ê³¼ ë¡œë“œ
    previous_file = 'benchmark_comprehensive_20251121_114452.json'
    print(f"\nğŸ“‚ ì´ì „ ê²°ê³¼ ë¡œë“œ: {previous_file}")
    
    try:
        original_data = load_previous_results(previous_file)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {previous_file}")
        return
    
    print(f"   ì´ í…ŒìŠ¤íŠ¸: {original_data['metadata']['total_tests']}ê°œ")
    print(f"   ì„±ê³µ: {original_data['metadata']['success_count']}ê°œ")
    
    # 2. ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì‹ë³„
    failed_cases = identify_failed_cases(original_data['results'])
    
    print(f"\nğŸ” ì¬ì‹œë„í•  ì¼€ì´ìŠ¤ ì‹ë³„")
    print(f"   ì´ ì‹¤íŒ¨: {len([r for r in original_data['results'] if not r.get('success', False)])}ê°œ")
    print(f"   ì¬ì‹œë„ ëŒ€ìƒ: {len(failed_cases)}ê°œ (404/ì ‘ê·¼ë¶ˆê°€ ì œì™¸)")
    
    if not failed_cases:
        print("\nâœ… ì¬ì‹œë„í•  ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤!")
        print("   (ëª¨ë“  ì‹¤íŒ¨ëŠ” 404 ë˜ëŠ” ì ‘ê·¼ ë¶ˆê°€ ëª¨ë¸)")
        
        # ê·¸ë˜ë„ ë¦¬í¬íŠ¸ëŠ” ìƒì„±
        generate_comprehensive_report(original_data)
        return
    
    # 3. ì¬ì‹œë„
    benchmark = ComprehensiveLLMBenchmark()
    retry_results = retry_failed_cases(benchmark, failed_cases)
    
    # 4. ê²°ê³¼ ë³‘í•©
    merged_data = merge_results(original_data, retry_results)
    
    # 5. ì €ì¥
    output_file = f"benchmark_merged_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ë³‘í•© ê²°ê³¼ ì €ì¥: {output_file}")
    
    # 6. ì¢…í•© ë¦¬í¬íŠ¸
    generate_comprehensive_report(merged_data)
    
    print("\nğŸ‰ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

