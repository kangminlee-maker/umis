#!/usr/bin/env python3
"""
GPT-5 ê²½ëŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Responses API)
gpt-5-low, gpt-5-minimalist ë“±
"""

import sys
import os
import time

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scripts.benchmark_comprehensive_2025 import ComprehensiveLLMBenchmark


def test_lightweight_gpt5_models():
    """GPT-5 ê²½ëŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("GPT-5 ê²½ëŸ‰ ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Responses API)")
    print("=" * 80)
    print()
    
    benchmark = ComprehensiveLLMBenchmark()
    
    # í…ŒìŠ¤íŠ¸í•  ê²½ëŸ‰ ëª¨ë¸ë“¤
    lightweight_models = [
        'gpt-5-low',           # ì €ì‚¬ì–‘ ë²„ì „
        'gpt-5-minimalist',    # ìµœì†Œì£¼ì˜ ë²„ì „
        'gpt-5',               # ê¸°ë³¸ ë²„ì „ (Responses APIìš©)
        'gpt-5.1',             # ê°œì„  ë²„ì „ (Responses APIìš©)
    ]
    
    # Responses API ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    original_responses_models = benchmark.responses_api_models.copy()
    benchmark.responses_api_models.extend(lightweight_models)
    
    print("âœ… ê²½ëŸ‰ GPT-5 ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("   ëª¨ë¸: 4ê°œ")
    print("   ì‹œë‚˜ë¦¬ì˜¤: Phase 0ë§Œ (ë¹ ë¥¸ ê²€ì¦)")
    print()
    print("í…ŒìŠ¤íŠ¸ ëª¨ë¸:")
    for model in lightweight_models:
        print(f"  - {model}")
    print()
    
    scenarios = benchmark.get_test_scenarios()[:1]  # Phase 0ë§Œ
    results = []
    
    for model in lightweight_models:
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸: {model}")
        
        for scenario in scenarios:
            try:
                start = time.time()
                result = benchmark.test_openai_model(model, scenario)
                elapsed = time.time() - start
                
                results.append(result)
                
                if result['success']:
                    print(f"   âœ… ì„±ê³µ!")
                    print(f"      API: {result.get('api_type', 'chat')}")
                    print(f"      ë¹„ìš©: ${result['cost']:.6f}")
                    print(f"      ì‹œê°„: {result['elapsed_seconds']:.2f}ì´ˆ")
                    print(f"      í’ˆì§ˆ: {result['quality_score']['total_score']}/100")
                else:
                    error = result.get('error', '')
                    if '404' in error:
                        print(f"   âš ï¸ ëª¨ë¸ ì—†ìŒ (404)")
                    elif 'not supported' in error.lower():
                        print(f"   âš ï¸ Responses API ë¯¸ì§€ì›")
                    else:
                        print(f"   âŒ ì˜¤ë¥˜: {error[:80]}")
                
                time.sleep(2)  # Rate limiting
            
            except Exception as e:
                print(f"   âŒ ì˜ˆì™¸: {str(e)[:80]}")
                results.append({
                    'model': model,
                    'scenario_id': scenario['id'],
                    'error': str(e),
                    'success': False
                })
                time.sleep(2)
        
        print()
    
    # ê²°ê³¼ ìš”ì•½
    print("=" * 80)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print()
    
    success = [r for r in results if r.get('success', False)]
    failed = [r for r in results if not r.get('success', False)]
    
    print(f"ì´ í…ŒìŠ¤íŠ¸: {len(results)}ê°œ")
    print(f"ì„±ê³µ: {len(success)}ê°œ")
    print(f"ì‹¤íŒ¨: {len(failed)}ê°œ")
    print()
    
    if success:
        print("âœ… ì„±ê³µí•œ ëª¨ë¸:")
        for r in success:
            print(f"   - {r['model']}: ${r['cost']:.6f}, {r['elapsed_seconds']:.2f}ì´ˆ, {r['quality_score']['total_score']}/100")
        print()
    
    if failed:
        print("âŒ ì‹¤íŒ¨í•œ ëª¨ë¸:")
        for r in failed:
            error = r.get('error', '')[:80]
            print(f"   - {r['model']}: {error}")
        print()
    
    # ë¹„êµ ë¶„ì„
    if success:
        print("=" * 80)
        print("ğŸ’° ë¹„ìš© ë¹„êµ")
        print("=" * 80)
        print()
        
        # gpt-4.1-nano ê¸°ì¤€
        baseline_cost = 0.000023
        baseline_time = 1.32
        
        print(f"{'ëª¨ë¸':<20} | {'ë¹„ìš©':<12} | {'ì‹œê°„':<10} | {'í’ˆì§ˆ':<8} | {'ê°€ì„±ë¹„':<10} | {'vs nano'}")
        print("-" * 90)
        print(f"{'gpt-4.1-nano (ê¸°ì¤€)':<20} | ${baseline_cost:<11.6f} | {baseline_time:<9.2f}ì´ˆ | {'100':>6}/100 | {'4347.8':>8} | ê¸°ì¤€")
        
        for r in sorted(success, key=lambda x: x['cost']):
            cost_ratio = r['cost'] / baseline_cost
            time_ratio = r['elapsed_seconds'] / baseline_time
            efficiency = r['quality_score']['total_score'] / (r['cost'] * 1000)
            
            print(f"{r['model']:<20} | ${r['cost']:<11.6f} | {r['elapsed_seconds']:<9.2f}ì´ˆ | {r['quality_score']['total_score']:>6}/100 | {efficiency:>8.1f} | {cost_ratio:>4.1f}ë°°")
    
    # ë³µì›
    benchmark.responses_api_models = original_responses_models
    
    print()
    print("ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_lightweight_gpt5_models()

