# UMIS Model API Configurations (v7.11.0)
#
# 4-Stage Fusion Architecture 기반
# Stage 2-3 Timeout 및 모델 설정
#
# 사용법:
#   .env에서 LLM_MODEL_STAGE3=o1-mini 설정하면
#   자동으로 해당 모델의 API 설정 적용
#
# 하위 호환성:
#   LLM_MODEL_PHASE3, LLM_MODEL_PHASE4도 계속 동작
#   (자동 매핑: PHASE3 → STAGE2, PHASE4 → STAGE3)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# 기본값 (미지원 모델 폴백)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
defaults:
  api_type: chat
  max_output_tokens: 4096
  temperature: 0.7
  timeout_seconds: 30
  reasoning_effort:
    support: false
    default: medium

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Stage별 Timeout 설정 (v7.11.0)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Stage 2: Generative Prior (구 Phase 3 Guestimation)
# Stage 3: Structural Explanation (구 Phase 4 Fermi, 재귀 제거)
# Stage 1, 4는 LLM 미사용으로 timeout 불필요
stage_timeouts:
  stage_2_generative_prior:  # 구 Phase 3
    description: "LLM 직접 값 요청 (단일 호출)"
    default: 45
    legacy_alias: phase_3    # LLM_MODEL_PHASE3 환경변수 지원
    models:
      gpt-4o-mini: 15      # 빠름 (2-6초)
      gpt-4.1-nano: 10     # 매우 빠름 (1-3초)
      gpt-5.1: 45          # reasoning 모델 (6-35초)
      o1-mini: 45          # reasoning 모델

  stage_3_fermi:  # 구 Phase 4 (재귀 제거)
    description: "구조적 설명 (Fermi 분해, 재귀 없음, max_depth=2)"
    default: 60
    legacy_alias: phase_4    # LLM_MODEL_PHASE4 환경변수 지원
    models:
      gpt-4o-mini: 20      # 빠름 (5-15초)
      gpt-5.1: 60          # reasoning high (11-14초, 여유)
      o1-mini: 60          # reasoning (12-13초)
      o1: 90               # 대형 reasoning (느림)
      o1-pro: 120          # Pro 모델 (30-70초)
      gpt-5-pro: 180       # Pro 모델 (73초)

  # Stage 1 (Evidence Collection): LLM 미사용 (Literal, RAG, Validator만)
  # Stage 4 (Fusion): LLM 미사용 (가중 합성만)

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Legacy Phase Timeout (하위 호환성)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Deprecated: v7.11.0에서 stage_timeouts 사용 권장
# 자동 매핑: phase_3 → stage_2_generative_prior
#           phase_4 → stage_3_fermi
phase_timeouts:
  phase_3:
    default: 45
    models:
      gpt-4o-mini: 15
      gpt-4.1-nano: 10
      gpt-5.1: 45
      o1-mini: 45

  phase_4:
    default: 60
    models:
      gpt-4o-mini: 20
      gpt-5.1: 60
      o1-mini: 60
      o1: 90
      o1-pro: 120
      gpt-5-pro: 180

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# 모델별 설정
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
models:

  # ===== o-series (OpenAI Reasoning Models) =====

  o1-mini:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 128000
    notes: "STEM 최적화, 80% 저렴, Stage 3 Fermi 기본 모델 (구 Phase 4)"

  o1:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "기본 reasoning 모델, function calling 지원, Stage 3 권장"

  o1-2024-12-17:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o1의 특정 버전, Stage 3 사용 가능"

  o1-pro:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [high]  # high 고정
      fixed: high
      default: high
    temperature_support: false
    context_window: 200000
    notes: "Responses API only, 최고 성능, 비용 높음 ($150/1M input), Stage 3 Premium"

  o1-pro-2025-03-19:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [high]
      fixed: high
      default: high
    temperature_support: false
    context_window: 200000
    notes: "o1-pro의 특정 버전, Stage 3 Premium"

  o3:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o3 시리즈, Stage 3 권장"

  o3-2025-04-16:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o3의 특정 버전, Stage 3 권장"

  o3-mini:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o3 mini 버전, Stage 3 권장"

  o3-mini-2025-01-31:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o3-mini 특정 버전, Stage 3 벤치마크 최우선 후보 (계산 연결성 50/50, 개념 일관성 15/15)"

  o4-mini:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o4 mini 버전, Stage 3 권장"

  o4-mini-2025-04-16:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [low, medium, high]
      default: medium
    temperature_support: false
    context_window: 200000
    notes: "o4-mini 특정 버전, Stage 3 벤치마크 최우선 후보 (계산 연결성 50/50, 개념 일관성 15/15)"

  # ===== gpt-5 series =====

  gpt-5.1:
    api_type: responses  # Chat Completions도 지원
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [none, low, medium, high]
      default: high
    temperature_support: true  # reasoning.effort=none일 때만
    temperature_condition: reasoning_effort_none
    context_window: 196000
    notes: "Advanced reasoning, JSON 형식 약함, Stage 2-3 권장, temperature는 reasoning.effort=none일 때만"

  gpt-5-pro:
    api_type: responses  # Responses API only
    max_output_tokens: 16000
    reasoning_effort:
      support: true
      levels: [high]  # high 고정
      fixed: high
      default: high
    temperature_support: false
    context_window: 400000
    notes: "Responses API only, reasoning.effort=high 고정, temperature 미지원, Stage 3 Premium, Fast Mode 권장"

  # ===== gpt-4.1 series =====

  gpt-4.1:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: false
    temperature_support: false  # Responses API에서는 미지원
    context_window: 128000
    notes: "reasoning 미지원, Stage 2 권장"

  gpt-4.1-mini:
    api_type: responses
    max_output_tokens: 16000
    reasoning_effort:
      support: false
    temperature_support: false
    context_window: 128000
    notes: "reasoning 미지원, Stage 2 권장"

  # ===== Stage 1-2 최적화 모델 =====

  gpt-4.1-nano:
    api_type: chat
    max_output_tokens: 4096
    reasoning_effort:
      support: false
    temperature_support: true
    temperature: 0.7
    context_window: 16000
    notes: "Stage 1 (Evidence) 최적화, $0.000033/작업, 정확도 100%"

  gpt-4o-mini:
    api_type: chat
    max_output_tokens: 16000
    reasoning_effort:
      support: false
    temperature_support: true
    temperature: 0.7
    context_window: 128000
    notes: "Stage 2 (Generative Prior) 최적화, $0.000121/작업, 정확도 100%, 빠르고 저렴 (구 Phase 3)"

  # ===== Cursor Native =====

  cursor-native:
    api_type: cursor
    description: "Cursor AI - 무료, 모든 파라미터는 Cursor 내부 관리"
    cost_per_1k_input: 0.0
    cost_per_1k_output: 0.0
    notes: "Native mode, API 불필요, 패턴 매칭 기반 직접 추론, Stage 2 Fallback 가능"

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Pro 모델 (Fast Mode 대상)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
pro_models:
  - gpt-5-pro
  - o1-pro
  - o1-pro-2025-03-19

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Stage별 모델 추천 (v7.11.0)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
stage_recommendations:
  stage_1_evidence:
    description: "Evidence Collection (Literal, RAG, Validator, Guardrails)"
    llm_usage: "Guardrail Engine만 LLM 사용"
    recommended_models:
      - gpt-4.1-nano  # 저렴, 빠름
      - gpt-4o-mini   # 빠름

  stage_2_generative_prior:
    description: "LLM 직접 값 요청 (단일 호출)"
    llm_usage: "LLM 1회 호출"
    recommended_models:
      - gpt-4o-mini   # 기본 (빠름, 저렴)
      - gpt-5.1       # 고급 (reasoning)
      - o1-mini       # Premium (reasoning)

  stage_3_fermi:
    description: "구조적 설명 (Fermi 분해, 재귀 없음, max_depth=2)"
    llm_usage: "LLM 3-5회 호출 (변수 추정)"
    recommended_models:
      - o1-mini       # 기본 (STEM 최적화)
      - o3-mini-2025-01-31  # 최우선 후보 (벤치마크)
      - o4-mini-2025-04-16  # 최우선 후보 (벤치마크)
      - o1-pro        # Premium (최고 성능)

  stage_4_fusion:
    description: "가중 합성 (LLM 미사용)"
    llm_usage: "LLM 미사용 (계산만)"
    recommended_models: []

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# 벤치마크 결과 (참고용, v7.11.0 업데이트)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#
# Stage 3 (Fermi) 최우선 후보:
#   - o3-mini-2025-01-31: 계산 연결성 50/50, 개념 일관성 15/15
#   - o4-mini-2025-04-16: 계산 연결성 50/50, 개념 일관성 15/15
#
# Stage 3 프리미엄:
#   - o3-2025-04-16: 최고 품질
#   - o1-pro-2025-03-19: 최고 성능
#
# Stage 2 (Prior) 최적:
#   - gpt-4o-mini: 빠름, 저렴, 정확도 100%
#   - gpt-5.1: 고급 reasoning
#
# 실험적:
#   - gpt-5.1: 높은 추론 능력, JSON 형식 약함 (형식 점수 0-2/5)
#
# 상세: benchmarks/estimator/phase4/analysis/model_recommendations.md
#
# v7.11.0 변경사항:
# - Phase 3-4 → Stage 2-3
# - 재귀 제거로 Stage 3 속도 향상 (10-30초 → 5-15초)
# - Budget 기반 탐색 (max_llm_calls, max_depth=2)
