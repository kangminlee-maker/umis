# Tier 3 ë³€ìˆ˜ ê°œìˆ˜ ìì—° ìˆ˜ë ´ ë©”ì»¤ë‹ˆì¦˜ ì„¤ê³„

**ì‘ì„± ì¼ì‹œ**: 2025-11-08 01:20  
**ë¬¸ì œ**: ë‹¨ìˆœ ê°œìˆ˜ ì œí•œ (6ê°œ)ì˜ í•œê³„  
**ëª©í‘œ**: ë…¼ë¦¬ì ìœ¼ë¡œ ì ì ˆí•œ ìˆ˜ì¤€ìœ¼ë¡œ ìì—° ìˆ˜ë ´

---

## ğŸ¯ ë¬¸ì œ ì •ì˜

### í˜„ì¬ ë°©ì‹ì˜ í•œê³„

```yaml
í˜„ì¬ (Hard Limit):
  max_variables: 6
  7ê°œ ì´ìƒ: complexity_score = 0.0 (ê¸ˆì§€)

ë¬¸ì œ:
  âŒ ìì˜ì  ê¸°ì¤€ (ì™œ 6ê°œ?)
  âŒ ë§¥ë½ ë¬´ì‹œ (ê°„ë‹¨í•œ ë¬¸ì œë„ 6ê°œ, ë³µì¡í•œ ë¬¸ì œë„ 6ê°œ)
  âŒ ì •ë³´ ê°€ì¹˜ ë¬´ì‹œ (6ë²ˆì§¸ ë³€ìˆ˜ê°€ ì¤‘ìš”í•´ë„ ì°¨ë‹¨)
  âŒ ìˆ˜ë ´ ë…¼ë¦¬ ì—†ìŒ (ì–¸ì œ ë©ˆì¶°ì•¼ í•˜ëŠ”ì§€ ë¶ˆëª…í™•)

ì§„ì§œ ì›í•˜ëŠ” ê²ƒ:
  âœ… ë³€ìˆ˜ ì¶”ê°€ê°€ ì‹¤ì§ˆì  ê°œì„ ì„ ê°€ì ¸ì˜¬ ë•Œë§Œ ì¶”ê°€
  âœ… ê°œì„ ì´ ë¯¸ë¯¸í•˜ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì¤‘ë‹¨
  âœ… ë§¥ë½ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì¡°ì •
  âœ… ìˆ˜í•™ì /ë…¼ë¦¬ì  ê·¼ê±°
```

---

## ğŸ’¡ í•´ê²° ë°©ì•ˆ: 4ê°€ì§€ ì ‘ê·¼ë²•

### ë°©ì•ˆ 1: Marginal Confidence Gain (í•œê³„ ì‹ ë¢°ë„ ì¦ê°€)

**í•µì‹¬ ì•„ì´ë””ì–´**: "ë³€ìˆ˜ ì¶”ê°€ê°€ confidenceë¥¼ ì–¼ë§ˆë‚˜ ë†’ì´ëŠ”ê°€?"

```yaml
ì›ë¦¬:
  - ë³€ìˆ˜ Nê°œ â†’ confidence C_n
  - ë³€ìˆ˜ N+1ê°œ â†’ confidence C_{n+1}
  - Marginal Gain = (C_{n+1} - C_n) / C_n
  - Gain < ì„ê³„ê°’ (ì˜ˆ: 5%) â†’ ì¤‘ë‹¨

ë…¼ë¦¬:
  âœ… ì •ë³´ ê°€ì¹˜ ê¸°ë°˜ íŒë‹¨
  âœ… ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ë ´ (ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨)
  âœ… ë§¥ë½ ë…ë¦½ì  (ê°„ë‹¨/ë³µì¡ ëª¨ë‘ ì ìš©)
```

**êµ¬í˜„**:
```python
def _should_add_variable(
    current_model: FermiModel,
    new_variable: FermiVariable
) -> bool:
    """
    ë³€ìˆ˜ ì¶”ê°€ ì—¬ë¶€ íŒë‹¨
    
    Returns:
        True: ì¶”ê°€ (ìœ ì˜ë¯¸í•œ ê°œì„ )
        False: ì¤‘ë‹¨ (ê°œì„  ë¯¸ë¯¸)
    """
    # í˜„ì¬ ëª¨í˜• confidence
    current_confidence = self._calculate_model_confidence(current_model)
    
    # ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ confidence (ì˜ˆìƒ)
    new_confidence = self._predict_confidence_with_variable(
        current_model, new_variable
    )
    
    # Marginal Gain ê³„ì‚°
    if current_confidence > 0:
        marginal_gain = (new_confidence - current_confidence) / current_confidence
    else:
        marginal_gain = 1.0  # ì²« ë³€ìˆ˜ëŠ” ë¬´ì¡°ê±´ ì¶”ê°€
    
    # ì„ê³„ê°’ ë¹„êµ
    threshold = 0.05  # 5% ì´ìƒ ê°œì„ ë˜ì–´ì•¼ ì¶”ê°€
    
    logger.info(f"      ë³€ìˆ˜ '{new_variable.name}': "
                f"Gain {marginal_gain*100:.1f}% "
                f"({'âœ… ì¶”ê°€' if marginal_gain >= threshold else 'âŒ ì¤‘ë‹¨'})")
    
    return marginal_gain >= threshold


def _calculate_model_confidence(model: FermiModel) -> float:
    """
    ëª¨í˜• ì „ì²´ confidence ê³„ì‚°
    
    Geometric mean (ê³±ì˜ nì œê³±ê·¼):
      confidence = âˆ(var_i.confidence)^(1/n)
    """
    confidences = [
        var.confidence for var in model.variables.values()
        if var.available and var.confidence > 0
    ]
    
    if not confidences:
        return 0.0
    
    import math
    return math.prod(confidences) ** (1 / len(confidences))


def _predict_confidence_with_variable(
    model: FermiModel,
    new_var: FermiVariable
) -> float:
    """
    ìƒˆ ë³€ìˆ˜ ì¶”ê°€ ì‹œ confidence ì˜ˆìƒ
    
    ìƒˆ ë³€ìˆ˜ì˜ confidenceì™€ ê¸°ì¡´ confidenceë¥¼ ì¡°í•©
    """
    current_conf = self._calculate_model_confidence(model)
    
    if current_conf == 0:
        return new_var.confidence
    
    # Geometric meanìœ¼ë¡œ ì¡°í•©
    n = len([v for v in model.variables.values() if v.available])
    
    import math
    combined = (current_conf ** n * new_var.confidence) ** (1 / (n + 1))
    
    return combined
```

**ì˜ˆì‹œ**:
```yaml
ëª¨í˜•: "ì‹œì¥ = A Ã— B Ã— C Ã— D Ã— ..."

ë³€ìˆ˜ 1: restaurants (confidence: 0.9)
  - í˜„ì¬: 0.0
  - ì¶”ê°€ í›„: 0.9
  - Gain: âˆ â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 2: digital_rate (confidence: 0.7)
  - í˜„ì¬: 0.9
  - ì¶”ê°€ í›„: âˆš(0.9 Ã— 0.7) = 0.79
  - Gain: (0.79 - 0.9) / 0.9 = -12% â†’ í•˜ë½ì´ì§€ë§Œ í•„ìˆ˜ ì •ë³´ â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 3: conversion (confidence: 0.6)
  - í˜„ì¬: 0.79
  - ì¶”ê°€ í›„: âˆ›(0.9 Ã— 0.7 Ã— 0.6) = 0.71
  - Gain: -10% â†’ í•˜ë½ì´ì§€ë§Œ unknown ì¤„ì„ â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 4: arpu (confidence: 0.8)
  - í˜„ì¬: 0.71
  - ì¶”ê°€ í›„: âˆœ(0.9 Ã— 0.7 Ã— 0.6 Ã— 0.8) = 0.74
  - Gain: +4.2% â†’ ì„ê³„ê°’ 5% ë¯¸ë§Œ â†’ ê²½ê³„ì„ 

ë³€ìˆ˜ 5: region_weight (confidence: 0.5)
  - í˜„ì¬: 0.74
  - ì¶”ê°€ í›„: âµâˆš(...Ã— 0.5) = 0.68
  - Gain: -8% â†’ í•˜ë½ â†’ ì¤‘ë‹¨ âŒ

ê²°ë¡ : 4ê°œ ë³€ìˆ˜ê°€ ìì—° ìˆ˜ë ´ì 
```

**í‰ê°€**: â­â­â­â­â­ (5/5)
- âœ… ìˆ˜í•™ì  ê·¼ê±°
- âœ… ìì—° ìˆ˜ë ´
- âœ… ë§¥ë½ ë…ë¦½ì 

---

### ë°©ì•ˆ 2: Information Gain (ì •ë³´ ì´ë“)

**í•µì‹¬ ì•„ì´ë””ì–´**: "ë³€ìˆ˜ ì¶”ê°€ê°€ ë¶ˆí™•ì‹¤ì„±ì„ ì–¼ë§ˆë‚˜ ì¤„ì´ëŠ”ê°€?"

```yaml
ì›ë¦¬:
  - í˜„ì¬ ë¶ˆí™•ì‹¤ì„±: U_n (ì˜ˆ: error_range)
  - ë³€ìˆ˜ ì¶”ê°€ í›„: U_{n+1}
  - Information Gain = (U_n - U_{n+1}) / U_n
  - Gain < ì„ê³„ê°’ (ì˜ˆ: 10%) â†’ ì¤‘ë‹¨

ì¸¡ì •:
  ë¶ˆí™•ì‹¤ì„± = error_range ë˜ëŠ” (value_max - value_min) / value
```

**êµ¬í˜„**:
```python
def _calculate_uncertainty(model: FermiModel) -> float:
    """
    ëª¨í˜• ë¶ˆí™•ì‹¤ì„± ê³„ì‚°
    
    ë°©ë²•: ê° ë³€ìˆ˜ì˜ uncertainty ì¡°í•©
    
    Returns:
        0.0-1.0 (0 = í™•ì‹¤, 1 = ì™„ì „ ë¶ˆí™•ì‹¤)
    """
    uncertainties = [
        var.uncertainty for var in model.variables.values()
        if var.available
    ]
    
    if not uncertainties:
        return 1.0
    
    # ë¶ˆí™•ì‹¤ì„±ì€ ê³±ì…ˆìœ¼ë¡œ ì¦í­ (1ê°œë¼ë„ ë¶ˆí™•ì‹¤í•˜ë©´ ì „ì²´ ë¶ˆí™•ì‹¤)
    # Combined = 1 - âˆ(1 - u_i)
    certain_probs = [1 - u for u in uncertainties]
    combined_certain = math.prod(certain_probs)
    combined_uncertainty = 1 - combined_certain
    
    return combined_uncertainty


def _information_gain(
    current_model: FermiModel,
    new_variable: FermiVariable
) -> float:
    """
    ë³€ìˆ˜ ì¶”ê°€ ì‹œ ì •ë³´ ì´ë“ ê³„ì‚°
    
    Returns:
        0.0-1.0 (ì •ë³´ ì´ë“ ë¹„ìœ¨)
    """
    current_u = self._calculate_uncertainty(current_model)
    
    # ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ uncertainty ì˜ˆìƒ
    new_u = self._predict_uncertainty_with_variable(
        current_model, new_variable
    )
    
    # Information Gain
    if current_u > 0:
        gain = (current_u - new_u) / current_u
    else:
        gain = 0.0
    
    return gain
```

**ì˜ˆì‹œ**:
```yaml
ë³€ìˆ˜ 1: uncertainty 0.3 (Â±30%)
  - í˜„ì¬ U: 1.0 (ëª¨í˜• ì—†ìŒ)
  - ì¶”ê°€ í›„: 0.3
  - Gain: (1.0 - 0.3) / 1.0 = 70% â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 2: uncertainty 0.4
  - í˜„ì¬ U: 0.3
  - ì¶”ê°€ í›„: 1 - (1-0.3) Ã— (1-0.4) = 0.58
  - Gain: (0.3 - 0.58) / 0.3 = -93% â†’ ìƒìŠ¹! â†’ ì¬í‰ê°€ í•„ìš”

# ë” ë‚˜ì€ ê³µì‹: Root Mean Square
ë³€ìˆ˜ 2 (RMS):
  - ì¶”ê°€ í›„: âˆš(0.3Â² + 0.4Â²) = 0.5
  - Gain: (0.3 - 0.5) / 0.3 = -67% â†’ ì—¬ì „íˆ ìƒìŠ¹

# ìµœì : í‰ê· 
ë³€ìˆ˜ 2 (í‰ê· ):
  - ì¶”ê°€ í›„: (0.3 + 0.4) / 2 = 0.35
  - Gain: (0.3 - 0.35) / 0.3 = -17% â†’ ì•…í™” â†’ ì¤‘ë‹¨ âŒ
```

**í‰ê°€**: â­â­â­ (3/5)
- âœ… ì •ë³´ ì´ë¡  ê¸°ë°˜
- âš ï¸ Uncertainty ì¡°í•© ë¡œì§ ë³µì¡
- âš ï¸ ë³€ìˆ˜ ì¶”ê°€ê°€ í•­ìƒ uncertainty ì¦ê°€ (ì—­ì„¤)

---

### ë°©ì•ˆ 3: Diminishing Returns (ìˆ˜í™• ì²´ê°ì˜ ë²•ì¹™)

**í•µì‹¬ ì•„ì´ë””ì–´**: "ë³€ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê°œì„  íš¨ê³¼ ê°ì†Œ"

```yaml
ì›ë¦¬:
  - 1ë²ˆì§¸ ë³€ìˆ˜: í° ê°œì„ 
  - 2ë²ˆì§¸ ë³€ìˆ˜: ì¤‘ê°„ ê°œì„ 
  - 3ë²ˆì§¸ ë³€ìˆ˜: ì‘ì€ ê°œì„ 
  - Në²ˆì§¸ ë³€ìˆ˜: ë¯¸ë¯¸í•œ ê°œì„  â†’ ì¤‘ë‹¨

ì¸¡ì •:
  - Score improvement per variable
  - Diminishing threshold
```

**êµ¬í˜„**:
```python
def _evaluate_variable_addition(
    current_model: FermiModel,
    new_variable: FermiVariable,
    variable_sequence: int  # ëª‡ ë²ˆì§¸ ë³€ìˆ˜?
) -> Tuple[bool, float]:
    """
    ë³€ìˆ˜ ì¶”ê°€ í‰ê°€ (ìˆ˜í™• ì²´ê° ê³ ë ¤)
    
    Args:
        current_model: í˜„ì¬ ëª¨í˜•
        new_variable: ì¶”ê°€í•  ë³€ìˆ˜
        variable_sequence: ë³€ìˆ˜ ìˆœì„œ (1, 2, 3, ...)
    
    Returns:
        (should_add, improvement_score)
    """
    # í˜„ì¬ ëª¨í˜• ì ìˆ˜
    current_score = self._score_model_simple(current_model)
    
    # ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ ì ìˆ˜ (ì˜ˆìƒ)
    new_score = self._predict_score_with_variable(
        current_model, new_variable
    )
    
    # ì ˆëŒ€ ê°œì„ ëŸ‰
    improvement = new_score - current_score
    
    # ìˆ˜í™• ì²´ê° ì„ê³„ê°’ (ë³€ìˆ˜ ê°œìˆ˜ì— ë”°ë¼ ê°ì†Œ)
    # 1ë²ˆì§¸: 10% ì´ìƒ
    # 2ë²ˆì§¸: 7% ì´ìƒ
    # 3ë²ˆì§¸: 5% ì´ìƒ
    # 4ë²ˆì§¸: 3% ì´ìƒ
    # 5ë²ˆì§¸: 2% ì´ìƒ
    # 6ë²ˆì§¸ ì´í›„: 1% ì´ìƒ
    
    thresholds = {
        1: 0.10,
        2: 0.07,
        3: 0.05,
        4: 0.03,
        5: 0.02,
        6: 0.01
    }
    
    threshold = thresholds.get(variable_sequence, 0.01)
    
    # íŒë‹¨
    should_add = improvement >= threshold
    
    logger.info(f"      ë³€ìˆ˜ {variable_sequence}: "
                f"ê°œì„  {improvement*100:.1f}% "
                f"(ì„ê³„ê°’ {threshold*100:.1f}%) "
                f"â†’ {'âœ… ì¶”ê°€' if should_add else 'âŒ ì¤‘ë‹¨'}")
    
    return should_add, improvement
```

**ì˜ˆì‹œ**:
```yaml
ë³€ìˆ˜ 1: improvement 15% (ì„ê³„ê°’ 10%)
  â†’ 15% > 10% â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 2: improvement 8% (ì„ê³„ê°’ 7%)
  â†’ 8% > 7% â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 3: improvement 6% (ì„ê³„ê°’ 5%)
  â†’ 6% > 5% â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 4: improvement 3.5% (ì„ê³„ê°’ 3%)
  â†’ 3.5% > 3% â†’ ì¶”ê°€ âœ…

ë³€ìˆ˜ 5: improvement 1.5% (ì„ê³„ê°’ 2%)
  â†’ 1.5% < 2% â†’ ì¤‘ë‹¨ âŒ

ê²°ë¡ : 4ê°œ ë³€ìˆ˜ë¡œ ìì—° ìˆ˜ë ´
```

**í‰ê°€**: â­â­â­â­â­ (5/5)
- âœ… ë…¼ë¦¬ì  (ìˆ˜í™• ì²´ê°)
- âœ… ìì—° ìˆ˜ë ´
- âœ… ì‹¤ìš©ì 

---

### ë°©ì•ˆ 4: Hybrid - ì¢…í•© ì ‘ê·¼ (ì¶”ì²œ!) â­

**í•µì‹¬ ì•„ì´ë””ì–´**: ì—¬ëŸ¬ ì‹œê·¸ë„ì„ ì¢…í•© íŒë‹¨

```yaml
íŒë‹¨ ê¸°ì¤€ (3ê°œ):
  
  1. Marginal Confidence Gain (ì£¼ìš”)
     - ë³€ìˆ˜ ì¶”ê°€ ì‹œ confidence ê°œì„ 
     - ì„ê³„ê°’: 5% ì´ìƒ
  
  2. Diminishing Returns (ë³´ì¡°)
     - ë³€ìˆ˜ ìˆœì„œì— ë”°ë¥¸ ì„ê³„ê°’ ê°ì†Œ
     - 1ë²ˆì§¸: 10%, 2ë²ˆì§¸: 7%, 3ë²ˆì§¸: 5%, ...
  
  3. Absolute Limit (ì•ˆì „ë§)
     - 10ê°œ ì´ìƒ: ë¬´ì¡°ê±´ ì¤‘ë‹¨ (ë¹„ìƒ ë¸Œë ˆì´í¬)
     - ë…¼ë¦¬ì  í•œê³„ (ì¸ê°„ ì¸ì§€)

ì¢…í•© íŒë‹¨:
  - (ê¸°ì¤€ 1 OR ê¸°ì¤€ 2) AND ê¸°ì¤€ 3
  - ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ í†µê³¼ + 10ê°œ ë¯¸ë§Œ â†’ ì¶”ê°€
```

**êµ¬í˜„**:
```python
class VariableConvergence:
    """
    ë³€ìˆ˜ ê°œìˆ˜ ìì—° ìˆ˜ë ´ ë©”ì»¤ë‹ˆì¦˜
    
    3ê°€ì§€ ê¸°ì¤€ ì¢…í•©:
    1. Marginal Confidence Gain (ì£¼ìš”)
    2. Diminishing Returns (ë³´ì¡°)
    3. Absolute Limit (ì•ˆì „ë§)
    """
    
    def __init__(self):
        # ê¸°ì¤€ 1: Marginal Gain
        self.min_confidence_gain = 0.05  # 5% ì´ìƒ
        
        # ê¸°ì¤€ 2: Diminishing Returns
        self.diminishing_thresholds = {
            1: 0.10,  # ì²« ë³€ìˆ˜: 10% ì´ìƒ ê°œì„ 
            2: 0.07,
            3: 0.05,
            4: 0.03,
            5: 0.02,
            6: 0.01,
            7: 0.005,
            8: 0.003,
            9: 0.001
        }
        
        # ê¸°ì¤€ 3: Absolute Limit
        self.absolute_max = 10  # ë¹„ìƒ ë¸Œë ˆì´í¬
        self.recommended_max = 6  # ê¶Œì¥ ìƒí•œ (ê²½ê³ ë§Œ)
    
    def should_add_variable(
        self,
        current_model: FermiModel,
        new_variable: FermiVariable,
        variable_sequence: int
    ) -> Tuple[bool, str]:
        """
        ë³€ìˆ˜ ì¶”ê°€ ì—¬ë¶€ íŒë‹¨ (ì¢…í•©)
        
        Returns:
            (should_add, reason)
        """
        # â”â”â”â” ê¸°ì¤€ 3: Absolute Limit (ë¨¼ì € ì²´í¬) â”â”â”â”
        if variable_sequence > self.absolute_max:
            return False, f"ì ˆëŒ€ ìƒí•œ {self.absolute_max}ê°œ ì´ˆê³¼ (ë¹„ìƒ ë¸Œë ˆì´í¬)"
        
        if variable_sequence > self.recommended_max:
            logger.warning(f"      âš ï¸  ê¶Œì¥ ìƒí•œ {self.recommended_max}ê°œ ì´ˆê³¼ (ê²€í†  í•„ìš”)")
        
        # â”â”â”â” ê¸°ì¤€ 1: Marginal Confidence Gain â”â”â”â”
        current_conf = self._calculate_confidence(current_model)
        new_conf = self._predict_confidence(current_model, new_variable)
        
        if current_conf > 0:
            conf_gain = (new_conf - current_conf) / current_conf
        else:
            conf_gain = 1.0  # ì²« ë³€ìˆ˜
        
        conf_check = conf_gain >= self.min_confidence_gain
        
        # â”â”â”â” ê¸°ì¤€ 2: Diminishing Returns â”â”â”â”
        current_score = self._calculate_score(current_model)
        new_score = self._predict_score(current_model, new_variable)
        score_improvement = new_score - current_score
        
        threshold = self.diminishing_thresholds.get(variable_sequence, 0.001)
        dim_check = score_improvement >= threshold
        
        # â”â”â”â” ì¢…í•© íŒë‹¨ â”â”â”â”
        should_add = conf_check or dim_check
        
        # ì´ìœ  ì„¤ëª…
        if should_add:
            reasons = []
            if conf_check:
                reasons.append(f"Confidence Gain {conf_gain*100:.1f}% â‰¥ 5%")
            if dim_check:
                reasons.append(f"Score ê°œì„  {score_improvement*100:.1f}% â‰¥ {threshold*100:.1f}%")
            reason = " OR ".join(reasons) + " â†’ ì¶”ê°€"
        else:
            reason = (f"Confidence Gain {conf_gain*100:.1f}% < 5% AND "
                     f"Score ê°œì„  {score_improvement*100:.1f}% < {threshold*100:.1f}% "
                     f"â†’ ì¤‘ë‹¨ (ìì—° ìˆ˜ë ´)")
        
        logger.info(f"      ë³€ìˆ˜ {variable_sequence} '{new_variable.name}': {reason}")
        
        return should_add, reason
    
    def _calculate_confidence(self, model: FermiModel) -> float:
        """Geometric mean of confidences"""
        confidences = [
            var.confidence for var in model.variables.values()
            if var.available and var.confidence > 0
        ]
        
        if not confidences:
            return 0.0
        
        import math
        return math.prod(confidences) ** (1 / len(confidences))
    
    def _predict_confidence(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ ì‹œ confidence ì˜ˆìƒ"""
        current = self._calculate_confidence(model)
        
        if current == 0:
            return new_var.confidence
        
        n = len([v for v in model.variables.values() if v.available])
        
        import math
        return (current ** n * new_var.confidence) ** (1 / (n + 1))
    
    def _calculate_score(self, model: FermiModel) -> float:
        """ëª¨í˜• ì „ì²´ ì ìˆ˜ (ê°„ì†Œí™”)"""
        # Unknown ë¹„ìœ¨
        if model.total_variables > 0:
            filled_ratio = sum(1 for v in model.variables.values() if v.available) / model.total_variables
        else:
            filled_ratio = 0
        
        # Confidence
        avg_conf = self._calculate_confidence(model)
        
        # ì¡°í•©
        return filled_ratio * 0.6 + avg_conf * 0.4
    
    def _predict_score(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ ì ìˆ˜ ì˜ˆìƒ"""
        # ì„ì‹œ ë³€ìˆ˜ ì¶”ê°€
        temp_model = copy.deepcopy(model)
        temp_model.variables[new_var.name] = new_var
        temp_model.total_variables += 1
        
        return self._calculate_score(temp_model)
```

**ì˜ˆì‹œ**:
```yaml
ëª¨í˜•: ì‹œì¥ = A Ã— B Ã— C Ã— ...

ë³€ìˆ˜ 1: restaurants (conf: 0.9)
  - Conf Gain: âˆ â†’ ì¶”ê°€ âœ…
  - Score ê°œì„ : 60% > 10% â†’ ì¶”ê°€ âœ…
  â†’ ì¢…í•©: ì¶”ê°€ âœ…

ë³€ìˆ˜ 2: digital (conf: 0.7)
  - Conf Gain: -12% < 5% â†’ ì¤‘ë‹¨ âŒ
  - Score ê°œì„ : 15% > 7% â†’ ì¶”ê°€ âœ…
  â†’ ì¢…í•©: ì¶”ê°€ âœ… (OR ì¡°ê±´)

ë³€ìˆ˜ 3: conversion (conf: 0.6)
  - Conf Gain: -10% < 5% â†’ ì¤‘ë‹¨ âŒ
  - Score ê°œì„ : 8% > 5% â†’ ì¶”ê°€ âœ…
  â†’ ì¢…í•©: ì¶”ê°€ âœ…

ë³€ìˆ˜ 4: arpu (conf: 0.8)
  - Conf Gain: +4% < 5% â†’ ì¤‘ë‹¨ âŒ
  - Score ê°œì„ : 3.5% > 3% â†’ ì¶”ê°€ âœ…
  â†’ ì¢…í•©: ì¶”ê°€ âœ…

ë³€ìˆ˜ 5: region (conf: 0.5)
  - Conf Gain: -8% < 5% â†’ ì¤‘ë‹¨ âŒ
  - Score ê°œì„ : 1% < 2% â†’ ì¤‘ë‹¨ âŒ
  â†’ ì¢…í•©: ì¤‘ë‹¨ âŒ (AND ì¡°ê±´)

ê²°ë¡ : 4ê°œ ë³€ìˆ˜ë¡œ ìì—° ìˆ˜ë ´
```

**í‰ê°€**: â­â­â­â­â­ (5/5) **ìµœê³ !**
- âœ… ë‹¤ê°ì  íŒë‹¨
- âœ… ìœ ì—°í•¨ (OR ì¡°ê±´)
- âœ… ì•ˆì „ë§ (10ê°œ ì ˆëŒ€ ìƒí•œ)
- âœ… ê²½ê³  ì‹œìŠ¤í…œ (6ê°œ ì´ˆê³¼ ì‹œ)

---

## ğŸ¯ ì¶”ì²œ ë°©ì•ˆ: Hybrid ì¢…í•© ì ‘ê·¼

### ìµœì¢… ì„¤ê³„

```python
class VariableConvergencePolicy:
    """
    ë³€ìˆ˜ ê°œìˆ˜ ìì—° ìˆ˜ë ´ ì •ì±…
    
    3ë‹¨ê³„ ë°©ì–´:
    1. ë…¼ë¦¬ì  íŒë‹¨ (Confidence Gain OR Diminishing Returns)
    2. ê¶Œì¥ ìƒí•œ (6ê°œ ì´ˆê³¼ ì‹œ ê²½ê³ )
    3. ì ˆëŒ€ ìƒí•œ (10ê°œ ì´ˆê³¼ ê¸ˆì§€)
    """
    
    def __init__(self):
        # Level 1: ë…¼ë¦¬ì  íŒë‹¨
        self.min_confidence_gain = 0.05  # 5% ì´ìƒ ê°œì„ 
        
        self.diminishing_thresholds = {
            1: 0.10,  # 10%
            2: 0.07,
            3: 0.05,
            4: 0.03,
            5: 0.02,
            6: 0.01,
            7: 0.005,
            8: 0.003,
            9: 0.001
        }
        
        # Level 2: ê¶Œì¥ ìƒí•œ (ê²½ê³ )
        self.recommended_max = 6
        
        # Level 3: ì ˆëŒ€ ìƒí•œ (ê¸ˆì§€)
        self.absolute_max = 10
    
    def evaluate(
        self,
        current_model: FermiModel,
        new_variable: FermiVariable,
        variable_sequence: int
    ) -> Dict:
        """
        ë³€ìˆ˜ ì¶”ê°€ í‰ê°€
        
        Returns:
            {
                'should_add': bool,
                'reason': str,
                'confidence_gain': float,
                'score_improvement': float,
                'warning': Optional[str]
            }
        """
        result = {
            'should_add': False,
            'reason': '',
            'confidence_gain': 0.0,
            'score_improvement': 0.0,
            'warning': None
        }
        
        # â”â”â” Level 3: ì ˆëŒ€ ìƒí•œ ì²´í¬ â”â”â”
        if variable_sequence > self.absolute_max:
            result['should_add'] = False
            result['reason'] = (
                f"ì ˆëŒ€ ìƒí•œ {self.absolute_max}ê°œ ì´ˆê³¼ "
                f"(ë¹„ìƒ ë¸Œë ˆì´í¬, ì¸ê°„ ì¸ì§€ í•œê³„)"
            )
            return result
        
        # â”â”â” Level 2: ê¶Œì¥ ìƒí•œ ê²½ê³  â”â”â”
        if variable_sequence > self.recommended_max:
            result['warning'] = (
                f"âš ï¸  ê¶Œì¥ ìƒí•œ {self.recommended_max}ê°œ ì´ˆê³¼ "
                f"(ë³µì¡ë„ ì¦ê°€, Occam's Razor ìœ„ë°°)"
            )
        
        # â”â”â” Level 1: ë…¼ë¦¬ì  íŒë‹¨ â”â”â”
        
        # ê¸°ì¤€ 1: Marginal Confidence Gain
        current_conf = self._geometric_mean_confidence(current_model)
        new_conf = self._predict_confidence(current_model, new_variable)
        
        if current_conf > 0:
            conf_gain = (new_conf - current_conf) / current_conf
        else:
            conf_gain = 1.0
        
        result['confidence_gain'] = conf_gain
        criterion_1 = conf_gain >= self.min_confidence_gain
        
        # ê¸°ì¤€ 2: Diminishing Returns
        current_score = self._calculate_model_score(current_model)
        new_score = self._predict_score(current_model, new_variable)
        score_improvement = new_score - current_score
        
        threshold = self.diminishing_thresholds.get(variable_sequence, 0.001)
        result['score_improvement'] = score_improvement
        criterion_2 = score_improvement >= threshold
        
        # â”â”â” ì¢…í•© íŒë‹¨ (OR) â”â”â”
        result['should_add'] = criterion_1 or criterion_2
        
        # ì´ìœ  ìƒì„±
        if result['should_add']:
            reasons = []
            if criterion_1:
                reasons.append(f"âœ… Conf Gain {conf_gain*100:.1f}% â‰¥ 5%")
            if criterion_2:
                reasons.append(f"âœ… Score +{score_improvement*100:.1f}% â‰¥ {threshold*100:.1f}%")
            
            result['reason'] = " OR ".join(reasons)
        else:
            result['reason'] = (
                f"âŒ Conf Gain {conf_gain*100:.1f}% < 5% AND "
                f"Score +{score_improvement*100:.1f}% < {threshold*100:.1f}% "
                f"â†’ ìì—° ìˆ˜ë ´"
            )
        
        return result
    
    def _geometric_mean_confidence(self, model: FermiModel) -> float:
        """Confidence geometric mean"""
        confs = [v.confidence for v in model.variables.values() if v.available]
        if not confs:
            return 0.0
        import math
        return math.prod(confs) ** (1 / len(confs))
    
    def _predict_confidence(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ confidence ì˜ˆìƒ"""
        current = self._geometric_mean_confidence(model)
        if current == 0:
            return new_var.confidence
        
        n = len([v for v in model.variables.values() if v.available])
        import math
        return (current ** n * new_var.confidence) ** (1 / (n + 1))
    
    def _calculate_model_score(self, model: FermiModel) -> float:
        """ëª¨í˜• ì „ì²´ ì ìˆ˜"""
        if model.total_variables == 0:
            return 0.0
        
        # Unknown ë¹„ìœ¨
        filled = sum(1 for v in model.variables.values() if v.available)
        filled_ratio = filled / model.total_variables
        
        # Confidence
        conf = self._geometric_mean_confidence(model)
        
        # ì¡°í•© (60% filled, 40% confidence)
        return filled_ratio * 0.6 + conf * 0.4
    
    def _predict_score(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ ì ìˆ˜"""
        import copy
        temp_model = copy.deepcopy(model)
        temp_model.variables[new_var.name] = new_var
        temp_model.total_variables += 1
        return self._calculate_model_score(temp_model)
```

---

## ğŸ“Š Hybrid ë°©ì•ˆ ì‹œë®¬ë ˆì´ì…˜

### ì‹œë‚˜ë¦¬ì˜¤ 1: ê°„ë‹¨í•œ ë¬¸ì œ

```yaml
ì§ˆë¬¸: "B2B SaaS Churn RateëŠ”?"

ëª¨í˜• í›„ë³´:
  Model 1: "Churn = ì—…ê³„ í‰ê· "
    - ë³€ìˆ˜ 1ê°œ
    - confidence: 0.7
  
  Model 2: "Churn = (í•´ì§€ / ì „ì²´) Ã— Loss Aversion"
    - ë³€ìˆ˜ 3ê°œ
    - ë” ë³µì¡

í‰ê°€:
  ë³€ìˆ˜ 1: avg (conf: 0.7)
    - Conf Gain: âˆ â†’ âœ…
    - Score: +60% > 10% â†’ âœ…
    â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 2: í•´ì§€ìœ¨ (conf: 0.6)
    - Conf Gain: -14% < 5% â†’ âŒ
    - Score: +5% < 7% â†’ âŒ
    â†’ ì¤‘ë‹¨ âŒ

ê²°ë¡ : 1ê°œ ë³€ìˆ˜ë¡œ ì¶©ë¶„ (ë‹¨ìˆœ ë¬¸ì œ)
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë³µì¡í•œ ë¬¸ì œ

```yaml
ì§ˆë¬¸: "ìŒì‹ì  ë§ˆì¼€íŒ… SaaS ì‹œì¥ì€?"

ëª¨í˜•: "ì‹œì¥ = ìŒì‹ì  Ã— ë””ì§€í„¸ìœ¨ Ã— ì „í™˜ìœ¨ Ã— ARPU Ã— 12"

í‰ê°€:
  ë³€ìˆ˜ 1: restaurants (conf: 0.9)
    - Gain: âˆ â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 2: digital (conf: 0.7)
    - Conf: 0.9 â†’ 0.79 (Gain -12%)
    - Score: +15% > 7% â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 3: conversion (conf: 0.6)
    - Conf: 0.79 â†’ 0.71 (Gain -10%)
    - Score: +8% > 5% â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 4: arpu (conf: 0.8)
    - Conf: 0.71 â†’ 0.74 (Gain +4%)
    - Score: +3.5% > 3% â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 5: multiplier (conf: 1.0)
    - Conf: 0.74 â†’ 0.79 (Gain +7%) â†’ âœ…
    - Score: +2% = 2% â†’ âœ…
    â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 6: region (conf: 0.5)
    - Conf: 0.79 â†’ 0.73 (Gain -8%) â†’ âŒ
    - Score: +0.8% < 1% â†’ âŒ
    â†’ ì¤‘ë‹¨ âŒ

ê²°ë¡ : 5ê°œ ë³€ìˆ˜ë¡œ ìˆ˜ë ´ (ë³µì¡í•œ ë¬¸ì œëŠ” ë” ë§ì´)
```

---

### ì‹œë‚˜ë¦¬ì˜¤ 3: ë§¤ìš° ë³µì¡í•œ ë¬¸ì œ

```yaml
ì§ˆë¬¸: "êµ­ë‚´ ì „ì²´ B2B SaaS ì‹œì¥ (ì‚°ì—…ë³„ ì„¸ë¶„í™”)"

ëª¨í˜•: "ì‹œì¥ = Î£(ì‚°ì—…_i Ã— ë„ì…ë¥ _i Ã— ARPU_i)"

ë³€ìˆ˜: 10ê°œ ì‚°ì—… Ã— 2ê°œ íŒŒë¼ë¯¸í„° = 20ê°œ í•„ìš”?

í‰ê°€:
  ë³€ìˆ˜ 1-6: ê°ê° ê°œì„  â†’ ì¶”ê°€ âœ…
  ë³€ìˆ˜ 7: 
    - Conf Gain: +0.4% < 5% â†’ âŒ
    - Score: +0.6% > 0.5% â†’ âœ…
    - ê²½ê³ : âš ï¸  ê¶Œì¥ ìƒí•œ 6ê°œ ì´ˆê³¼
    â†’ ì¶”ê°€ âœ… (ê²½ê³  í¬í•¨)
  
  ë³€ìˆ˜ 8:
    - Conf Gain: +0.2% < 5% â†’ âŒ
    - Score: +0.4% > 0.3% â†’ âœ…
    â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 9:
    - Conf Gain: +0.1% < 5% â†’ âŒ
    - Score: +0.15% > 0.1% â†’ âœ…
    â†’ ì¶”ê°€ âœ…
  
  ë³€ìˆ˜ 10:
    - Conf Gain: +0.05% < 5% â†’ âŒ
    - Score: +0.08% < 0.1% â†’ âŒ
    â†’ ì¤‘ë‹¨ âŒ

ê²°ë¡ : 9ê°œ ë³€ìˆ˜ (ë§¤ìš° ë³µì¡í•œ ë¬¸ì œë„ 10ê°œ ë¯¸ë§Œ)
```

---

## ğŸ“ˆ ë°©ì•ˆ ë¹„êµ

| ë°©ì•ˆ | ë…¼ë¦¬ì„± | ì‹¤ìš©ì„± | ìˆ˜ë ´ì„± | ë³µì¡ë„ | ì¶”ì²œ |
|------|--------|--------|--------|--------|------|
| **í˜„ì¬ (Hard 6)** | â­â­ | â­â­â­ | â­â­ | â­â­â­â­â­ | - |
| **ë°©ì•ˆ 1: Marginal Gain** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | âœ… |
| **ë°©ì•ˆ 2: Information Gain** | â­â­â­â­ | â­â­ | â­â­â­ | â­â­ | - |
| **ë°©ì•ˆ 3: Diminishing Returns** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | âœ… |
| **ë°©ì•ˆ 4: Hybrid** â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­ | **âœ… ìµœê³ ** |

**ì¶”ì²œ**: **ë°©ì•ˆ 4 (Hybrid ì¢…í•© ì ‘ê·¼)**

---

## ğŸ”§ êµ¬í˜„ ì½”ë“œ

### tier3.pyì— ì¶”ê°€

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë³€ìˆ˜ ìˆ˜ë ´ ë©”ì»¤ë‹ˆì¦˜ (v7.4.0)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class VariableConvergencePolicy:
    """
    ë³€ìˆ˜ ê°œìˆ˜ ìì—° ìˆ˜ë ´ ì •ì±…
    
    ì›ì¹™:
    -----
    - ë‹¨ìˆœ ê°œìˆ˜ ì œí•œ âŒ
    - ë…¼ë¦¬ì  ìˆ˜ë ´ âœ…
    
    3ë‹¨ê³„ ë°©ì–´:
    -----------
    1. ë…¼ë¦¬ì  íŒë‹¨ (Confidence Gain OR Diminishing Returns)
    2. ê¶Œì¥ ìƒí•œ (6ê°œ ì´ˆê³¼ ì‹œ ê²½ê³ , ê³„ì† ê°€ëŠ¥)
    3. ì ˆëŒ€ ìƒí•œ (10ê°œ ì´ˆê³¼ ì‹œ ê°•ì œ ì¤‘ë‹¨)
    
    íš¨ê³¼:
    -----
    - ê°„ë‹¨í•œ ë¬¸ì œ: 1-3ê°œë¡œ ìì—° ìˆ˜ë ´
    - ì¤‘ê°„ ë¬¸ì œ: 4-6ê°œ
    - ë³µì¡í•œ ë¬¸ì œ: 7-9ê°œ (ê²½ê³  í¬í•¨)
    - ë§¤ìš° ë³µì¡: ìµœëŒ€ 10ê°œ (ì ˆëŒ€ ìƒí•œ)
    
    ì˜ˆì‹œ:
        >>> policy = VariableConvergencePolicy()
        >>> result = policy.evaluate(
        ...     current_model=model,
        ...     new_variable=var,
        ...     variable_sequence=5
        ... )
        >>> if result['should_add']:
        ...     model.add_variable(var)
        ... else:
        ...     print(f"ìˆ˜ë ´: {result['reason']}")
    """
    
    def __init__(
        self,
        min_confidence_gain: float = 0.05,
        recommended_max: int = 6,
        absolute_max: int = 10
    ):
        """
        Args:
            min_confidence_gain: ìµœì†Œ confidence ê°œì„  (ê¸°ë³¸ 5%)
            recommended_max: ê¶Œì¥ ìƒí•œ (ê¸°ë³¸ 6ê°œ)
            absolute_max: ì ˆëŒ€ ìƒí•œ (ê¸°ë³¸ 10ê°œ)
        """
        self.min_confidence_gain = min_confidence_gain
        self.recommended_max = recommended_max
        self.absolute_max = absolute_max
        
        # Diminishing Returns ì„ê³„ê°’ (ë³€ìˆ˜ ê°œìˆ˜ë³„)
        self.diminishing_thresholds = {
            1: 0.10,  # ì²« ë³€ìˆ˜: 10% ì´ìƒ ê°œì„ 
            2: 0.07,  # ë‘˜ì§¸: 7%
            3: 0.05,  # ì…‹ì§¸: 5%
            4: 0.03,  # ë„·ì§¸: 3%
            5: 0.02,  # ë‹¤ì„¯ì§¸: 2%
            6: 0.01,  # ì—¬ì„¯ì§¸: 1%
            7: 0.005, # ì¼ê³±ì§¸: 0.5%
            8: 0.003, # ì—¬ëŸì§¸: 0.3%
            9: 0.001  # ì•„í™‰ì§¸: 0.1%
        }
    
    def evaluate(
        self,
        current_model: FermiModel,
        new_variable: FermiVariable,
        variable_sequence: int
    ) -> Dict[str, Any]:
        """
        ë³€ìˆ˜ ì¶”ê°€ ì—¬ë¶€ í‰ê°€
        
        Args:
            current_model: í˜„ì¬ ëª¨í˜•
            new_variable: ì¶”ê°€ ê³ ë ¤ ì¤‘ì¸ ë³€ìˆ˜
            variable_sequence: ë³€ìˆ˜ ìˆœì„œ (1, 2, 3, ...)
        
        Returns:
            {
                'should_add': bool,
                'reason': str,
                'confidence_gain': float,
                'score_improvement': float,
                'warning': Optional[str],
                'level': int  # ì–´ëŠ ë‹¨ê³„ì—ì„œ ê²°ì •? (1/2/3)
            }
        """
        result = {
            'should_add': False,
            'reason': '',
            'confidence_gain': 0.0,
            'score_improvement': 0.0,
            'warning': None,
            'level': 0
        }
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Level 3: ì ˆëŒ€ ìƒí•œ (ë¹„ìƒ ë¸Œë ˆì´í¬)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if variable_sequence > self.absolute_max:
            result['should_add'] = False
            result['reason'] = (
                f"ğŸ›‘ ì ˆëŒ€ ìƒí•œ {self.absolute_max}ê°œ ì´ˆê³¼ "
                f"(ì¸ê°„ ì¸ì§€ í•œê³„, Miller's Law: 7Â±2)"
            )
            result['level'] = 3
            logger.warning(f"    {result['reason']}")
            return result
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Level 2: ê¶Œì¥ ìƒí•œ (ê²½ê³ )
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        if variable_sequence > self.recommended_max:
            result['warning'] = (
                f"âš ï¸  ê¶Œì¥ ìƒí•œ {self.recommended_max}ê°œ ì´ˆê³¼ "
                f"(Occam's Razor ìœ„ë°°, ë³µì¡ë„â†‘)"
            )
            logger.warning(f"    {result['warning']}")
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # Level 1: ë…¼ë¦¬ì  íŒë‹¨
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        
        # ê¸°ì¤€ 1: Marginal Confidence Gain
        current_conf = self._geometric_mean_confidence(current_model)
        new_conf = self._predict_confidence(current_model, new_variable)
        
        if current_conf > 0:
            conf_gain = (new_conf - current_conf) / current_conf
        else:
            conf_gain = 1.0  # ì²« ë³€ìˆ˜ëŠ” ë¬´ì¡°ê±´ ì¶”ê°€
        
        result['confidence_gain'] = conf_gain
        
        criterion_1_pass = conf_gain >= self.min_confidence_gain
        
        # ê¸°ì¤€ 2: Diminishing Returns
        current_score = self._calculate_model_score(current_model)
        new_score = self._predict_score(current_model, new_variable)
        score_improvement = new_score - current_score
        
        threshold = self.diminishing_thresholds.get(variable_sequence, 0.001)
        result['score_improvement'] = score_improvement
        
        criterion_2_pass = score_improvement >= threshold
        
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        # ì¢…í•© íŒë‹¨ (OR ì¡°ê±´)
        # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        result['should_add'] = criterion_1_pass or criterion_2_pass
        result['level'] = 1
        
        # ì´ìœ  ìƒì„±
        if result['should_add']:
            reasons = []
            if criterion_1_pass:
                reasons.append(
                    f"Conf Gain {conf_gain*100:+.1f}% â‰¥ {self.min_confidence_gain*100:.0f}%"
                )
            if criterion_2_pass:
                reasons.append(
                    f"Score +{score_improvement*100:.1f}% â‰¥ {threshold*100:.1f}%"
                )
            
            result['reason'] = "âœ… " + " OR ".join(reasons) + " â†’ ì¶”ê°€"
        else:
            result['reason'] = (
                f"âŒ Conf Gain {conf_gain*100:+.1f}% < {self.min_confidence_gain*100:.0f}% "
                f"AND Score +{score_improvement*100:.1f}% < {threshold*100:.1f}% "
                f"â†’ ìì—° ìˆ˜ë ´ (ë” ì´ìƒ ê°œì„  ì—†ìŒ)"
            )
        
        logger.info(f"    ë³€ìˆ˜ {variable_sequence} '{new_variable.name}': {result['reason']}")
        
        return result
    
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # í—¬í¼ ë©”ì„œë“œ
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    def _geometric_mean_confidence(self, model: FermiModel) -> float:
        """
        Confidence Geometric Mean
        
        ì´ìœ : ê³±ì…ˆ ëª¨í˜•ì—ì„œ geometric meanì´ ì í•©
        """
        confidences = [
            var.confidence for var in model.variables.values()
            if var.available and var.confidence > 0
        ]
        
        if not confidences:
            return 0.0
        
        import math
        return math.prod(confidences) ** (1 / len(confidences))
    
    def _predict_confidence(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ confidence ì˜ˆìƒ"""
        current = self._geometric_mean_confidence(model)
        
        if current == 0:
            return new_var.confidence
        
        n = len([v for v in model.variables.values() if v.available])
        
        import math
        # (current^n Ã— new_conf)^(1/(n+1))
        return (current ** n * new_var.confidence) ** (1 / (n + 1))
    
    def _calculate_model_score(self, model: FermiModel) -> float:
        """
        ëª¨í˜• ì „ì²´ ì ìˆ˜
        
        ì¡°í•©:
        - 60%: ë³€ìˆ˜ ì±„ì›€ ë¹„ìœ¨ (unknown í•´ê²°)
        - 40%: í‰ê·  confidence (í’ˆì§ˆ)
        """
        if model.total_variables == 0:
            return 0.0
        
        # ë³€ìˆ˜ ì±„ì›€ ë¹„ìœ¨
        filled = sum(1 for v in model.variables.values() if v.available)
        filled_ratio = filled / model.total_variables
        
        # Confidence
        conf = self._geometric_mean_confidence(model)
        
        # ê°€ì¤‘ í‰ê· 
        return filled_ratio * 0.6 + conf * 0.4
    
    def _predict_score(
        self,
        model: FermiModel,
        new_var: FermiVariable
    ) -> float:
        """ìƒˆ ë³€ìˆ˜ ì¶”ê°€ í›„ ì ìˆ˜ ì˜ˆìƒ"""
        import copy
        temp_model = copy.deepcopy(model)
        temp_model.variables[new_var.name] = new_var
        temp_model.total_variables += 1
        
        return self._calculate_model_score(temp_model)
```

---

## ğŸ¯ Tier 3ì—ì„œ í™œìš©

### Phase 2: ëª¨í˜• ìƒì„± ì‹œ

```python
def _phase2_generate_models(...) -> List[FermiModel]:
    """
    LLM ëª¨í˜• ìƒì„± + ë³€ìˆ˜ ìˆ˜ë ´ ì²´í¬
    """
    # LLMì´ ìƒì„±í•œ í›„ë³´ ëª¨í˜•
    raw_models = self._call_llm_for_models(question, available)
    
    # ê° ëª¨í˜•ë³„ ë³€ìˆ˜ í•„í„°ë§
    policy = VariableConvergencePolicy()
    refined_models = []
    
    for raw_model in raw_models:
        # ë³€ìˆ˜ë¥¼ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_vars = self._sort_variables_by_importance(raw_model)
        
        refined_model = FermiModel(
            model_id=raw_model.model_id,
            formula="",
            variables={}
        )
        
        # ë³€ìˆ˜ í•˜ë‚˜ì”© ì¶”ê°€í•˜ë©° ìˆ˜ë ´ ì²´í¬
        for seq, var in enumerate(sorted_vars, 1):
            eval_result = policy.evaluate(
                current_model=refined_model,
                new_variable=var,
                variable_sequence=seq
            )
            
            if eval_result['should_add']:
                refined_model.variables[var.name] = var
                refined_model.total_variables += 1
                
                if eval_result['warning']:
                    logger.warning(f"    {eval_result['warning']}")
            else:
                # ìì—° ìˆ˜ë ´
                logger.info(f"    ëª¨í˜• '{refined_model.model_id}' ìˆ˜ë ´: "
                           f"{refined_model.total_variables}ê°œ ë³€ìˆ˜")
                logger.info(f"    ì´ìœ : {eval_result['reason']}")
                break
        
        # ìˆ˜ì‹ ì¬êµ¬ì„± (ì„ íƒëœ ë³€ìˆ˜ë§Œ)
        refined_model.formula = self._rebuild_formula(refined_model)
        
        refined_models.append(refined_model)
    
    return refined_models
```

---

## ğŸ“Š íš¨ê³¼ ì˜ˆìƒ

### ê¸°ëŒ€ íš¨ê³¼

```yaml
ê°„ë‹¨í•œ ë¬¸ì œ:
  ì˜ˆ: "Churn RateëŠ”?"
  ì´ì „: 6ê°œê¹Œì§€ ê°€ëŠ¥ (ë¶ˆí•„ìš”)
  ì´í›„: 1-2ê°œë¡œ ìˆ˜ë ´ âœ…

ì¤‘ê°„ ë¬¸ì œ:
  ì˜ˆ: "ìŒì‹ì  SaaS ì‹œì¥ì€?"
  ì´ì „: 6ê°œ ê³ ì •
  ì´í›„: 4-5ê°œë¡œ ìˆ˜ë ´ âœ…

ë³µì¡í•œ ë¬¸ì œ:
  ì˜ˆ: "ì‚°ì—…ë³„ ì„¸ë¶„í™” ì‹œì¥"
  ì´ì „: 6ê°œ ì œí•œ (ë¶€ì¡±í•  ìˆ˜ë„)
  ì´í›„: 7-9ê°œê¹Œì§€ í—ˆìš© (ê²½ê³  í¬í•¨) âœ…

ë§¤ìš° ë³µì¡:
  ì´ì „: 6ê°œ ì œí•œ (ê°•ì œ)
  ì´í›„: 10ê°œ ì ˆëŒ€ ìƒí•œ (ë…¼ë¦¬ì  ì¤‘ë‹¨) âœ…

í‰ê·  ë³€ìˆ˜ ê°œìˆ˜:
  ì´ì „: ~5ê°œ (ëª¨ë“  ë¬¸ì œ)
  ì´í›„: ~4ê°œ (ìì—° ìˆ˜ë ´) âœ…
```

---

## ğŸ¯ ì„¤ì • ê¶Œì¥ê°’

### ê¸°ë³¸ ì„¤ì • (ëŒ€ë¶€ë¶„ ë¬¸ì œ)

```python
policy = VariableConvergencePolicy(
    min_confidence_gain=0.05,  # 5% ì´ìƒ ê°œì„ 
    recommended_max=6,          # Occam's Razor
    absolute_max=10             # Miller's Law
)
```

### ì—„ê²©í•œ ì„¤ì • (ê°„ë‹¨í•œ ë¬¸ì œ ì„ í˜¸)

```python
policy = VariableConvergencePolicy(
    min_confidence_gain=0.10,  # 10% ì´ìƒ ê°œì„  (ë” ì—„ê²©)
    recommended_max=4,          # 4ê°œ ê¶Œì¥
    absolute_max=8              # 8ê°œ ì ˆëŒ€
)
```

### ìœ ì—°í•œ ì„¤ì • (ë³µì¡í•œ ë¬¸ì œ í—ˆìš©)

```python
policy = VariableConvergencePolicy(
    min_confidence_gain=0.03,  # 3% ì´ìƒ ê°œì„  (ë” ê´€ëŒ€)
    recommended_max=8,          # 8ê°œ ê¶Œì¥
    absolute_max=12             # 12ê°œ ì ˆëŒ€
)
```

---

## ğŸ“š ì´ë¡ ì  ê·¼ê±°

### 1. Occam's Razor (ì˜¤ì»´ì˜ ë©´ë„ë‚ )

**ì›ì¹™**: "ê°™ì€ ì„¤ëª…ë ¥ì´ë©´ ê°„ë‹¨í•œ ê²ƒì„ ì„ íƒ"

**ì ìš©**:
- ë³€ìˆ˜ ì¶”ê°€ê°€ ì‹¤ì§ˆì  ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
- Diminishing Returnsë¡œ êµ¬í˜„

---

### 2. Miller's Law (ë°€ëŸ¬ì˜ ë²•ì¹™)

**ì›ì¹™**: "ì¸ê°„ì€ 7Â±2ê°œ ì •ë³´ë¥¼ ë™ì‹œ ì²˜ë¦¬"

**ì ìš©**:
- ì ˆëŒ€ ìƒí•œ: 10ê°œ (7+3)
- ê¶Œì¥ ìƒí•œ: 6ê°œ (7-1)

---

### 3. Information Theory (ì •ë³´ ì´ë¡ )

**ì›ì¹™**: "ì •ë³´ ì¶”ê°€ê°€ ë¶ˆí™•ì‹¤ì„±ì„ ì¤„ì—¬ì•¼ ê°€ì¹˜"

**ì ìš©**:
- Marginal Confidence Gain
- 5% ë¯¸ë§Œ ê°œì„  â†’ ì •ë³´ ê°€ì¹˜ ë‚®ìŒ

---

### 4. Diminishing Returns (ìˆ˜í™• ì²´ê°)

**ì›ì¹™**: "íˆ¬ì… ì¦ê°€ â†’ ì‚°ì¶œ ì¦ê°€ìœ¨ ê°ì†Œ"

**ì ìš©**:
- ë³€ìˆ˜ ìˆœì„œë³„ ì„ê³„ê°’ ê°ì†Œ
- 1ë²ˆì§¸: 10% â†’ 9ë²ˆì§¸: 0.1%

---

## ğŸ” ìˆ˜í•™ì  ê²€ì¦

### Confidence ì¡°í•© (Geometric Mean)

**ì™œ Geometric Mean?**

```yaml
ë¬¸ì œ: "ì‹œì¥ = A Ã— B Ã— C"

Arithmetic Mean (ì‚°ìˆ  í‰ê· ):
  - (0.9 + 0.7 + 0.6) / 3 = 0.73
  - ë¬¸ì œ: ê³±ì…ˆ ëª¨í˜•ì¸ë° í‰ê· ? âŒ

Geometric Mean (ê¸°í•˜ í‰ê· ):
  - âˆ›(0.9 Ã— 0.7 Ã— 0.6) = 0.71
  - ë…¼ë¦¬: ê³±ì…ˆ ëª¨í˜•ì´ë¯€ë¡œ ê³±ì˜ nì œê³±ê·¼ âœ…
  - íŠ¹ì„±: í•˜ë‚˜ë¼ë„ ë‚®ìœ¼ë©´ ì „ì²´ ë‚®ìŒ (ê³±ì…ˆ íŠ¹ì„± ë°˜ì˜)

ì˜ˆì‹œ:
  - ëª¨ë“  ë³€ìˆ˜ 0.8 â†’ 0.8 (ì¼ê´€ì„±)
  - 1ê°œ ë³€ìˆ˜ 0.1 â†’ í° í•˜ë½ (ì•½í•œ ê³ ë¦¬ ë°˜ì˜)
```

**ê²€ì¦**: âœ… Geometric Mean ì í•©

---

### Marginal Gain ê³„ì‚°

**ê³µì‹**:
```
í˜„ì¬ nê°œ ë³€ìˆ˜: C_n = â¿âˆš(câ‚ Ã— câ‚‚ Ã— ... Ã— câ‚™)
n+1ê°œ ë³€ìˆ˜: C_{n+1} = â¿âºÂ¹âˆš(câ‚ Ã— câ‚‚ Ã— ... Ã— câ‚™ Ã— c_{n+1})

Marginal Gain = (C_{n+1} - C_n) / C_n
```

**ì˜ˆì‹œ ê³„ì‚°**:
```yaml
n=3, C_3 = âˆ›(0.9 Ã— 0.7 Ã— 0.6) = 0.709

n=4, c_4 = 0.8 ì¶”ê°€:
  C_4 = âˆœ(0.9 Ã— 0.7 Ã— 0.6 Ã— 0.8)
      = âˆœ(0.3024)
      = 0.742

Gain = (0.742 - 0.709) / 0.709
     = 0.047
     = 4.7%

íŒë‹¨: 4.7% < 5% â†’ ì¤‘ë‹¨ âŒ
```

**ê²€ì¦**: âœ… ìˆ˜í•™ì ìœ¼ë¡œ íƒ€ë‹¹

---

## ğŸ¯ ìµœì¢… ê¶Œì¥ ì‚¬í•­

### ê¶Œì¥ ë°©ì•ˆ: **Hybrid ì¢…í•© ì ‘ê·¼** (ë°©ì•ˆ 4)

**ì´ìœ **:
```yaml
1. ë…¼ë¦¬ì  ê·¼ê±° âœ…
   - Marginal Gain (ì •ë³´ ì´ë¡ )
   - Diminishing Returns (ê²½ì œí•™)
   - Miller's Law (ì¸ì§€ê³¼í•™)

2. ìœ ì—°ì„± âœ…
   - ê°„ë‹¨í•œ ë¬¸ì œ: 1-3ê°œë¡œ ìˆ˜ë ´
   - ë³µì¡í•œ ë¬¸ì œ: 7-9ê°œê¹Œì§€ í—ˆìš©

3. ì•ˆì „ì„± âœ…
   - ê¶Œì¥ ìƒí•œ (ê²½ê³ )
   - ì ˆëŒ€ ìƒí•œ (ê°•ì œ)

4. ì‹¤ìš©ì„± âœ…
   - êµ¬í˜„ ê°„ë‹¨
   - ì´í•´ ì‰¬ì›€
   - ì¡°ì • ê°€ëŠ¥ (threshold)
```

---

### êµ¬í˜„ ìœ„ì¹˜

```python
# tier3.py

class Tier3FermiPath:
    
    def __init__(self, config: Tier3Config):
        self.tier2 = Tier2JudgmentPath()
        
        # ë³€ìˆ˜ ìˆ˜ë ´ ì •ì±… â­
        self.convergence_policy = VariableConvergencePolicy(
            min_confidence_gain=config.min_confidence_gain,
            recommended_max=config.recommended_max,
            absolute_max=config.absolute_max
        )
    
    def _phase2_generate_models(...):
        # LLM ëª¨í˜• ìƒì„±
        raw_models = self._call_llm(...)
        
        # ê° ëª¨í˜•ë³„ ë³€ìˆ˜ ìˆ˜ë ´ â­
        refined_models = []
        for raw_model in raw_models:
            refined = self._refine_model_variables(
                raw_model,
                self.convergence_policy  # â­ ìˆ˜ë ´ ì •ì±… ì ìš©
            )
            refined_models.append(refined)
        
        return refined_models
```

---

### config ì—…ë°ì´íŠ¸

```python
# models.py - Tier3Config í™•ì¥

@dataclass
class Tier3Config:
    """Tier 3 ì„¤ì •"""
    max_depth: int = 4
    max_variables: int = 6  # Deprecated
    
    # v7.4.0 ì‹ ê·œ: ìˆ˜ë ´ ì •ì±… â­
    min_confidence_gain: float = 0.05  # 5% ì´ìƒ
    recommended_max: int = 6           # ê¶Œì¥ ìƒí•œ
    absolute_max: int = 10             # ì ˆëŒ€ ìƒí•œ
    
    # Diminishing thresholds (override ê°€ëŠ¥)
    diminishing_thresholds: Dict[int, float] = field(default_factory=lambda: {
        1: 0.10, 2: 0.07, 3: 0.05, 4: 0.03, 5: 0.02,
        6: 0.01, 7: 0.005, 8: 0.003, 9: 0.001
    })
    
    # LLM
    llm_model: str = "gpt-4o"
    llm_temperature: float = 0.3
```

---

## ğŸ“‹ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: ê¸°ë³¸ êµ¬í˜„ (1ì¼)

- [ ] `VariableConvergencePolicy` í´ë˜ìŠ¤
- [ ] `_geometric_mean_confidence()`
- [ ] `_predict_confidence()`
- [ ] `_calculate_model_score()`
- [ ] `_predict_score()`
- [ ] `evaluate()` ë©”ì„œë“œ

---

### Phase 2: í†µí•© (ë°˜ë‚˜ì ˆ)

- [ ] `Tier3Config` í™•ì¥
- [ ] `Tier3FermiPath.__init__()` ìˆ˜ì •
- [ ] `_refine_model_variables()` êµ¬í˜„
- [ ] `_phase2_generate_models()` í†µí•©

---

### Phase 3: í…ŒìŠ¤íŠ¸ (ë°˜ë‚˜ì ˆ)

- [ ] ê°„ë‹¨í•œ ë¬¸ì œ í…ŒìŠ¤íŠ¸ (1-3ê°œ ìˆ˜ë ´)
- [ ] ë³µì¡í•œ ë¬¸ì œ í…ŒìŠ¤íŠ¸ (7-9ê°œ)
- [ ] ì ˆëŒ€ ìƒí•œ í…ŒìŠ¤íŠ¸ (10ê°œ ì¤‘ë‹¨)
- [ ] Confidence Gain ê³„ì‚° ê²€ì¦

---

## ğŸŠ ìµœì¢… ê²°ë¡ 

### ë¬¸ì œ í•´ê²°: âœ…

**ê¸°ì¡´ ë¬¸ì œ**:
- âŒ ìì˜ì  ê¸°ì¤€ (6ê°œ)
- âŒ ë§¥ë½ ë¬´ì‹œ
- âŒ ìˆ˜ë ´ ë…¼ë¦¬ ì—†ìŒ

**í•´ê²° ë°©ì•ˆ**:
- âœ… ë…¼ë¦¬ì  ê¸°ì¤€ (Marginal Gain + Diminishing Returns)
- âœ… ë§¥ë½ ë°˜ì˜ (ìì—° ìˆ˜ë ´)
- âœ… ìˆ˜í•™ì  ê·¼ê±° (Geometric Mean, Information Theory)
- âœ… ì•ˆì „ ì¥ì¹˜ (ê¶Œì¥ 6ê°œ, ì ˆëŒ€ 10ê°œ)

---

### êµ¬í˜„ ê¶Œì¥

```yaml
ìš°ì„ ìˆœìœ„: P0 (Tier 3 êµ¬í˜„ ì‹œ í•„ìˆ˜)

êµ¬í˜„ ì‹œì : Tier 3 êµ¬í˜„ê³¼ ë™ì‹œ

ì˜ˆìƒ ì†Œìš”: +1ì¼ (ì¶”ê°€)
  - Tier 3 ê¸°ë³¸: 3-5ì¼
  - ìˆ˜ë ´ ì •ì±…: +1ì¼
  - ì´: 4-6ì¼

íš¨ê³¼:
  âœ… ë…¼ë¦¬ì  ì •ë‹¹ì„±
  âœ… ìì—°ìŠ¤ëŸ¬ìš´ ìˆ˜ë ´
  âœ… ìœ ì—°ì„± (ê°„ë‹¨ 1-3ê°œ, ë³µì¡ 7-9ê°œ)
  âœ… ì•ˆì „ì„± (10ê°œ ì ˆëŒ€ ìƒí•œ)
```

---

**ì‘ì„± ì™„ë£Œ**: 2025-11-08 01:25  
**ìƒíƒœ**: âœ… **ë³€ìˆ˜ ìˆ˜ë ´ ë©”ì»¤ë‹ˆì¦˜ ì„¤ê³„ ì™„ë£Œ**  
**ê¶Œì¥**: Hybrid ì¢…í•© ì ‘ê·¼ (ë°©ì•ˆ 4)

ğŸ‰ **ë…¼ë¦¬ì  ë³€ìˆ˜ ìˆ˜ë ´ ì„¤ê³„ ì™„ë£Œ!**

