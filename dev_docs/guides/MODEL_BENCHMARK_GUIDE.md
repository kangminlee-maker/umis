# OpenAI 모델 벤치마크 가이드
**실제 성능 테스트 및 평가 방법**

---

## 🎯 목적

최신 OpenAI 모델들(gpt-5-nano, gpt-4.1-nano 등)의 **실제 성능**을 UMIS 작업으로 테스트하여:
1. 문서상 추정치 검증
2. 비용-품질 트레이드오프 확인
3. 최적 모델 구성 결정

---

## 🚀 빠른 시작

### 1단계: 빠른 테스트 (추천!)

```bash
cd /Users/kangmin/umis_main_1103/umis
python3 scripts/interactive_model_benchmark.py
```

메뉴에서 **1번 선택** (빠른 테스트):
- 모델: gpt-5-nano, gpt-4.1-nano, gpt-4o-mini (3개)
- 시나리오: Phase 0-3 (3개)
- 총: 9개 테스트
- 소요 시간: 10-15분

### 2단계: 각 응답 평가

각 모델 응답마다:

```
1. 응답 내용 확인
   - JSON 형식 준수?
   - 답변 값이 합리적?
   - 논리적 근거 있음?

2. 품질 점수 입력 (0-100):
   100: 완벽 (정확, 논리적, 완전)
   90: 우수 (약간의 개선 여지)
   80: 양호 (실용적으로 사용 가능)
   70: 보통 (개선 필요하지만 기본 OK)
   60: 미흡 (논리 약함 또는 부정확)
   50 이하: 불합격

3. 세부 평가 (y/n):
   각 평가 기준마다 만족 여부

4. 코멘트 (선택):
   특이사항, 개선점 등
```

### 3단계: 결과 확인

```
테스트 완료 후:
- benchmark_nano_quick.json 생성
- 콘솔에 리포트 출력
  - 모델별 평균 품질
  - 평균 비용
  - 평균 시간
  - 가성비 TOP 3
```

---

## 📋 평가 기준

### Phase 0 (Literal)

```yaml
100점 기준:
  ✅ 정확한 값 (200,000)
  ✅ 올바른 단위 (원)
  ✅ JSON 형식 완벽
  ✅ Confidence 1.0
  ✅ 1초 이내

감점 요인:
  - 값 오류: -50점
  - JSON 파싱 실패: -30점
  - 단위 누락: -10점
  - 느린 응답 (>3초): -5점
```

### Phase 2 (계산)

```yaml
100점 기준:
  ✅ 정확한 계산 (1,600,000)
  ✅ 공식 이해 (LTV = ARPU / Churn)
  ✅ JSON 형식
  ✅ Confidence 1.0

감점 요인:
  - 계산 오류: -50점
  - 공식 오해: -30점
  - 단위 오류: -10점
```

### Phase 3 (템플릿 있음)

```yaml
100점 기준:
  ✅ 합리적 범위 (15-25만원)
  ✅ 단계별 계산 명확
  ✅ 벤치마크 활용
  ✅ 조정 계수 논리적
  ✅ Confidence 0.65-0.80

90점 기준:
  ✅ 범위 약간 벗어남 (±20%)
  ✅ 단계 일부 생략
  ✅ 논리는 합리적

80점 기준:
  ✅ 범위 벗어남 (±30%)
  ⚠️ 단계 설명 부족
  ✅ 기본 논리 OK

70점 이하:
  ❌ 범위 크게 벗어남 (>±50%)
  ❌ 논리 빈약
  ❌ 근거 불충분
```

### Phase 3 (템플릿 없음)

```yaml
100점 기준:
  ✅ 창의적 접근
  ✅ 시장 이해도 높음
  ✅ 경쟁사 분석 포함
  ✅ 합리적 결과 (2-5만원)
  ✅ 상세한 근거

90점 기준:
  ✅ 논리적 접근
  ⚠️ 일부 요소 누락
  ✅ 합리적 범위

80점 기준:
  ✅ 기본 논리 OK
  ⚠️ 깊이 부족
  ✅ 사용 가능한 결과

70점 이하:
  ❌ 논리 빈약
  ❌ 시장 이해 부족
  ❌ 근거 불충분
```

### Phase 4 (단순 Fermi)

```yaml
100점 기준:
  ✅ 2개 이상 독립적 모형
  ✅ 각 모형 논리 명확
  ✅ 변수 합리적
  ✅ 검증 단계 포함
  ✅ 합리적 결과 (2000-3500개)

90점 기준:
  ✅ 2개 모형
  ⚠️ 논리 약간 약함
  ✅ 범위 OK

80점 기준:
  ⚠️ 1개 모형만
  ✅ 논리 있음
  ✅ 범위 합리적

70점 이하:
  ❌ 모형 부실
  ❌ 논리 빈약
  ❌ 범위 벗어남
```

### Phase 4 (복잡 Fermi)

```yaml
100점 기준:
  ✅ 3개 이상 다각도 모형
  ✅ 창의적 분해
  ✅ 변수 간 관계 명확
  ✅ 상세한 근거
  ✅ 합리적 결과 (1000-5000억)

90점 기준:
  ✅ 2-3개 모형
  ✅ 논리적 분해
  ⚠️ 일부 변수 단순화
  ✅ 범위 OK

80점 기준:
  ⚠️ 1-2개 모형
  ✅ 기본 논리
  ✅ 사용 가능

70점 이하:
  ❌ 분해 부실
  ❌ 창의성 부족
  ❌ 논리 빈약
```

---

## 💡 테스트 전략

### 전략 1: 단계적 접근 (권장!)

```yaml
Week 1: nano 모델 집중 테스트
  모델: gpt-5-nano, gpt-4.1-nano, gpt-4o-mini
  시나리오: Phase 0-3 (SC-001, 002, 003)
  
  목표:
    - gpt-5-nano 성능 검증
    - 85점 이상이면 채택
    - 품질-비용 트레이드오프 확인
  
  의사결정:
    gpt-5-nano >= 85점:
      → 70% 작업 전환 결정
      → 추가 37% 비용 절감
    
    gpt-5-nano < 85점:
      → GPT-4o-mini 유지
      → 현재 계획 유지

Week 2: 중급 모델 (Phase 3-4)
  모델: gpt-4o, gpt-4.1, gpt-5-mini
  시나리오: SC-004, 005
  
  목표:
    - Phase 3 (템플릿 X) 최적 모델
    - Phase 4 (단순) 최적 모델

Week 3: Thinking 모델 (Phase 4)
  모델: o1-mini, o3-mini, o3
  시나리오: SC-006
  
  목표:
    - Phase 4 (복잡) 최적 모델
    - o1-mini vs Sonnet Think 비교
```

### 전략 2: 비교 테스트

```yaml
목적: 두 모델 직접 비교

예시: gpt-5-nano vs GPT-4o-mini
  1. 같은 시나리오 양쪽 테스트
  2. 응답 나란히 비교
  3. 품질 차이 측정
  4. 비용 차이 고려
  
  의사결정:
    품질 차이 < 10점:
      → gpt-5-nano 채택 (44% 절감)
    
    품질 차이 >= 10점:
      → GPT-4o-mini 유지 (안전)
```

---

## 📊 결과 분석 방법

### 1. 원시 데이터 확인

```json
// benchmark_results_YYYYMMDD_HHMMSS.json

{
  "metadata": {
    "timestamp": "2025-11-18T...",
    "total_tests": 9,
    "successful_tests": 9
  },
  "results": [
    {
      "model": "gpt-5-nano",
      "scenario_id": "SC-001",
      "scenario_name": "Phase 0 (Literal)",
      "response": {
        "value": 200000,
        "unit": "원",
        "confidence": 1.0
      },
      "cost": 0.000000125,
      "elapsed_seconds": 0.85,
      "user_evaluation": {
        "quality_score": 95,
        "evaluations": {
          "criterion_1": "y",
          "criterion_2": "y",
          "criterion_3": "y"
        },
        "comment": "완벽한 응답, 매우 빠름"
      }
    },
    ...
  ]
}
```

### 2. 모델별 평균 계산

```python
# 간단한 분석 스크립트

import json

with open('benchmark_results_20251118_123456.json') as f:
    data = json.load(f)

# 모델별 그룹화
models = {}
for result in data['results']:
    if not result['success']:
        continue
    
    model = result['model']
    if model not in models:
        models[model] = {'quality': [], 'cost': [], 'time': []}
    
    eval_data = result.get('user_evaluation', {})
    if eval_data.get('quality_score'):
        models[model]['quality'].append(eval_data['quality_score'])
        models[model]['cost'].append(result['cost'])
        models[model]['time'].append(result['elapsed_seconds'])

# 평균 계산
for model, stats in models.items():
    avg_quality = sum(stats['quality']) / len(stats['quality'])
    avg_cost = sum(stats['cost']) / len(stats['cost'])
    avg_time = sum(stats['time']) / len(stats['time'])
    
    print(f"{model}: 품질 {avg_quality:.1f}, 비용 ${avg_cost:.6f}, 시간 {avg_time:.2f}초")
```

### 3. 의사결정 프레임워크

```yaml
gpt-5-nano 채택 기준:

필수 조건 (모두 만족):
  - Phase 0-2 평균 품질 >= 85점
  - Phase 3 (템플릿 O) 품질 >= 80점
  - 응답 시간 < 3초
  - JSON 파싱 성공률 >= 95%
  - 에러율 < 5%

선택 조건 (하나 이상):
  - GPT-4o-mini 대비 품질 차이 < 10점
  - 비용 절감 > 40%
  - 속도 동등 이상

최종 결정:
  모든 필수 + 1개 선택 만족:
    → 70% 작업에 gpt-5-nano 채택
    → 문서 업데이트
    → 라우터 구현
  
  불만족:
    → GPT-4o-mini 유지
    → nano는 제외
```

---

## 🎯 예상 시나리오

### 시나리오 1: gpt-5-nano 성공 (85점 이상)

```yaml
결과 예시:
  gpt-5-nano:
    - Phase 0: 95점
    - Phase 2: 90점
    - Phase 3 (템플릿 O): 85점
    평균: 90점
    
  GPT-4o-mini:
    - Phase 0: 95점
    - Phase 2: 95점
    - Phase 3 (템플릿 O): 90점
    평균: 93점
  
  차이: -3점 (허용 가능!)

결정:
  ✅ gpt-5-nano 채택
  
  새 구성:
    - 70%: gpt-5-nano ($0.00025)
    - 15%: GPT-4o-mini ($0.00045)
    - 8%: gpt-4o ($0.0075)
    - 7%: o1-mini ($0.0033)
  
  효과:
    비용: $1.21 → $1.07 (12% 추가 절감)
    품질: 90% → 87% (-3%, 허용)
    총 절감: 93% (vs 현재 $15)
```

### 시나리오 2: gpt-5-nano 실패 (85점 미만)

```yaml
결과 예시:
  gpt-5-nano:
    - Phase 0: 80점 (가끔 오류)
    - Phase 2: 75점 (계산 실수)
    - Phase 3 (템플릿 O): 70점 (논리 약함)
    평균: 75점 (불합격!)
  
  GPT-4o-mini:
    - 평균: 93점

결정:
  ❌ gpt-5-nano 제외
  
  유지 구성:
    - 85%: GPT-4o-mini ($0.00045)
    - 8%: gpt-4o ($0.0075)
    - 7%: o1-mini ($0.0033)
  
  효과:
    비용: $1.21/1,000회
    품질: 90%
    절감: 92% (vs 현재)
    
  여전히 매우 좋음!
```

### 시나리오 3: 중간 성능 (80-84점)

```yaml
결과 예시:
  gpt-5-nano:
    평균: 82점 (애매한 영역)

결정:
  ⚠️ 조건부 채택
  
  전략:
    - Phase 0-2만 nano 사용 (50%)
    - Phase 3은 mini 유지 (35%)
  
  효과:
    비용: $1.21 → $1.14 (6% 추가 절감)
    품질: 90% 유지
```

---

## 🔧 실행 가이드

### 모드 1: 빠른 테스트 (10-15분)

```bash
python3 scripts/interactive_model_benchmark.py

선택: 1 (빠른 테스트)

진행:
  테스트 1/9: gpt-5-nano + Phase 0
  → 응답 확인
  → 품질 입력: 95
  → 세부 평가: y, y, y
  
  테스트 2/9: gpt-5-nano + Phase 2
  → ...
  
  ...

완료:
  ✅ benchmark_nano_quick.json 생성
  📊 리포트 출력
```

### 모드 2: Phase별 테스트 (선택적)

```bash
python3 scripts/interactive_model_benchmark.py

선택: 2 (Phase별)

Phase 선택:
  0: Phase 0
  1: Phase 2
  2: Phase 3 (템플릿 O)
  3: Phase 3 (템플릿 X)
  4: Phase 4 (단순)
  5: Phase 4 (복잡)

선택: 0 (Phase 0만 테스트)

모델 선택:
  1: nano 모델 (3개)
  2: mini 모델 (3개)
  3: standard 모델 (3개)
  4: thinking 모델 (4개)

선택: 1 (nano만)

진행:
  테스트 1/3: gpt-5-nano + Phase 0
  테스트 2/3: gpt-4.1-nano + Phase 0
  테스트 3/3: gpt-4o-mini + Phase 0

완료:
  ✅ benchmark_phase0.json
```

### 모드 3: 전체 벤치마크 (2-3시간)

```
⚠️ 권장하지 않음:
  - 모든 모델 (10개) × 모든 시나리오 (6개) = 60개 테스트
  - 소요 시간 2-3시간
  - 평가 피로
  
대신: Phase별로 나눠서 진행
```

---

## 📋 체크리스트

### 테스트 전

- [ ] OPENAI_API_KEY 설정 확인
- [ ] OpenAI 계정 크레딧 확인 (최소 $5 권장)
- [ ] 평가 기준 숙지
- [ ] 시간 확보 (빠른 테스트: 15분)

### 테스트 중

- [ ] 각 응답 꼼꼼히 확인
- [ ] 일관된 기준으로 평가
- [ ] 특이사항 코멘트 기록
- [ ] 5개마다 중간 저장 확인

### 테스트 후

- [ ] 결과 파일 백업
- [ ] 리포트 스크린샷 저장
- [ ] 의사결정 기록
- [ ] 문서 업데이트

---

## 💡 팁

### 평가 일관성 유지

```yaml
기준점 설정:
  GPT-4o-mini = 90-95점 (기준)
  
  다른 모델:
    GPT-4o-mini보다 좋음: 95점+
    비슷함: 85-90점
    약간 나쁨: 80-85점
    나쁨: 70-80점
    매우 나쁨: 70점 미만

동일 시나리오:
  첫 번째 모델: 절대 평가
  이후 모델: 첫 번째와 비교 평가
  
  예:
    gpt-5-nano: 85점 (기준)
    GPT-4o-mini: 더 좋음 → 90점
    gpt-4.1-nano: 비슷함 → 85점
```

### 효율적 테스트

```yaml
우선순위:
  1. nano 모델 (가장 중요!)
     - gpt-5-nano vs GPT-4o-mini
     - Phase 0-3만
     - 의사결정에 직접 영향

  2. Thinking 모델
     - o1-mini vs Sonnet Think (비교 필요)
     - Phase 4만
     - 가성비 검증

  3. 나머지
     - 필요 시만
```

### 시간 절약

```
한 번에 3개 시나리오만:
  - 집중력 유지
  - 일관된 평가
  - 휴식 가능

여러 세션으로 분할:
  Session 1: nano 모델 + Phase 0-2 (6개, 10분)
  Session 2: nano 모델 + Phase 3 (3개, 10분)
  Session 3: o1-mini + Phase 4 (2개, 10분)
```

---

## 🎬 실행 예시

```bash
# Terminal
$ cd /Users/kangmin/umis_main_1103/umis
$ python3 scripts/interactive_model_benchmark.py

OpenAI 모델 벤치마크 도구
================================================

모드를 선택하세요:
  1: 빠른 테스트 (nano 모델 + Phase 0-3, 9개 테스트)
  2: Phase별 테스트 (선택적)
  3: 전체 벤치마크 (모든 모델 + 모든 시나리오)

선택 (1-3): 1

🚀 빠른 테스트 모드 (nano 모델 + Phase 0-3)

테스트 모델: 3개
테스트 시나리오: 3개
총 테스트 수: 9개

진행하시겠습니까? (y/n): y

################################################################################
# 모델: gpt-5-nano
# 가격: $0.05/1M 입력, $0.4/1M 출력
################################################################################

진행: 1/9

================================================================================
📝 시나리오: Phase 0 (Literal - 확정 데이터 조회)
🤖 모델: gpt-5-nano
================================================================================

📋 프롬프트:
--------------------------------------------------------------------------------
다음 데이터에서 "한국 B2B SaaS ARPU"를 찾아 반환하세요:
...
--------------------------------------------------------------------------------

⏳ gpt-5-nano 호출 중...

✅ 응답 받음
   비용: $0.000000125
   시간: 0.85초
   토큰: 250 (200→50)

📄 응답 내용:
--------------------------------------------------------------------------------
{
  "value": 200000,
  "unit": "원",
  "confidence": 1.0
}
--------------------------------------------------------------------------------

🎯 기대 답변:
   200,000원

📊 평가 기준:
   1. 정확한 값 (200,000)
   2. JSON 형식 준수
   3. 빠른 응답 (<2초)

================================================================================
👤 사용자 평가
================================================================================

품질 점수 (0-100, Enter=건너뛰기): 95

세부 평가 (각 항목 y/n):
  1. 정확한 값 (200,000)? (y/n/Enter=skip): y
  2. JSON 형식 준수? (y/n/Enter=skip): y
  3. 빠른 응답 (<2초)? (y/n/Enter=skip): y

코멘트 (선택, Enter=건너뛰기): 완벽함, 매우 빠름

계속 진행? (y/Enter=yes, n=중단, s=저장 후 중단): y

...

🎉 벤치마크 완료!

================================================================================
📊 벤치마크 리포트
================================================================================

총 테스트: 9개
성공: 9개
사용자 평가: 9개

================================================================================
모델별 성능 요약
================================================================================

모델                  | 평균 품질    | 평균 비용      | 평균 시간    | 가성비
--------------------------------------------------------------------------------
gpt-5-nano           |   88.3점    | $0.000000150  |   1.20초    |  588666
GPT-4o-mini          |   93.3점    | $0.000000450  |   0.95초    |  207407
gpt-4.1-nano         |   85.0점    | $0.000000200  |   1.10초    |  425000

================================================================================
🏆 가성비 TOP 3
================================================================================

1위: gpt-5-nano
   품질: 88.3점
   비용: $0.000000150/작업
   시간: 1.20초
   가성비: 588666

2위: gpt-4.1-nano
   품질: 85.0점
   비용: $0.000000200/작업
   시간: 1.10초
   가성비: 425000

3위: GPT-4o-mini
   품질: 93.3점
   비용: $0.000000450/작업
   시간: 0.95초
   가성비: 207407
```

---

## 📝 결과 해석 가이드

### 품질 점수 해석

```yaml
90-100점:
  의미: 프로덕션 레벨
  행동: 즉시 채택 가능
  
80-89점:
  의미: 실용적 사용 가능
  행동: 비용 절감 크면 채택 고려
  
70-79점:
  의미: 개선 필요
  행동: 템플릿/프롬프트 개선 후 재테스트
  
70점 미만:
  의미: 부적합
  행동: 해당 작업에서 제외
```

### 가성비 점수 해석

```yaml
가성비 = 품질 / (비용 × 1,000,000)

예시:
  gpt-5-nano: 88점 / ($0.00025 × 1M) = 352
  GPT-4o-mini: 93점 / ($0.00045 × 1M) = 207
  
  → nano가 가성비 1.7배 우수
  
결정:
  품질 차이 < 10점 AND 가성비 > 1.5배:
    → 저가 모델 채택
```

---

## 🎯 최종 의사결정 트리

```yaml
Step 1: gpt-5-nano 평가
  품질 >= 85점:
    → Step 2
  
  품질 < 85점:
    → gpt-5-nano 제외
    → GPT-4o-mini 기반 구성 ($1.21/1,000회)

Step 2: 품질-비용 트레이드오프
  품질 차이 (vs GPT-4o-mini) < 5점:
    → 70% 작업에 nano 채택
    → $1.07/1,000회 (93% 절감)
  
  품질 차이 5-10점:
    → 50% 작업만 nano 채택
    → $1.14/1,000회 (92% 절감)
  
  품질 차이 > 10점:
    → nano 제외
    → $1.21/1,000회 (92% 절감)

Step 3: o1-mini 평가 (Phase 4)
  Phase 4 품질 >= 85점:
    → o1-mini 채택 (7% 작업)
  
  품질 < 85점:
    → Sonnet Think 유지 (5% 작업)

최종 구성 결정!
```

---

**작성자**: AI Assistant  
**작성일**: 2025-11-18  
**목적**: 실제 성능 테스트를 통한 최적 모델 구성 결정  
**예상 소요**: 빠른 테스트 15분, 전체 테스트 1-2시간  

---

*실제 성능 테스트가 가장 정확합니다. 추정치는 참고용일 뿐, 실제 테스트로 검증하세요!*




